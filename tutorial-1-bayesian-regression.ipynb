{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian amplitude regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lectures you should've now covered, or at least discussed Bayesian networks.  Here we'll show you how to build these networks and use them in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by constructing a Bayesian layer in pytorch, and then building the Bayesian loss function.  We will then construct a Bayesian network from these layers, and use it to perform the same amplitude regression from the previous tutorial.  We will discuss how to analyse the outputs of the Bayesian network, and how this gives us the ability to estimate the error on our analysis.  This last step is crucial to the application of any numerical technique in physics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outline / tasks:\n",
    " - Imports \\& plotting set-up\n",
    " - Loading the data\n",
    " - Visualising the data\n",
    "     - visualise some of the kinematics of the process (transverse momentum of photons/gluons, MET)\n",
    "     - histogram the amplitudes\n",
    " - Preprocessing the data\n",
    "     - neural networks like $\\mathcal{O}(1)$ numbers\n",
    "     - how should we preprocess the data?\n",
    " - Datasets and dataloaders\n",
    "     - details are in the pytorch docs\n",
    " - Building a Bayesian layer\n",
    " - Constructing the Bayesian loss function\n",
    " - Building the Bayesian neural network\n",
    " - Optimising the neural network\n",
    " - Plot the train and validation losses as a function of the epochs\n",
    " - Study the results\n",
    "\n",
    "     \n",
    "Most practical pytorch skills you need for this work is covered in the basics tutorial at https://pytorch.org/tutorials/beginner/basics/intro.html.\n",
    "\n",
    "Please download the training data `tutorial-2-data.zip` and extract it to the folder `data/tutorial-2-data/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to run this tutorial in google colab, you can open a new colab and then upload this file. \n",
    "\n",
    "The data can be downloaded using\n",
    "\n",
    "```\n",
    "NEEDS TO BE CHANGED!\n",
    "!wget -O tutorial-2-data.zip https://www.dropbox.com/s/n5e66w91rgmbqz2/dlpp-data.zip?dl=0&file_subpath=%2Fdlpp-data%2Ftutorial-2-data\n",
    "!unzip \"tutorial-2-data.zip\"\n",
    "!mkdir tutorial-2-data\n",
    "!mv dlpp-data/tutorial-2-data/* tutorial-2-data/.\n",
    "!rm -r __MACOSX/\n",
    "!rm -r dlpp-data/\n",
    "!ls\n",
    "```\n",
    "\n",
    "Make sure you switch to a GPU runtime to fully utilize the colab. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import Tensor\n",
    "from torch.nn.parameter import Parameter, UninitializedParameter\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import init\n",
    "from torch.nn import Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import matplotlib.colors as mcolors\n",
    "import colorsys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dat = np.load(\"data/tutorial-2-data/trn_dat.npy\")\n",
    "trn_amp = np.load(\"data/tutorial-2-data/trn_amp.npy\")\n",
    "\n",
    "val_dat = np.load(\"data/tutorial-2-data/val_dat.npy\")\n",
    "val_amp = np.load(\"data/tutorial-2-data/val_amp.npy\")\n",
    "\n",
    "tst_dat = np.load(\"data/tutorial-2-data/tst_dat.npy\")\n",
    "tst_amp = np.load(\"data/tutorial-2-data/tst_amp.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (30000, 5, 4)\n",
      "train amp  shape: (30000,)\n",
      "test  data shape: (30000, 5, 4)\n",
      "test  amp  shape: (30000,)\n",
      "val   data shape: (30000, 5, 4)\n",
      "val   amp  shape: (30000,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"train data shape: {trn_dat.shape}\")\n",
    "print(f\"train amp  shape: {trn_amp.shape}\")\n",
    "print(f\"test  data shape: {tst_dat.shape}\")\n",
    "print(f\"test  amp  shape: {tst_amp.shape}\")\n",
    "print(f\"val   data shape: {val_dat.shape}\")\n",
    "print(f\"val   amp  shape: {val_amp.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not repeat the visualisation of the data, see the previous tutorial for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pt(fv):\n",
    "    \"\"\" returns p_T of given four vector \"\"\"\n",
    "    ptsq = np.round(fv[:, 1]**2 + fv[:, 2]**2, 5)\n",
    "    return np.sqrt(ptsq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index 2 is leading photon, not gluon (which is 4)\n",
    "trn_dat_gluon_pt = get_pt(trn_dat[:, 4])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nev = trn_dat.shape[0]\n",
    "trn_datf = np.reshape(trn_dat, (nev,-1))\n",
    "val_datf = np.reshape(val_dat, (nev,-1))\n",
    "tst_datf = np.reshape(tst_dat, (nev,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 20)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_datf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = np.mean(trn_dat_gluon_pt)\n",
    "trn_datf = trn_datf / gpt\n",
    "val_datf = val_datf / gpt\n",
    "tst_datf = tst_datf / gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_datfp = torch.Tensor(trn_datf)\n",
    "val_datfp = torch.Tensor(val_datf)\n",
    "tst_datfp = torch.Tensor(tst_datf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_ampl = np.log(trn_amp)\n",
    "val_ampl = np.log(val_amp)\n",
    "tst_ampl = np.log(tst_amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_amplp = torch.Tensor(trn_ampl)\n",
    "val_amplp = torch.Tensor(val_ampl)\n",
    "tst_amplp = torch.Tensor(tst_ampl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class amp_dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data, amp):\n",
    "        self.data = data\n",
    "        self.amp = amp\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.amp)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.amp[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dataset = amp_dataset(trn_datfp, trn_amplp.unsqueeze(-1))\n",
    "val_dataset = amp_dataset(val_datfp, val_amplp.unsqueeze(-1))\n",
    "tst_dataset = amp_dataset(tst_datfp, tst_amplp.unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dataloader = DataLoader(trn_dataset, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
    "tst_dataloader = DataLoader(tst_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Bayesian layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's look at the source code for a **basic linear layer** in pytorch:\n",
    "\n",
    "(https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#Linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "class Linear(Module):\n",
    "    \n",
    "    __constants__ = ['in_features', 'out_features']\n",
    "    in_features: int\n",
    "    out_features: int\n",
    "    weight: Tensor\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True,\n",
    "                 device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.empty(out_features, **factory_kwargs))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return F.linear(input, self.weight, self.bias)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return 'in_features={}, out_features={}, bias={}'.format(\n",
    "            self.in_features, self.out_features, self.bias is not None\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objects of this class apply a linear transformation to the incoming data: $y = xA^T + b$.\n",
    "\n",
    "The input arguments are required to initialise the layer, so, in the \\_\\_init()\\_\\_ function.  We have:\n",
    "- in_features: size of each input sample\n",
    "- out_features: size of each output sample\n",
    "- bias: If set to ``False``, the layer will not learn an additive bias.  Default: ``True``\n",
    "\n",
    "The shapes are:\n",
    "- Input: $(*, H_{in})$ where $*$ means any number of dimensions including none and $H_{in} = \\text{in_features}$.\n",
    "- Output: $(*, H_{out})$ where all but the last dimension are the same shape as the input and $H_{out} = \\text{out_features}$.\n",
    "\n",
    "The layer has attributes:\n",
    "- weight: the learnable weights of the module of shape $(\\text{out_features}, \\text{in_features})$. The values are initialized from $\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})$, where $k = \\frac{1}{\\text{in_features}}$\n",
    "- bias:   the learnable bias of the module of shape $(\\text{out_features})$.  If `bias` is ``True``, the values are initialized from $\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})$ where $k = \\frac{1}{\\text{in_features}}$.\n",
    "\n",
    "Examples::\n",
    "\n",
    "    >>> m = nn.Linear(20, 30)\n",
    "    \n",
    "    >>> input = torch.randn(128, 20)\n",
    "    \n",
    "    >>> output = m(input)\n",
    "    \n",
    "    >>> print(output.size())\n",
    "    \n",
    "    torch.Size([128, 30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the lecture, we know that in a Bayesian network the weights are replaced by Gaussian distributions, and on a forward pass we get the output by sampling from that distribution.\n",
    "\n",
    "So the biases are the same as in the linear layer.  But not each weight is a Gaussian distibution and so needs a mean and a variance.  The bias and the mean and variance of the weight distribution will be learnable.  In practice it's easier to work with the log of the variance as it is more stable when optimising the network.\n",
    "\n",
    "We need to be able to sample from the Gaussian weight distributions, and compute derivatives of the output in order to update the network parameters.  To do this we use something called the 're-parameterisation trick'.  It involves sampling random noise from a unit normal distribution, and then transforming that number using the mean and variance of the weight distribution in this way:\n",
    "\\begin{equation}\n",
    "w = \\mu + \\sigma\\times r\n",
    "\\end{equation}\n",
    "where $r$ is a random number sampled from a unit normal distribution (Gaussian with mean and variance equal to one), $\\mu$ is the mean of the weight distribution, and $\\sigma$ is the standard deviation.  In this way we separate the stochastic part of the function from the parameters defining the distribution.  And so if we take any differentiable function of $x$ (e.g. an activation function), we can compute derivatives of that function with respect to the mean and variance of the weight distribution.\n",
    "\n",
    "In the `forward` method we then need to implement this reparameterisation trick for the weights, and then apply the same linear transformation as in the standard linear layer.\n",
    "\n",
    "On each forward pass we need to generate a set of random numbers with the same shape as our means and variances.  Choosing a set of random numbers for the sampling is equivalent to 'sampling' a new neural network from the Bayesian neural network.  And sometimes at the end of the analysis, we will want to keep the same network for testing.  So we don't always want to re-sample the random numbers on each forward pass.  To control this we define a flag called `self.resample`, with default set to ``True``.\n",
    "\n",
    "We also need a `reset_parameters` function to reset the parameters in the network.  This is standard for layers in pytorch.  We use a slightly different function to do this than is used in the pytorch linear layer, as can be seen below.\n",
    "\n",
    "From the lecture, you know that the weight distributions require a prior.  The simplest choice for this prior is just a Gaussian distribution with a mean and variance of one.  Results are typically not too sensitive to this prior, as long as the values are within reasonable limits.  For example, $\\mathcal{O}(1)$ means and standard deviations.  Going beyond $\\mathcal{O}(1)$ numbers just leads to numerical instabilities in the training.  The Bayesian loss function contains a term coming from the KL divergence between the weight distributions in the network and their priors.  So the layers should have some funcitonality to return these values.  The KL divergence for the layer is:\n",
    "\\begin{equation}\n",
    "\\text{KL} = \\sum_{\\text{weights}} 0.5 \\times (  \\mu^2 + \\sigma^2 - \\log\\sigma^2 - 1 )\n",
    "\\end{equation}\n",
    "\n",
    "So let's build the **simplest Bayesian layer** we can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VBLinear(Module):\n",
    "    # VB -> Variational Bayes\n",
    "    \n",
    "    __constants__ = ['in_features', 'out_features']\n",
    "    in_features: int\n",
    "    out_features: int\n",
    "    weight: Tensor\n",
    "    \n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(VBLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.resample = True\n",
    "        self.bias = Parameter(Tensor(out_features))\n",
    "        self.mu_w = Parameter(Tensor(out_features, in_features))\n",
    "        self.logsig2_w = Parameter(Tensor(out_features, in_features))\n",
    "        self.random = torch.randn_like(self.logsig2_w)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def forward(self, inpt):\n",
    "        if self.resample:\n",
    "            self.random = torch.randn_like(self.logsig2_w)\n",
    "        s2_w = self.logsig2_w.exp()\n",
    "        weight = self.mu_w + s2_w.sqrt() * self.random\n",
    "        return nn.functional.linear(inpt, weight, self.bias) #+ 1e-8\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / np.sqrt(self.mu_w.size(1))\n",
    "        self.mu_w.data.normal_(0, stdv)\n",
    "        self.logsig2_w.data.zero_().normal_(-9, 0.001)\n",
    "        self.bias.data.zero_()\n",
    "        \n",
    "    def KL(self, loguniform=False):\n",
    "        kl = 0.5 * (self.mu_w.pow(2) + self.logsig2_w.exp() - self.logsig2_w - 1).sum()\n",
    "        return kl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the layers, define:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlr0 = VBLinear(10, 5)\n",
    "tlr1 = VBLinear(5, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now some test data, a batch of 20 vectors with 10 elements each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(20, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now pass the data first through layer0 then through layer1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.7731e-01,  8.3171e-01],\n",
       "        [-3.1769e-02,  5.3920e-01],\n",
       "        [ 2.2206e-01,  4.9462e-01],\n",
       "        [ 2.7117e-01,  6.9837e-03],\n",
       "        [ 3.4637e-01,  3.6407e-01],\n",
       "        [ 5.1559e-01,  7.8347e-01],\n",
       "        [ 7.0615e-01,  1.1547e-01],\n",
       "        [ 4.9982e-01,  2.6103e-01],\n",
       "        [-5.5030e-02,  8.1928e-01],\n",
       "        [ 5.1803e-02,  7.7110e-01],\n",
       "        [ 3.0633e-04,  5.1951e-01],\n",
       "        [ 3.3047e-01,  7.2719e-01],\n",
       "        [ 7.0073e-01,  9.4546e-02],\n",
       "        [ 2.0342e-01,  9.4636e-01],\n",
       "        [ 5.6899e-01,  7.6980e-02],\n",
       "        [ 2.5679e-01,  2.9424e-01],\n",
       "        [ 3.9362e-01,  8.4933e-01],\n",
       "        [ 6.9639e-01,  2.9881e-01],\n",
       "        [ 2.0888e-01,  7.4399e-01],\n",
       "        [ 4.7607e-02,  1.1618e-01]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tlr1(tlr0(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note this has the correct shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tlr1(tlr0(x)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also try running the same data through the layer multiple times, you get different results.  This is because of the sampling in the Bayesian layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9953, grad_fn=<SelectBackward0>)\n",
      "tensor(0.9405, grad_fn=<SelectBackward0>)\n",
      "tensor(0.9374, grad_fn=<SelectBackward0>)\n",
      "tensor(0.9313, grad_fn=<SelectBackward0>)\n",
      "tensor(0.9450, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(tlr0(x)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bayesian loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the lectures we know that there are two parts to the Bayesian loss function:\n",
    "- The negative log Gaussian\n",
    "- the KL from the network weights\n",
    "\n",
    "The second comes from the layers, and the first is defined below.  This negative log Gaussian term acts on the two outputs from the Bayesian neural network:\n",
    "- the mean\n",
    "- the variance (which we parameterise as the log variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_log_gauss(outputs, targets):\n",
    "\n",
    "    mu = outputs[:, 0]\n",
    "    logsigma2 = outputs[:, 1]\n",
    "    out = torch.pow(mu - targets, 2) / (2 * logsigma2.exp()) + 1./2. * logsigma2\n",
    "    \n",
    "    return torch.mean(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Bayesian neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll build a simple network with one input and one output layer, and two hidden layers.  We define the dimensions of these layers below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipt_dim = 20\n",
    "opt_dim = 1\n",
    "hdn_dim = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build a very simple class for our neural network, which we call amp_net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bayes_amp_net(Module):\n",
    "    \n",
    "    # default hdn_dim is 30, but can be changed upon initialisation\n",
    "    def __init__(self, training_size, hdn_dim=50):\n",
    "\n",
    "        super(bayes_amp_net, self).__init__()\n",
    "\n",
    "        # the loss function depends on the amount of training data we have, so we need to store this\n",
    "        self.training_size = training_size\n",
    "        \n",
    "        # the activation layers of the network are not bayesian\n",
    "        # and we need to be able to access the bayesian layers separately\n",
    "\n",
    "        self.vb_layers = []\n",
    "        self.all_layers = []\n",
    "\n",
    "        # define the input layer\n",
    "        vb_layer = VBLinear(ipt_dim, hdn_dim)\n",
    "        self.vb_layers.append(vb_layer)\n",
    "        self.all_layers.append(vb_layer)\n",
    "        self.all_layers.append(nn.ReLU())\n",
    "\n",
    "        # loop over hidden layers\n",
    "        for i in range(2):\n",
    "            vb_layer = VBLinear(hdn_dim, hdn_dim)\n",
    "            self.vb_layers.append(vb_layer)\n",
    "            self.all_layers.append(vb_layer)\n",
    "            self.all_layers.append(nn.ReLU())\n",
    "\n",
    "        # define the output layer. use 2 dimensions to learn the uncertainty sigma_stoch=sigma_model,\n",
    "        # see eq. 1.57 of 2211.01421\n",
    "        vb_layer = VBLinear(hdn_dim, 2)\n",
    "        self.vb_layers.append(vb_layer)\n",
    "        self.all_layers.append(vb_layer)\n",
    "\n",
    "        # define the model as a Sequential net over all layers\n",
    "        self.model = nn.Sequential(*self.all_layers)\n",
    "\n",
    "    # and of course the forward function\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out\n",
    "\n",
    "    # we need the KL from the bayesian layers to compute the loss function\n",
    "    def KL(self):\n",
    "        kl = 0\n",
    "        for vb_layer in self.vb_layers:\n",
    "            kl += vb_layer.KL()\n",
    "        return kl / self.training_size\n",
    "    \n",
    "    # let's put the neg_log_gauss in the network class aswell since it is key to bayesian networks\n",
    "    def neg_log_gauss(self, outputs, targets):\n",
    "        mu = outputs[:, 0]\n",
    "        logsigma2 = outputs[:, 1]\n",
    "        out = torch.pow(mu - targets, 2) / (2 * logsigma2.exp()) + 1./2. * logsigma2\n",
    "        return torch.mean(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if we have a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise the neural network and send to the GPU if we have one.  We can also print the model to see what layers it has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bayes_amp_net(\n",
      "  (model): Sequential(\n",
      "    (0): VBLinear()\n",
      "    (1): ReLU()\n",
      "    (2): VBLinear()\n",
      "    (3): ReLU()\n",
      "    (4): VBLinear()\n",
      "    (5): ReLU()\n",
      "    (6): VBLinear()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "trn_len = trn_amplp.shape[0]\n",
    "hdn_dim = 50\n",
    "model = bayes_amp_net(trn_len, hdn_dim=hdn_dim).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test it briefly by throwing some random numbers into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(5, 20, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing the numbers through the Bayesian network gives us different outputs each time we run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0228, -0.0855],\n",
       "        [-0.0264,  0.0029],\n",
       "        [-0.1179, -0.1174],\n",
       "        [-0.1168, -0.0795],\n",
       "        [ 0.0148, -0.0195]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note again that the output has the correct shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicitons of the full Bayesian network should vary with each forward pass because of the weight sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0204, -0.0988], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor([-0.0088, -0.0850], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor([ 0.0019, -0.0879], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor([ 0.0020, -0.0992], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor([-0.0120, -0.0994], device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# print predictions for the same input data\n",
    "for i in range(5):\n",
    "    print(model(X)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimising (training) the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bayesian loss function has two terms which we have already definedl; the negative los Gaussian, and the KL divergence between the network and the network prior.  The latter comes from the KL divergence over the weights in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write a training loop for a single epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, model, optimizer):\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    loss_tot, kl_tot, neg_log_tot = 0.0, 0.0, 0.0\n",
    "    loss_during_opt, kl_during_opt, neg_log_during_opt = 0., 0., 0.\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        \n",
    "        # pass data through network\n",
    "        pred = model(X)\n",
    "        \n",
    "        # compute loss\n",
    "        nl = model.neg_log_gauss(pred, y.reshape(-1))\n",
    "        kl = model.KL()\n",
    "        loss = nl + kl\n",
    "\n",
    "        loss_during_opt += loss.item()\n",
    "        kl_during_opt += kl.item()\n",
    "        neg_log_during_opt += nl.item()\n",
    "\n",
    "        # reset gradients in optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # update weights with optimizer\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print the training loss every 100 updates\n",
    "        if batch % 100 == 0:\n",
    "            current = batch * len( X )\n",
    "            print(f\"current batch loss: {loss:>8f} KL: {kl:>8f} Neg-log {nl:>8f}  [{current:>5d}/{size:>5d}]\")\n",
    "    loss_live = loss_during_opt/len(dataloader)\n",
    "    kl_live = kl_during_opt / len(dataloader)\n",
    "    nl_live = neg_log_during_opt / len(dataloader)\n",
    "    \n",
    "    print(f\"avg train loss per batch in training: {loss_live:>8f}\")        \n",
    "    return loss_live, kl_live, nl_live"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To monitor the performance of the network on the regression task we want to calculate the loss of both the training data and the validation data on the same network, so we have the following functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_pass(dataloader, model):\n",
    "    \n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    nls = 0.0\n",
    "    kls = 0.0\n",
    "    vls = 0.0\n",
    "    mse = 0.0\n",
    "\n",
    "    # we don't need gradients here since we only use the forward pass\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            nl = model.neg_log_gauss(pred, y.reshape(-1))\n",
    "            kl = model.KL()\n",
    "            vl = nl.item() + kl.item()\n",
    "            mse += torch.mean((pred[:, 0] - y.reshape(-1))**2)\n",
    "            nls += nl\n",
    "            kls += kl\n",
    "            vls += vl\n",
    "\n",
    "    nls /= num_batches\n",
    "    kls /= num_batches\n",
    "    vls /= num_batches\n",
    "    mse /= num_batches\n",
    "    print( f\"avg val loss per batch: {vls:>8f} KL: {kls:>8f} Neg-log {nls:>8f} MSE {mse:>8}\" )\n",
    "    \n",
    "    return nls.cpu().numpy(), kls.cpu().numpy(), vls, mse.cpu().numpy()\n",
    "\n",
    "def trn_pass(dataloader, model):\n",
    "    \n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    nls = 0.0\n",
    "    kls = 0.0\n",
    "    tls = 0.0\n",
    "    mse = 0.0\n",
    "\n",
    "    # we don't need gradients here since we only use the forward pass\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            nl = model.neg_log_gauss(pred, y.reshape(-1))\n",
    "            kl = model.KL()\n",
    "            tl = nl.item() + kl.item()\n",
    "            mse += torch.mean((pred[:, 0] - y.reshape(-1))**2)\n",
    "            nls += nl\n",
    "            kls += kl\n",
    "            tls += tl\n",
    "\n",
    "    nls /= num_batches\n",
    "    kls /= num_batches\n",
    "    tls /= num_batches\n",
    "    mse /= num_batches\n",
    "    print( f\"avg trn loss per batch: {tls:>8f} KL: {kls:>8f} Neg-log {nls:>8f} MSE {mse:>8}\" )\n",
    "    \n",
    "    return nls.cpu().numpy(), kls.cpu().numpy(), tls, mse.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset length: 30000\n",
      "-----------------------------------------------\n",
      "model architecture\n",
      "-----------------------------------------------\n",
      "bayes_amp_net(\n",
      "  (model): Sequential(\n",
      "    (0): VBLinear()\n",
      "    (1): ReLU()\n",
      "    (2): VBLinear()\n",
      "    (3): ReLU()\n",
      "    (4): VBLinear()\n",
      "    (5): ReLU()\n",
      "    (6): VBLinear()\n",
      "  )\n",
      ")\n",
      "Model has 12352 trainable parameters\n",
      "-----------------------------------------------\n",
      "Epoch 1\n",
      "-----------------------------------------------\n",
      "current batch loss: 131.257019 KL: 0.815803 Neg-log 130.441223  [    0/30000]\n",
      "current batch loss: 5.723981 KL: 0.814350 Neg-log 4.909632  [12800/30000]\n",
      "current batch loss: 4.793431 KL: 0.812986 Neg-log 3.980446  [25600/30000]\n",
      "avg train loss per batch in training: 14.400713\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 4.356837 KL: 0.812527 Neg-log 3.544311 MSE 79.37751007080078\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 4.355132 KL: 0.812527 Neg-log 3.542607 MSE 78.64068603515625\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 2\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.263848 KL: 0.812526 Neg-log 3.451323  [    0/30000]\n",
      "current batch loss: 2.980782 KL: 0.811270 Neg-log 2.169511  [12800/30000]\n",
      "current batch loss: 2.730236 KL: 0.809976 Neg-log 1.920260  [25600/30000]\n",
      "avg train loss per batch in training: 3.130753\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 2.666157 KL: 0.809526 Neg-log 1.856628 MSE 90.20284271240234\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 2.667507 KL: 0.809526 Neg-log 1.857979 MSE 91.28723907470703\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 3\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.617321 KL: 0.809528 Neg-log 1.807792  [    0/30000]\n",
      "current batch loss: 2.599536 KL: 0.808277 Neg-log 1.791259  [12800/30000]\n",
      "current batch loss: 2.437778 KL: 0.807059 Neg-log 1.630719  [25600/30000]\n",
      "avg train loss per batch in training: 2.518255\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 2.349776 KL: 0.806637 Neg-log 1.543140 MSE 51.7949333190918\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 2.347802 KL: 0.806637 Neg-log 1.541166 MSE 53.08290481567383\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 4\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.341143 KL: 0.806636 Neg-log 1.534507  [    0/30000]\n",
      "current batch loss: 2.417443 KL: 0.805441 Neg-log 1.612001  [12800/30000]\n",
      "current batch loss: 2.017655 KL: 0.804301 Neg-log 1.213354  [25600/30000]\n",
      "avg train loss per batch in training: 2.166382\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 2.002768 KL: 0.803910 Neg-log 1.198856 MSE 11.864791870117188\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 2.011304 KL: 0.803910 Neg-log 1.207392 MSE 12.756367683410645\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 5\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.901593 KL: 0.803912 Neg-log 1.097681  [    0/30000]\n",
      "current batch loss: 1.736023 KL: 0.802821 Neg-log 0.933202  [12800/30000]\n",
      "current batch loss: 1.697657 KL: 0.801768 Neg-log 0.895889  [25600/30000]\n",
      "avg train loss per batch in training: 1.815318\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.584731 KL: 0.801411 Neg-log 0.783319 MSE 4.456842422485352\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.610179 KL: 0.801411 Neg-log 0.808767 MSE 5.124379634857178\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 6\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.470056 KL: 0.801412 Neg-log 0.668644  [    0/30000]\n",
      "current batch loss: 1.520059 KL: 0.800407 Neg-log 0.719652  [12800/30000]\n",
      "current batch loss: 1.308039 KL: 0.799429 Neg-log 0.508611  [25600/30000]\n",
      "avg train loss per batch in training: 1.552963\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.885331 KL: 0.799089 Neg-log 1.086241 MSE 5.21268367767334\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.851860 KL: 0.799089 Neg-log 1.052770 MSE 5.341855049133301\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 7\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.333853 KL: 0.799090 Neg-log 1.534763  [    0/30000]\n",
      "current batch loss: 1.493729 KL: 0.798104 Neg-log 0.695625  [12800/30000]\n",
      "current batch loss: 1.348718 KL: 0.797144 Neg-log 0.551575  [25600/30000]\n",
      "avg train loss per batch in training: 1.449324\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.596398 KL: 0.796799 Neg-log 0.799598 MSE 3.1796445846557617\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.610549 KL: 0.796799 Neg-log 0.813748 MSE 3.318739175796509\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 8\n",
      "-----------------------------------------------\n",
      "current batch loss: 4.568061 KL: 0.796800 Neg-log 3.771261  [    0/30000]\n",
      "current batch loss: 0.975494 KL: 0.795840 Neg-log 0.179653  [12800/30000]\n",
      "current batch loss: 1.135208 KL: 0.794879 Neg-log 0.340329  [25600/30000]\n",
      "avg train loss per batch in training: 1.315514\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.260253 KL: 0.794540 Neg-log 0.465712 MSE 1.5746943950653076\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.209550 KL: 0.794540 Neg-log 0.415009 MSE 1.5492500066757202\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 9\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.433927 KL: 0.794541 Neg-log 0.639386  [    0/30000]\n",
      "current batch loss: 0.884275 KL: 0.793564 Neg-log 0.090711  [12800/30000]\n",
      "current batch loss: 1.124073 KL: 0.792595 Neg-log 0.331478  [25600/30000]\n",
      "avg train loss per batch in training: 1.290758\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.885379 KL: 0.792256 Neg-log 1.093123 MSE 2.350398063659668\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 2.230340 KL: 0.792256 Neg-log 1.438083 MSE 2.8451333045959473\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 10\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.386258 KL: 0.792257 Neg-log 0.594001  [    0/30000]\n",
      "current batch loss: 0.999045 KL: 0.791290 Neg-log 0.207756  [12800/30000]\n",
      "current batch loss: 1.027163 KL: 0.790306 Neg-log 0.236857  [25600/30000]\n",
      "avg train loss per batch in training: 1.199288\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.363977 KL: 0.789964 Neg-log 0.574012 MSE 1.310792326927185\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.392885 KL: 0.789964 Neg-log 0.602920 MSE 1.3981369733810425\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 11\n",
      "-----------------------------------------------\n",
      "current batch loss: 3.373440 KL: 0.789965 Neg-log 2.583475  [    0/30000]\n",
      "current batch loss: 1.065807 KL: 0.788988 Neg-log 0.276819  [12800/30000]\n",
      "current batch loss: 1.004946 KL: 0.788003 Neg-log 0.216943  [25600/30000]\n",
      "avg train loss per batch in training: 1.199993\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.078179 KL: 0.787658 Neg-log 0.290523 MSE 0.9609704613685608\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.076028 KL: 0.787658 Neg-log 0.288371 MSE 1.0198616981506348\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 12\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.903522 KL: 0.787656 Neg-log 0.115865  [    0/30000]\n",
      "current batch loss: 0.788987 KL: 0.786662 Neg-log 0.002325  [12800/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 1.056168 KL: 0.785687 Neg-log 0.270482  [25600/30000]\n",
      "avg train loss per batch in training: 1.131860\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.075168 KL: 0.785340 Neg-log 0.289827 MSE 1.0716502666473389\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.079632 KL: 0.785340 Neg-log 0.294291 MSE 1.1142600774765015\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 13\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.918219 KL: 0.785341 Neg-log 0.132878  [    0/30000]\n",
      "current batch loss: 0.936009 KL: 0.784354 Neg-log 0.151655  [12800/30000]\n",
      "current batch loss: 1.260835 KL: 0.783359 Neg-log 0.477477  [25600/30000]\n",
      "avg train loss per batch in training: 1.158134\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.162962 KL: 0.783016 Neg-log 0.379946 MSE 1.0420767068862915\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.182907 KL: 0.783016 Neg-log 0.399891 MSE 1.0638974905014038\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 14\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.199603 KL: 0.783016 Neg-log 0.416588  [    0/30000]\n",
      "current batch loss: 0.768507 KL: 0.782017 Neg-log -0.013509  [12800/30000]\n",
      "current batch loss: 0.781531 KL: 0.781012 Neg-log 0.000519  [25600/30000]\n",
      "avg train loss per batch in training: 1.091892\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.976940 KL: 0.780669 Neg-log 0.196274 MSE 0.7696216106414795\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.992501 KL: 0.780669 Neg-log 0.211835 MSE 0.8416728973388672\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 15\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.125749 KL: 0.780667 Neg-log 0.345082  [    0/30000]\n",
      "current batch loss: 0.735633 KL: 0.779663 Neg-log -0.044030  [12800/30000]\n",
      "current batch loss: 0.754389 KL: 0.778654 Neg-log -0.024264  [25600/30000]\n",
      "avg train loss per batch in training: 1.020836\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.991866 KL: 0.778301 Neg-log 0.213566 MSE 0.7979868054389954\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.957625 KL: 0.778301 Neg-log 0.179325 MSE 0.752643346786499\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 16\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.086974 KL: 0.778300 Neg-log 0.308674  [    0/30000]\n",
      "current batch loss: 1.027576 KL: 0.777282 Neg-log 0.250294  [12800/30000]\n",
      "current batch loss: 0.541028 KL: 0.776254 Neg-log -0.235226  [25600/30000]\n",
      "avg train loss per batch in training: 1.023840\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.944288 KL: 0.775893 Neg-log 0.168398 MSE 0.744443416595459\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.968500 KL: 0.775893 Neg-log 0.192609 MSE 0.798332691192627\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 17\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.962355 KL: 0.775891 Neg-log 0.186464  [    0/30000]\n",
      "current batch loss: 0.517538 KL: 0.774868 Neg-log -0.257331  [12800/30000]\n",
      "current batch loss: 0.987589 KL: 0.773832 Neg-log 0.213757  [25600/30000]\n",
      "avg train loss per batch in training: 0.943678\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.173626 KL: 0.773468 Neg-log 0.400157 MSE 1.038030982017517\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.166430 KL: 0.773468 Neg-log 0.392961 MSE 1.0434850454330444\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 18\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.141506 KL: 0.773469 Neg-log 0.368037  [    0/30000]\n",
      "current batch loss: 1.180020 KL: 0.772437 Neg-log 0.407582  [12800/30000]\n",
      "current batch loss: 0.978016 KL: 0.771403 Neg-log 0.206612  [25600/30000]\n",
      "avg train loss per batch in training: 0.950536\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.897429 KL: 0.771042 Neg-log 0.126389 MSE 0.6879248023033142\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.884116 KL: 0.771042 Neg-log 0.113075 MSE 0.696190357208252\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 19\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.458806 KL: 0.771041 Neg-log -0.312235  [    0/30000]\n",
      "current batch loss: 0.661740 KL: 0.770003 Neg-log -0.108263  [12800/30000]\n",
      "current batch loss: 1.127073 KL: 0.768955 Neg-log 0.358118  [25600/30000]\n",
      "avg train loss per batch in training: 0.975365\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.870880 KL: 0.768599 Neg-log 0.102282 MSE 0.663570761680603\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.887379 KL: 0.768599 Neg-log 0.118782 MSE 0.6860237717628479\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 20\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.098513 KL: 0.768598 Neg-log 0.329915  [    0/30000]\n",
      "current batch loss: 0.713210 KL: 0.767567 Neg-log -0.054357  [12800/30000]\n",
      "current batch loss: 0.812079 KL: 0.766530 Neg-log 0.045549  [25600/30000]\n",
      "avg train loss per batch in training: 0.910295\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.091504 KL: 0.766164 Neg-log 0.325338 MSE 0.8903533220291138\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.104658 KL: 0.766164 Neg-log 0.338492 MSE 0.8986374735832214\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 21\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.815086 KL: 0.766166 Neg-log 0.048921  [    0/30000]\n",
      "current batch loss: 0.676076 KL: 0.765131 Neg-log -0.089054  [12800/30000]\n",
      "current batch loss: 0.851700 KL: 0.764167 Neg-log 0.087534  [25600/30000]\n",
      "avg train loss per batch in training: 0.947110\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.826863 KL: 0.763825 Neg-log 0.063037 MSE 0.6056721806526184\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.827630 KL: 0.763825 Neg-log 0.063805 MSE 0.6162622570991516\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 22\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.510608 KL: 0.763825 Neg-log -0.253218  [    0/30000]\n",
      "current batch loss: 0.513674 KL: 0.762908 Neg-log -0.249234  [12800/30000]\n",
      "current batch loss: 0.759562 KL: 0.761989 Neg-log -0.002427  [25600/30000]\n",
      "avg train loss per batch in training: 0.839390\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.821758 KL: 0.761659 Neg-log 0.060098 MSE 0.6321442127227783\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.781238 KL: 0.761659 Neg-log 0.019579 MSE 0.6172065734863281\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 23\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.951181 KL: 0.761660 Neg-log 1.189521  [    0/30000]\n",
      "current batch loss: 0.950835 KL: 0.760738 Neg-log 0.190097  [12800/30000]\n",
      "current batch loss: 0.498488 KL: 0.759815 Neg-log -0.261327  [25600/30000]\n",
      "avg train loss per batch in training: 0.855601\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.709687 KL: 0.759496 Neg-log -0.049811 MSE 0.5045697689056396\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.710665 KL: 0.759496 Neg-log -0.048832 MSE 0.5147484540939331\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 24\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.511728 KL: 0.759498 Neg-log -0.247769  [    0/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.468721 KL: 0.758576 Neg-log -0.289855  [12800/30000]\n",
      "current batch loss: 0.507516 KL: 0.757650 Neg-log -0.250134  [25600/30000]\n",
      "avg train loss per batch in training: 0.795077\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.777771 KL: 0.757324 Neg-log 0.020448 MSE 0.5684816241264343\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.799526 KL: 0.757324 Neg-log 0.042203 MSE 0.5905191898345947\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 25\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.634956 KL: 0.757323 Neg-log -0.122367  [    0/30000]\n",
      "current batch loss: 0.658388 KL: 0.756382 Neg-log -0.097994  [12800/30000]\n",
      "current batch loss: 0.561633 KL: 0.755438 Neg-log -0.193806  [25600/30000]\n",
      "avg train loss per batch in training: 0.849031\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.688726 KL: 0.755112 Neg-log -0.066385 MSE 0.4772060811519623\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.720685 KL: 0.755112 Neg-log -0.034427 MSE 0.5190063118934631\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 26\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.523039 KL: 0.755112 Neg-log -0.232073  [    0/30000]\n",
      "current batch loss: 0.719178 KL: 0.754174 Neg-log -0.034996  [12800/30000]\n",
      "current batch loss: 0.545934 KL: 0.753232 Neg-log -0.207299  [25600/30000]\n",
      "avg train loss per batch in training: 0.811567\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.117405 KL: 0.752899 Neg-log 0.364507 MSE 0.855509877204895\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 1.083505 KL: 0.752899 Neg-log 0.330607 MSE 0.8184139728546143\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 27\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.134448 KL: 0.752898 Neg-log 0.381550  [    0/30000]\n",
      "current batch loss: 1.019898 KL: 0.751956 Neg-log 0.267942  [12800/30000]\n",
      "current batch loss: 2.013688 KL: 0.751011 Neg-log 1.262677  [25600/30000]\n",
      "avg train loss per batch in training: 0.851749\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.666928 KL: 0.750691 Neg-log -0.083764 MSE 0.46509385108947754\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.682415 KL: 0.750691 Neg-log -0.068277 MSE 0.5010567903518677\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 28\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.781710 KL: 0.750692 Neg-log 0.031018  [    0/30000]\n",
      "current batch loss: 0.715353 KL: 0.749766 Neg-log -0.034414  [12800/30000]\n",
      "current batch loss: 0.797601 KL: 0.748845 Neg-log 0.048756  [25600/30000]\n",
      "avg train loss per batch in training: 0.806085\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.610797 KL: 0.748514 Neg-log -0.137716 MSE 0.45366910099983215\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.644805 KL: 0.748514 Neg-log -0.103707 MSE 0.4864481985569\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 29\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.571429 KL: 0.748512 Neg-log -0.177083  [    0/30000]\n",
      "current batch loss: 0.390069 KL: 0.747599 Neg-log -0.357529  [12800/30000]\n",
      "current batch loss: 1.566005 KL: 0.746684 Neg-log 0.819321  [25600/30000]\n",
      "avg train loss per batch in training: 0.683107\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.715589 KL: 0.746364 Neg-log -0.030775 MSE 0.5320581793785095\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.749615 KL: 0.746364 Neg-log 0.003251 MSE 0.6112431883811951\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 30\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.495401 KL: 0.746364 Neg-log -0.250963  [    0/30000]\n",
      "current batch loss: 0.365695 KL: 0.745430 Neg-log -0.379735  [12800/30000]\n",
      "current batch loss: 0.395056 KL: 0.744511 Neg-log -0.349455  [25600/30000]\n",
      "avg train loss per batch in training: 0.661165\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.668267 KL: 0.744182 Neg-log -0.075914 MSE 0.4564164876937866\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.712710 KL: 0.744182 Neg-log -0.031471 MSE 0.4805832505226135\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 31\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.364888 KL: 0.744181 Neg-log -0.379293  [    0/30000]\n",
      "current batch loss: 0.524901 KL: 0.743253 Neg-log -0.218352  [12800/30000]\n",
      "current batch loss: 0.893036 KL: 0.742314 Neg-log 0.150722  [25600/30000]\n",
      "avg train loss per batch in training: 0.641782\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.568544 KL: 0.741988 Neg-log -0.173443 MSE 0.401413232088089\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.586790 KL: 0.741988 Neg-log -0.155196 MSE 0.4197428524494171\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 32\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.626386 KL: 0.741986 Neg-log -0.115600  [    0/30000]\n",
      "current batch loss: 0.377950 KL: 0.741055 Neg-log -0.363105  [12800/30000]\n",
      "current batch loss: 0.364165 KL: 0.740158 Neg-log -0.375993  [25600/30000]\n",
      "avg train loss per batch in training: 0.590986\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.552007 KL: 0.739858 Neg-log -0.187853 MSE 0.377300500869751\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.536959 KL: 0.739858 Neg-log -0.202901 MSE 0.3657417297363281\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 33\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.275871 KL: 0.739860 Neg-log -0.463990  [    0/30000]\n",
      "current batch loss: 0.331220 KL: 0.739011 Neg-log -0.407791  [12800/30000]\n",
      "current batch loss: 0.389566 KL: 0.738151 Neg-log -0.348585  [25600/30000]\n",
      "avg train loss per batch in training: 0.611322\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.489156 KL: 0.737853 Neg-log -0.248695 MSE 0.3352208435535431\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.534565 KL: 0.737853 Neg-log -0.203285 MSE 0.3717695474624634\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 34\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.546912 KL: 0.737851 Neg-log -0.190938  [    0/30000]\n",
      "current batch loss: 0.695655 KL: 0.737004 Neg-log -0.041349  [12800/30000]\n",
      "current batch loss: 0.442773 KL: 0.736152 Neg-log -0.293379  [25600/30000]\n",
      "avg train loss per batch in training: 0.667292\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.585093 KL: 0.735850 Neg-log -0.150755 MSE 0.4293121099472046\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.681112 KL: 0.735850 Neg-log -0.054737 MSE 0.5081257224082947\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 35\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.032176 KL: 0.735848 Neg-log 0.296327  [    0/30000]\n",
      "current batch loss: 0.790990 KL: 0.734998 Neg-log 0.055992  [12800/30000]\n",
      "current batch loss: 0.371591 KL: 0.734143 Neg-log -0.362552  [25600/30000]\n",
      "avg train loss per batch in training: 0.599428\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.597405 KL: 0.733841 Neg-log -0.136436 MSE 0.4092305600643158\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.587088 KL: 0.733841 Neg-log -0.146753 MSE 0.3924054801464081\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 36\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.349586 KL: 0.733841 Neg-log -0.384255  [    0/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.424978 KL: 0.732969 Neg-log -0.307991  [12800/30000]\n",
      "current batch loss: 0.329804 KL: 0.732100 Neg-log -0.402296  [25600/30000]\n",
      "avg train loss per batch in training: 0.560289\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.438673 KL: 0.731802 Neg-log -0.293129 MSE 0.3056888282299042\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.461772 KL: 0.731802 Neg-log -0.270030 MSE 0.31292036175727844\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 37\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.764642 KL: 0.731802 Neg-log 0.032839  [    0/30000]\n",
      "current batch loss: 0.462128 KL: 0.730925 Neg-log -0.268797  [12800/30000]\n",
      "current batch loss: 0.314425 KL: 0.730051 Neg-log -0.415626  [25600/30000]\n",
      "avg train loss per batch in training: 0.537434\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.467092 KL: 0.729742 Neg-log -0.262650 MSE 0.31232041120529175\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.486275 KL: 0.729742 Neg-log -0.243468 MSE 0.33036279678344727\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 38\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.903583 KL: 0.729743 Neg-log 0.173840  [    0/30000]\n",
      "current batch loss: 0.090484 KL: 0.728892 Neg-log -0.638408  [12800/30000]\n",
      "current batch loss: 0.272003 KL: 0.728058 Neg-log -0.456056  [25600/30000]\n",
      "avg train loss per batch in training: 0.499536\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.538123 KL: 0.727767 Neg-log -0.189643 MSE 0.3244549632072449\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.596036 KL: 0.727767 Neg-log -0.131729 MSE 0.38410913944244385\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 39\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.554165 KL: 0.727765 Neg-log -0.173600  [    0/30000]\n",
      "current batch loss: 0.621973 KL: 0.726915 Neg-log -0.104943  [12800/30000]\n",
      "current batch loss: 0.153041 KL: 0.726064 Neg-log -0.573023  [25600/30000]\n",
      "avg train loss per batch in training: 0.490509\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.400438 KL: 0.725769 Neg-log -0.325330 MSE 0.28611722588539124\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.361282 KL: 0.725769 Neg-log -0.364485 MSE 0.2684025764465332\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 40\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.701015 KL: 0.725768 Neg-log -0.024753  [    0/30000]\n",
      "current batch loss: 0.188953 KL: 0.724942 Neg-log -0.535989  [12800/30000]\n",
      "current batch loss: 0.477006 KL: 0.724127 Neg-log -0.247121  [25600/30000]\n",
      "avg train loss per batch in training: 0.461792\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.511775 KL: 0.723851 Neg-log -0.212077 MSE 0.37794989347457886\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.558892 KL: 0.723851 Neg-log -0.164961 MSE 0.41096627712249756\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 41\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.322180 KL: 0.723852 Neg-log 0.598328  [    0/30000]\n",
      "current batch loss: 0.092430 KL: 0.723054 Neg-log -0.630624  [12800/30000]\n",
      "current batch loss: 0.551329 KL: 0.722265 Neg-log -0.170937  [25600/30000]\n",
      "avg train loss per batch in training: 0.499190\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.360035 KL: 0.721985 Neg-log -0.361952 MSE 0.2717023491859436\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.359193 KL: 0.721985 Neg-log -0.362793 MSE 0.2632080614566803\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 42\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.170405 KL: 0.721987 Neg-log -0.551581  [    0/30000]\n",
      "current batch loss: 0.438649 KL: 0.721208 Neg-log -0.282559  [12800/30000]\n",
      "current batch loss: 0.729027 KL: 0.720424 Neg-log 0.008603  [25600/30000]\n",
      "avg train loss per batch in training: 0.458225\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.330875 KL: 0.720144 Neg-log -0.389271 MSE 0.24578940868377686\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.335427 KL: 0.720144 Neg-log -0.384719 MSE 0.25285589694976807\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 43\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.390815 KL: 0.720146 Neg-log -0.329331  [    0/30000]\n",
      "current batch loss: 0.371087 KL: 0.719375 Neg-log -0.348288  [12800/30000]\n",
      "current batch loss: 0.887428 KL: 0.718581 Neg-log 0.168846  [25600/30000]\n",
      "avg train loss per batch in training: 0.474321\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.741825 KL: 0.718317 Neg-log 0.023509 MSE 0.499894917011261\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.720706 KL: 0.718317 Neg-log 0.002391 MSE 0.4948940575122833\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 44\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.612682 KL: 0.718316 Neg-log -0.105634  [    0/30000]\n",
      "current batch loss: 0.038642 KL: 0.717545 Neg-log -0.678904  [12800/30000]\n",
      "current batch loss: 0.710382 KL: 0.716789 Neg-log -0.006407  [25600/30000]\n",
      "avg train loss per batch in training: 0.456445\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.363145 KL: 0.716523 Neg-log -0.353379 MSE 0.25895369052886963\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.361498 KL: 0.716523 Neg-log -0.355026 MSE 0.25505736470222473\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 45\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.122025 KL: 0.716524 Neg-log -0.594500  [    0/30000]\n",
      "current batch loss: 0.468944 KL: 0.715762 Neg-log -0.246817  [12800/30000]\n",
      "current batch loss: 0.107619 KL: 0.714980 Neg-log -0.607361  [25600/30000]\n",
      "avg train loss per batch in training: 0.453638\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.437271 KL: 0.714708 Neg-log -0.277439 MSE 0.29329681396484375\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.384455 KL: 0.714708 Neg-log -0.330255 MSE 0.26453229784965515\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 46\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.371483 KL: 0.714710 Neg-log -0.343226  [    0/30000]\n",
      "current batch loss: 0.572675 KL: 0.713921 Neg-log -0.141245  [12800/30000]\n",
      "current batch loss: 0.321174 KL: 0.713129 Neg-log -0.391955  [25600/30000]\n",
      "avg train loss per batch in training: 0.401235\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.415766 KL: 0.712850 Neg-log -0.297085 MSE 0.3056752681732178\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.368238 KL: 0.712850 Neg-log -0.344613 MSE 0.27863794565200806\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 47\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.392148 KL: 0.712851 Neg-log -0.320703  [    0/30000]\n",
      "current batch loss: 0.244505 KL: 0.712058 Neg-log -0.467553  [12800/30000]\n",
      "current batch loss: 0.350204 KL: 0.711282 Neg-log -0.361078  [25600/30000]\n",
      "avg train loss per batch in training: 0.423218\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.328333 KL: 0.711008 Neg-log -0.382674 MSE 0.2575656771659851\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.288915 KL: 0.711008 Neg-log -0.422092 MSE 0.23007184267044067\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 48\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.148521 KL: 0.711007 Neg-log -0.562486  [    0/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.184444 KL: 0.710210 Neg-log -0.525766  [12800/30000]\n",
      "current batch loss: 0.113640 KL: 0.709427 Neg-log -0.595787  [25600/30000]\n",
      "avg train loss per batch in training: 0.317502\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.235497 KL: 0.709154 Neg-log -0.473659 MSE 0.20335382223129272\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.265473 KL: 0.709154 Neg-log -0.443683 MSE 0.2183050811290741\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 49\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.179960 KL: 0.709156 Neg-log 0.470804  [    0/30000]\n",
      "current batch loss: 0.492767 KL: 0.708381 Neg-log -0.215615  [12800/30000]\n",
      "current batch loss: 0.136197 KL: 0.707611 Neg-log -0.571414  [25600/30000]\n",
      "avg train loss per batch in training: 0.340100\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.386427 KL: 0.707356 Neg-log -0.320929 MSE 0.26380398869514465\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.399263 KL: 0.707356 Neg-log -0.308093 MSE 0.2821349799633026\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 50\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.019028 KL: 0.707356 Neg-log -0.688328  [    0/30000]\n",
      "current batch loss: 0.205616 KL: 0.706630 Neg-log -0.501014  [12800/30000]\n",
      "current batch loss: -0.067509 KL: 0.705896 Neg-log -0.773405  [25600/30000]\n",
      "avg train loss per batch in training: 0.324225\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.252361 KL: 0.705644 Neg-log -0.453284 MSE 0.21324531733989716\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.288031 KL: 0.705644 Neg-log -0.417613 MSE 0.23222482204437256\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 51\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.382074 KL: 0.705645 Neg-log -0.323571  [    0/30000]\n",
      "current batch loss: 0.037856 KL: 0.704934 Neg-log -0.667077  [12800/30000]\n",
      "current batch loss: 0.885482 KL: 0.704218 Neg-log 0.181264  [25600/30000]\n",
      "avg train loss per batch in training: 0.317237\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.682600 KL: 0.703965 Neg-log -0.021366 MSE 0.3603874146938324\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.666147 KL: 0.703965 Neg-log -0.037819 MSE 0.3611283600330353\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 52\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.667499 KL: 0.703966 Neg-log -0.036467  [    0/30000]\n",
      "current batch loss: 0.362418 KL: 0.703250 Neg-log -0.340832  [12800/30000]\n",
      "current batch loss: 0.184181 KL: 0.702530 Neg-log -0.518349  [25600/30000]\n",
      "avg train loss per batch in training: 0.415341\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.452044 KL: 0.702285 Neg-log -0.250241 MSE 0.26070305705070496\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.429153 KL: 0.702285 Neg-log -0.273132 MSE 0.24014271795749664\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 53\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.406170 KL: 0.702285 Neg-log -0.296115  [    0/30000]\n",
      "current batch loss: -0.058948 KL: 0.701539 Neg-log -0.760487  [12800/30000]\n",
      "current batch loss: -0.004663 KL: 0.700797 Neg-log -0.705460  [25600/30000]\n",
      "avg train loss per batch in training: 0.313072\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.268470 KL: 0.700552 Neg-log -0.432082 MSE 0.2054918259382248\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.262220 KL: 0.700552 Neg-log -0.438332 MSE 0.19532987475395203\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 54\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.294246 KL: 0.700552 Neg-log -0.406306  [    0/30000]\n",
      "current batch loss: 0.904189 KL: 0.699817 Neg-log 0.204372  [12800/30000]\n",
      "current batch loss: 0.146229 KL: 0.699075 Neg-log -0.552846  [25600/30000]\n",
      "avg train loss per batch in training: 0.343265\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.195935 KL: 0.698823 Neg-log -0.502890 MSE 0.2070772498846054\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.209793 KL: 0.698823 Neg-log -0.489032 MSE 0.20531976222991943\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 55\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.283604 KL: 0.698825 Neg-log -0.415221  [    0/30000]\n",
      "current batch loss: 0.096129 KL: 0.698101 Neg-log -0.601972  [12800/30000]\n",
      "current batch loss: 0.010678 KL: 0.697394 Neg-log -0.686716  [25600/30000]\n",
      "avg train loss per batch in training: 0.240029\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.198344 KL: 0.697139 Neg-log -0.498794 MSE 0.199269637465477\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.176417 KL: 0.697139 Neg-log -0.520721 MSE 0.195698544383049\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 56\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.625854 KL: 0.697138 Neg-log -0.071284  [    0/30000]\n",
      "current batch loss: -0.034708 KL: 0.696411 Neg-log -0.731119  [12800/30000]\n",
      "current batch loss: 0.020970 KL: 0.695675 Neg-log -0.674705  [25600/30000]\n",
      "avg train loss per batch in training: 0.311845\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.190385 KL: 0.695419 Neg-log -0.505033 MSE 0.1923353523015976\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.214901 KL: 0.695419 Neg-log -0.480517 MSE 0.20720697939395905\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 57\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.015394 KL: 0.695418 Neg-log -0.710812  [    0/30000]\n",
      "current batch loss: -0.000774 KL: 0.694706 Neg-log -0.695480  [12800/30000]\n",
      "current batch loss: 0.232987 KL: 0.694002 Neg-log -0.461016  [25600/30000]\n",
      "avg train loss per batch in training: 0.241400\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.167707 KL: 0.693757 Neg-log -0.526051 MSE 0.1834215670824051\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.223494 KL: 0.693757 Neg-log -0.470265 MSE 0.21096228063106537\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 58\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.167966 KL: 0.693759 Neg-log -0.525793  [    0/30000]\n",
      "current batch loss: 0.757641 KL: 0.693071 Neg-log 0.064570  [12800/30000]\n",
      "current batch loss: -0.080243 KL: 0.692374 Neg-log -0.772617  [25600/30000]\n",
      "avg train loss per batch in training: 0.219527\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.156180 KL: 0.692134 Neg-log -0.535954 MSE 0.18275922536849976\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.164162 KL: 0.692134 Neg-log -0.527973 MSE 0.1897745281457901\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 59\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.106322 KL: 0.692135 Neg-log -0.798456  [    0/30000]\n",
      "current batch loss: 0.256552 KL: 0.691444 Neg-log -0.434892  [12800/30000]\n",
      "current batch loss: 0.390690 KL: 0.690778 Neg-log -0.300088  [25600/30000]\n",
      "avg train loss per batch in training: 0.190894\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.253486 KL: 0.690538 Neg-log -0.437054 MSE 0.22252929210662842\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.290531 KL: 0.690538 Neg-log -0.400009 MSE 0.22616243362426758\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 60\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.029805 KL: 0.690540 Neg-log -0.660735  [    0/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.080303 KL: 0.689848 Neg-log -0.609544  [12800/30000]\n",
      "current batch loss: 0.165904 KL: 0.689153 Neg-log -0.523249  [25600/30000]\n",
      "avg train loss per batch in training: 0.261409\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.150375 KL: 0.688913 Neg-log -0.538538 MSE 0.1902751624584198\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.124913 KL: 0.688913 Neg-log -0.563999 MSE 0.1801215410232544\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 61\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.778314 KL: 0.688913 Neg-log 0.089402  [    0/30000]\n",
      "current batch loss: -0.003189 KL: 0.688224 Neg-log -0.691413  [12800/30000]\n",
      "current batch loss: -0.166669 KL: 0.687552 Neg-log -0.854221  [25600/30000]\n",
      "avg train loss per batch in training: 0.207345\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.098873 KL: 0.687323 Neg-log -0.588451 MSE 0.1649813950061798\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.103100 KL: 0.687323 Neg-log -0.584224 MSE 0.16605143249034882\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 62\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.104760 KL: 0.687324 Neg-log -0.792084  [    0/30000]\n",
      "current batch loss: 0.551598 KL: 0.686659 Neg-log -0.135060  [12800/30000]\n",
      "current batch loss: 0.345440 KL: 0.686001 Neg-log -0.340560  [25600/30000]\n",
      "avg train loss per batch in training: 0.221021\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.159572 KL: 0.685771 Neg-log -0.526199 MSE 0.18813565373420715\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.159798 KL: 0.685771 Neg-log -0.525974 MSE 0.18650418519973755\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 63\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.012233 KL: 0.685772 Neg-log -0.698005  [    0/30000]\n",
      "current batch loss: 4.029921 KL: 0.685122 Neg-log 3.344798  [12800/30000]\n",
      "current batch loss: -0.126948 KL: 0.684506 Neg-log -0.811454  [25600/30000]\n",
      "avg train loss per batch in training: 0.265258\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.250198 KL: 0.684294 Neg-log -0.434095 MSE 0.21774445474147797\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.231987 KL: 0.684294 Neg-log -0.452307 MSE 0.21159765124320984\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 64\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.417785 KL: 0.684294 Neg-log -0.266509  [    0/30000]\n",
      "current batch loss: 0.344571 KL: 0.683687 Neg-log -0.339116  [12800/30000]\n",
      "current batch loss: -0.022917 KL: 0.683071 Neg-log -0.705989  [25600/30000]\n",
      "avg train loss per batch in training: 0.181877\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.142156 KL: 0.682855 Neg-log -0.540699 MSE 0.1832219362258911\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.176344 KL: 0.682855 Neg-log -0.506510 MSE 0.18570303916931152\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 65\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.024629 KL: 0.682854 Neg-log -0.707483  [    0/30000]\n",
      "current batch loss: 0.124610 KL: 0.682262 Neg-log -0.557652  [12800/30000]\n",
      "current batch loss: 0.714900 KL: 0.681663 Neg-log 0.033238  [25600/30000]\n",
      "avg train loss per batch in training: 0.189960\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.271125 KL: 0.681457 Neg-log -0.410331 MSE 0.22850030660629272\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.200504 KL: 0.681457 Neg-log -0.480952 MSE 0.2270236313343048\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 66\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.140803 KL: 0.681456 Neg-log -0.822260  [    0/30000]\n",
      "current batch loss: 0.357436 KL: 0.680875 Neg-log -0.323438  [12800/30000]\n",
      "current batch loss: 0.455483 KL: 0.680308 Neg-log -0.224824  [25600/30000]\n",
      "avg train loss per batch in training: 0.181025\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.413987 KL: 0.680101 Neg-log -0.266116 MSE 0.2918396294116974\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.386566 KL: 0.680101 Neg-log -0.293537 MSE 0.28556180000305176\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 67\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.061322 KL: 0.680103 Neg-log -0.741425  [    0/30000]\n",
      "current batch loss: 0.389708 KL: 0.679522 Neg-log -0.289814  [12800/30000]\n",
      "current batch loss: -0.169370 KL: 0.678943 Neg-log -0.848313  [25600/30000]\n",
      "avg train loss per batch in training: 0.185407\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.039315 KL: 0.678737 Neg-log -0.639421 MSE 0.1504523903131485\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.077044 KL: 0.678737 Neg-log -0.601692 MSE 0.16443468630313873\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 68\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.114218 KL: 0.678736 Neg-log -0.792954  [    0/30000]\n",
      "current batch loss: 0.625071 KL: 0.678140 Neg-log -0.053069  [12800/30000]\n",
      "current batch loss: -0.019770 KL: 0.677552 Neg-log -0.697322  [25600/30000]\n",
      "avg train loss per batch in training: 0.136160\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.164986 KL: 0.677352 Neg-log -0.512365 MSE 0.1841592639684677\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.125663 KL: 0.677352 Neg-log -0.551688 MSE 0.18531976640224457\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 69\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.194713 KL: 0.677351 Neg-log -0.872064  [    0/30000]\n",
      "current batch loss: 2.140974 KL: 0.676779 Neg-log 1.464194  [12800/30000]\n",
      "current batch loss: 0.036847 KL: 0.676202 Neg-log -0.639355  [25600/30000]\n",
      "avg train loss per batch in training: 0.139411\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.090725 KL: 0.676001 Neg-log -0.585277 MSE 0.16202090680599213\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.058109 KL: 0.676001 Neg-log -0.617893 MSE 0.147073894739151\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 70\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.082284 KL: 0.676002 Neg-log -0.758286  [    0/30000]\n",
      "current batch loss: 0.003229 KL: 0.675427 Neg-log -0.672199  [12800/30000]\n",
      "current batch loss: 0.811173 KL: 0.674851 Neg-log 0.136322  [25600/30000]\n",
      "avg train loss per batch in training: 0.161978\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.017232 KL: 0.674638 Neg-log -0.657407 MSE 0.1454606056213379\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.047111 KL: 0.674638 Neg-log -0.627527 MSE 0.15292835235595703\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 71\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.218869 KL: 0.674639 Neg-log -0.893508  [    0/30000]\n",
      "current batch loss: 0.041014 KL: 0.674068 Neg-log -0.633054  [12800/30000]\n",
      "current batch loss: -0.143521 KL: 0.673492 Neg-log -0.817013  [25600/30000]\n",
      "avg train loss per batch in training: 0.098857\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.008875 KL: 0.673288 Neg-log -0.664412 MSE 0.14519430696964264\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.040697 KL: 0.673288 Neg-log -0.632590 MSE 0.15297310054302216\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 72\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.121088 KL: 0.673287 Neg-log -0.794376  [    0/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.382622 KL: 0.672717 Neg-log -0.290095  [12800/30000]\n",
      "current batch loss: -0.170943 KL: 0.672127 Neg-log -0.843071  [25600/30000]\n",
      "avg train loss per batch in training: 0.171180\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.053793 KL: 0.671915 Neg-log -0.618121 MSE 0.1472889631986618\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.124012 KL: 0.671915 Neg-log -0.547902 MSE 0.16948972642421722\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 73\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.241400 KL: 0.671914 Neg-log -0.430514  [    0/30000]\n",
      "current batch loss: 0.168800 KL: 0.671332 Neg-log -0.502532  [12800/30000]\n",
      "current batch loss: -0.132042 KL: 0.670778 Neg-log -0.802820  [25600/30000]\n",
      "avg train loss per batch in training: 0.059157\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.336029 KL: 0.670572 Neg-log -0.334544 MSE 0.22504186630249023\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.267923 KL: 0.670572 Neg-log -0.402650 MSE 0.20410983264446259\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 74\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.082546 KL: 0.670572 Neg-log -0.753119  [    0/30000]\n",
      "current batch loss: -0.131744 KL: 0.670001 Neg-log -0.801745  [12800/30000]\n",
      "current batch loss: -0.186632 KL: 0.669455 Neg-log -0.856088  [25600/30000]\n",
      "avg train loss per batch in training: 0.034514\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.446545 KL: 0.669261 Neg-log -0.222715 MSE 0.2879798412322998\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.478377 KL: 0.669261 Neg-log -0.190883 MSE 0.2956021726131439\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 75\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.530983 KL: 0.669260 Neg-log -0.138277  [    0/30000]\n",
      "current batch loss: -0.116667 KL: 0.668722 Neg-log -0.785390  [12800/30000]\n",
      "current batch loss: -0.121254 KL: 0.668176 Neg-log -0.789429  [25600/30000]\n",
      "avg train loss per batch in training: 0.091608\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.004394 KL: 0.667979 Neg-log -0.672372 MSE 0.13711030781269073\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.001592 KL: 0.667979 Neg-log -0.666386 MSE 0.1339852660894394\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 76\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.151543 KL: 0.667979 Neg-log -0.819521  [    0/30000]\n",
      "current batch loss: 0.563112 KL: 0.667437 Neg-log -0.104325  [12800/30000]\n",
      "current batch loss: 0.389975 KL: 0.666904 Neg-log -0.276930  [25600/30000]\n",
      "avg train loss per batch in training: 0.055026\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.266179 KL: 0.666731 Neg-log -0.400550 MSE 0.21016578376293182\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.201814 KL: 0.666731 Neg-log -0.464915 MSE 0.19102564454078674\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 77\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.717435 KL: 0.666729 Neg-log 0.050706  [    0/30000]\n",
      "current batch loss: -0.183229 KL: 0.666234 Neg-log -0.849463  [12800/30000]\n",
      "current batch loss: 0.115565 KL: 0.665765 Neg-log -0.550200  [25600/30000]\n",
      "avg train loss per batch in training: 0.089828\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.948287 KL: 0.665593 Neg-log 0.282694 MSE 0.38367515802383423\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.915190 KL: 0.665593 Neg-log 0.249596 MSE 0.374284565448761\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 78\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.197803 KL: 0.665593 Neg-log -0.467790  [    0/30000]\n",
      "current batch loss: -0.215418 KL: 0.665104 Neg-log -0.880522  [12800/30000]\n",
      "current batch loss: 0.275287 KL: 0.664633 Neg-log -0.389346  [25600/30000]\n",
      "avg train loss per batch in training: 0.070427\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.029286 KL: 0.664465 Neg-log -0.635180 MSE 0.1513342261314392\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.019003 KL: 0.664465 Neg-log -0.645463 MSE 0.14627236127853394\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 79\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.202777 KL: 0.664466 Neg-log -0.867243  [    0/30000]\n",
      "current batch loss: -0.187426 KL: 0.663986 Neg-log -0.851412  [12800/30000]\n",
      "current batch loss: -0.357288 KL: 0.663512 Neg-log -1.020800  [25600/30000]\n",
      "avg train loss per batch in training: -0.000408\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.168301 KL: 0.663341 Neg-log -0.495040 MSE 0.18423637747764587\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.142214 KL: 0.663341 Neg-log -0.521127 MSE 0.17355448007583618\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 80\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.269994 KL: 0.663341 Neg-log -0.933335  [    0/30000]\n",
      "current batch loss: -0.072092 KL: 0.662861 Neg-log -0.734953  [12800/30000]\n",
      "current batch loss: -0.319740 KL: 0.662387 Neg-log -0.982127  [25600/30000]\n",
      "avg train loss per batch in training: 0.053877\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.215240 KL: 0.662226 Neg-log -0.446986 MSE 0.21664339303970337\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.178692 KL: 0.662226 Neg-log -0.483533 MSE 0.20990829169750214\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 81\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.111728 KL: 0.662226 Neg-log -0.773953  [    0/30000]\n",
      "current batch loss: 0.229047 KL: 0.661784 Neg-log -0.432737  [12800/30000]\n",
      "current batch loss: 0.133469 KL: 0.661342 Neg-log -0.527873  [25600/30000]\n",
      "avg train loss per batch in training: 0.035131\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.015017 KL: 0.661180 Neg-log -0.646165 MSE 0.15834686160087585\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.012463 KL: 0.661180 Neg-log -0.648719 MSE 0.1604405790567398\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 82\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.494013 KL: 0.661182 Neg-log -0.167169  [    0/30000]\n",
      "current batch loss: -0.260419 KL: 0.660746 Neg-log -0.921165  [12800/30000]\n",
      "current batch loss: -0.253499 KL: 0.660284 Neg-log -0.913783  [25600/30000]\n",
      "avg train loss per batch in training: 0.084467\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.263178 KL: 0.660132 Neg-log -0.396954 MSE 0.19461271166801453\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.181595 KL: 0.660132 Neg-log -0.478538 MSE 0.17421433329582214\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 83\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.269113 KL: 0.660132 Neg-log -0.391019  [    0/30000]\n",
      "current batch loss: -0.076931 KL: 0.659683 Neg-log -0.736614  [12800/30000]\n",
      "current batch loss: -0.190048 KL: 0.659252 Neg-log -0.849299  [25600/30000]\n",
      "avg train loss per batch in training: 0.016285\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.049472 KL: 0.659094 Neg-log -0.609623 MSE 0.15387217700481415\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.079609 KL: 0.659094 Neg-log -0.579486 MSE 0.161854550242424\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 84\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.380095 KL: 0.659096 Neg-log -1.039191  [    0/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: -0.388063 KL: 0.658641 Neg-log -1.046704  [12800/30000]\n",
      "current batch loss: 0.551653 KL: 0.658202 Neg-log -0.106549  [25600/30000]\n",
      "avg train loss per batch in training: 0.052424\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.499898 KL: 0.658051 Neg-log -0.158154 MSE 0.3045263886451721\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.470036 KL: 0.658051 Neg-log -0.188016 MSE 0.28645190596580505\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 85\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.524472 KL: 0.658052 Neg-log -0.133580  [    0/30000]\n",
      "current batch loss: -0.127619 KL: 0.657602 Neg-log -0.785221  [12800/30000]\n",
      "current batch loss: -0.050494 KL: 0.657153 Neg-log -0.707647  [25600/30000]\n",
      "avg train loss per batch in training: 0.035040\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.105199 KL: 0.656992 Neg-log -0.762191 MSE 0.11747510731220245\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.110763 KL: 0.656992 Neg-log -0.767754 MSE 0.11975482106208801\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 86\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.096787 KL: 0.656992 Neg-log -0.753779  [    0/30000]\n",
      "current batch loss: -0.205909 KL: 0.656548 Neg-log -0.862458  [12800/30000]\n",
      "current batch loss: 0.007347 KL: 0.656111 Neg-log -0.648765  [25600/30000]\n",
      "avg train loss per batch in training: -0.009095\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.075894 KL: 0.655959 Neg-log -0.731850 MSE 0.12032943964004517\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.113699 KL: 0.655959 Neg-log -0.769656 MSE 0.12139847874641418\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 87\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.399554 KL: 0.655957 Neg-log -0.256403  [    0/30000]\n",
      "current batch loss: -0.077088 KL: 0.655519 Neg-log -0.732607  [12800/30000]\n",
      "current batch loss: -0.375139 KL: 0.655081 Neg-log -1.030219  [25600/30000]\n",
      "avg train loss per batch in training: -0.013377\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.103179 KL: 0.654924 Neg-log -0.758104 MSE 0.13231605291366577\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.101020 KL: 0.654924 Neg-log -0.755945 MSE 0.1259048879146576\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 88\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.444213 KL: 0.654925 Neg-log -1.099138  [    0/30000]\n",
      "current batch loss: -0.247918 KL: 0.654491 Neg-log -0.902408  [12800/30000]\n",
      "current batch loss: -0.057472 KL: 0.654074 Neg-log -0.711546  [25600/30000]\n",
      "avg train loss per batch in training: 0.004721\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.154842 KL: 0.653917 Neg-log -0.499077 MSE 0.1889636069536209\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.129956 KL: 0.653917 Neg-log -0.523963 MSE 0.17723582684993744\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 89\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.157032 KL: 0.653919 Neg-log -0.810952  [    0/30000]\n",
      "current batch loss: 0.507776 KL: 0.653502 Neg-log -0.145726  [12800/30000]\n",
      "current batch loss: 0.550338 KL: 0.653078 Neg-log -0.102740  [25600/30000]\n",
      "avg train loss per batch in training: 0.013998\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.765693 KL: 0.652928 Neg-log 0.112766 MSE 0.3137976825237274\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.655271 KL: 0.652928 Neg-log 0.002343 MSE 0.28537803888320923\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 90\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.150655 KL: 0.652928 Neg-log -0.502272  [    0/30000]\n",
      "current batch loss: -0.243184 KL: 0.652494 Neg-log -0.895678  [12800/30000]\n",
      "current batch loss: -0.221682 KL: 0.652042 Neg-log -0.873724  [25600/30000]\n",
      "avg train loss per batch in training: -0.019171\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.174550 KL: 0.651891 Neg-log -0.826440 MSE 0.10783395171165466\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.156679 KL: 0.651891 Neg-log -0.808570 MSE 0.10695576667785645\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 91\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.382692 KL: 0.651891 Neg-log -1.034582  [    0/30000]\n",
      "current batch loss: -0.239707 KL: 0.651470 Neg-log -0.891177  [12800/30000]\n",
      "current batch loss: -0.369588 KL: 0.651044 Neg-log -1.020633  [25600/30000]\n",
      "avg train loss per batch in training: -0.061823\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.154715 KL: 0.650893 Neg-log -0.805606 MSE 0.10623878985643387\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.166540 KL: 0.650893 Neg-log -0.817431 MSE 0.1072821244597435\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 92\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.344526 KL: 0.650891 Neg-log -0.995417  [    0/30000]\n",
      "current batch loss: -0.171683 KL: 0.650469 Neg-log -0.822152  [12800/30000]\n",
      "current batch loss: -0.336264 KL: 0.650047 Neg-log -0.986311  [25600/30000]\n",
      "avg train loss per batch in training: -0.062713\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.178852 KL: 0.649889 Neg-log -0.828743 MSE 0.10588907450437546\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.183554 KL: 0.649889 Neg-log -0.833445 MSE 0.10439733415842056\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 93\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.207689 KL: 0.649891 Neg-log -0.857579  [    0/30000]\n",
      "current batch loss: -0.295891 KL: 0.649476 Neg-log -0.945367  [12800/30000]\n",
      "current batch loss: 0.652748 KL: 0.649049 Neg-log 0.003699  [25600/30000]\n",
      "avg train loss per batch in training: -0.051854\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.126452 KL: 0.648895 Neg-log -0.775348 MSE 0.11872628331184387\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.096848 KL: 0.648895 Neg-log -0.745743 MSE 0.12997281551361084\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 94\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.469869 KL: 0.648895 Neg-log -1.118764  [    0/30000]\n",
      "current batch loss: -0.213025 KL: 0.648482 Neg-log -0.861506  [12800/30000]\n",
      "current batch loss: -0.039503 KL: 0.648057 Neg-log -0.687560  [25600/30000]\n",
      "avg train loss per batch in training: -0.075727\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.032342 KL: 0.647913 Neg-log -0.615570 MSE 0.13813355565071106\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.009510 KL: 0.647913 Neg-log -0.638403 MSE 0.13104067742824554\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 95\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.257460 KL: 0.647912 Neg-log -0.390452  [    0/30000]\n",
      "current batch loss: -0.464767 KL: 0.647493 Neg-log -1.112261  [12800/30000]\n",
      "current batch loss: -0.311526 KL: 0.647083 Neg-log -0.958609  [25600/30000]\n",
      "avg train loss per batch in training: -0.098958\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.176373 KL: 0.646926 Neg-log -0.823298 MSE 0.10033775120973587\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.178559 KL: 0.646926 Neg-log -0.825483 MSE 0.10204675048589706\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 96\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.055102 KL: 0.646924 Neg-log -0.591823  [    0/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: -0.277197 KL: 0.646495 Neg-log -0.923692  [12800/30000]\n",
      "current batch loss: -0.138528 KL: 0.646085 Neg-log -0.784613  [25600/30000]\n",
      "avg train loss per batch in training: -0.093766\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.059605 KL: 0.645940 Neg-log -0.705545 MSE 0.12045171856880188\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.046842 KL: 0.645940 Neg-log -0.692782 MSE 0.1254294067621231\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 97\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.362508 KL: 0.645939 Neg-log -1.008448  [    0/30000]\n",
      "current batch loss: -0.385916 KL: 0.645513 Neg-log -1.031428  [12800/30000]\n",
      "current batch loss: -0.393457 KL: 0.645102 Neg-log -1.038559  [25600/30000]\n",
      "avg train loss per batch in training: -0.121834\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.072376 KL: 0.644958 Neg-log -0.717333 MSE 0.1258702129125595\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.043455 KL: 0.644958 Neg-log -0.688412 MSE 0.1348232924938202\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 98\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.088344 KL: 0.644957 Neg-log -0.733301  [    0/30000]\n",
      "current batch loss: -0.408745 KL: 0.644520 Neg-log -1.053265  [12800/30000]\n",
      "current batch loss: 0.671781 KL: 0.644084 Neg-log 0.027697  [25600/30000]\n",
      "avg train loss per batch in training: -0.078934\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.173494 KL: 0.643942 Neg-log -0.817437 MSE 0.10315515846014023\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.198982 KL: 0.643942 Neg-log -0.842924 MSE 0.09733348339796066\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 99\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.368967 KL: 0.643943 Neg-log -1.012909  [    0/30000]\n",
      "current batch loss: 0.130064 KL: 0.643523 Neg-log -0.513459  [12800/30000]\n",
      "current batch loss: -0.385466 KL: 0.643083 Neg-log -1.028549  [25600/30000]\n",
      "avg train loss per batch in training: -0.110615\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.175844 KL: 0.642929 Neg-log -0.818774 MSE 0.10161968320608139\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.137697 KL: 0.642929 Neg-log -0.780627 MSE 0.10824434459209442\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 100\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.133888 KL: 0.642931 Neg-log -0.776818  [    0/30000]\n",
      "current batch loss: 1.083394 KL: 0.642507 Neg-log 0.440886  [12800/30000]\n",
      "current batch loss: -0.286579 KL: 0.642057 Neg-log -0.928636  [25600/30000]\n",
      "avg train loss per batch in training: -0.022867\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.067305 KL: 0.641897 Neg-log -0.709202 MSE 0.12920960783958435\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.066874 KL: 0.641897 Neg-log -0.708771 MSE 0.1307723969221115\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 101\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.335958 KL: 0.641897 Neg-log -0.977855  [    0/30000]\n",
      "current batch loss: 0.559047 KL: 0.641464 Neg-log -0.082417  [12800/30000]\n",
      "current batch loss: -0.183636 KL: 0.641029 Neg-log -0.824665  [25600/30000]\n",
      "avg train loss per batch in training: -0.094243\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.137443 KL: 0.640879 Neg-log -0.503436 MSE 0.14159415662288666\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.070226 KL: 0.640879 Neg-log -0.711105 MSE 0.12906694412231445\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 102\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.260914 KL: 0.640879 Neg-log -0.901793  [    0/30000]\n",
      "current batch loss: 0.199923 KL: 0.640444 Neg-log -0.440521  [12800/30000]\n",
      "current batch loss: -0.068178 KL: 0.640013 Neg-log -0.708190  [25600/30000]\n",
      "avg train loss per batch in training: -0.112295\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.001862 KL: 0.639856 Neg-log -0.637994 MSE 0.13929569721221924\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.022571 KL: 0.639856 Neg-log -0.617285 MSE 0.1465095430612564\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 103\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.339984 KL: 0.639856 Neg-log -0.299872  [    0/30000]\n",
      "current batch loss: -0.314385 KL: 0.639408 Neg-log -0.953793  [12800/30000]\n",
      "current batch loss: -0.518797 KL: 0.638980 Neg-log -1.157777  [25600/30000]\n",
      "avg train loss per batch in training: -0.176174\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.254814 KL: 0.638840 Neg-log -0.893656 MSE 0.08894605934619904\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.250354 KL: 0.638840 Neg-log -0.889195 MSE 0.09007064998149872\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 104\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.444252 KL: 0.638841 Neg-log -1.083093  [    0/30000]\n",
      "current batch loss: -0.117475 KL: 0.638428 Neg-log -0.755902  [12800/30000]\n",
      "current batch loss: -0.293250 KL: 0.638013 Neg-log -0.931263  [25600/30000]\n",
      "avg train loss per batch in training: -0.198169\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.225271 KL: 0.637877 Neg-log -0.863147 MSE 0.0993322804570198\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.223316 KL: 0.637877 Neg-log -0.861192 MSE 0.10165370255708694\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 105\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.428905 KL: 0.637876 Neg-log -1.066781  [    0/30000]\n",
      "current batch loss: -0.371380 KL: 0.637485 Neg-log -1.008865  [12800/30000]\n",
      "current batch loss: -0.234031 KL: 0.637086 Neg-log -0.871118  [25600/30000]\n",
      "avg train loss per batch in training: -0.076257\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.212096 KL: 0.636953 Neg-log -0.849049 MSE 0.0911564975976944\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.167109 KL: 0.636953 Neg-log -0.804062 MSE 0.10171326994895935\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 106\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.320662 KL: 0.636953 Neg-log -0.957615  [    0/30000]\n",
      "current batch loss: -0.081454 KL: 0.636526 Neg-log -0.717979  [12800/30000]\n",
      "current batch loss: -0.329071 KL: 0.636138 Neg-log -0.965209  [25600/30000]\n",
      "avg train loss per batch in training: -0.085695\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.189219 KL: 0.636008 Neg-log -0.825227 MSE 0.09681659191846848\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.160247 KL: 0.636008 Neg-log -0.796255 MSE 0.10512834787368774\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 107\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.341379 KL: 0.636008 Neg-log -0.977387  [    0/30000]\n",
      "current batch loss: -0.254109 KL: 0.635639 Neg-log -0.889748  [12800/30000]\n",
      "current batch loss: -0.355757 KL: 0.635262 Neg-log -0.991020  [25600/30000]\n",
      "avg train loss per batch in training: -0.128444\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.210272 KL: 0.635137 Neg-log -0.845409 MSE 0.10281569510698318\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.213853 KL: 0.635137 Neg-log -0.848991 MSE 0.1031450629234314\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 108\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.030905 KL: 0.635137 Neg-log -0.604232  [    0/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: -0.377302 KL: 0.634769 Neg-log -1.012072  [12800/30000]\n",
      "current batch loss: 0.535386 KL: 0.634387 Neg-log -0.099000  [25600/30000]\n",
      "avg train loss per batch in training: -0.125965\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.215213 KL: 0.634261 Neg-log -0.849473 MSE 0.0963691771030426\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.224996 KL: 0.634261 Neg-log -0.859255 MSE 0.09927673637866974\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 109\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.237823 KL: 0.634259 Neg-log -0.872082  [    0/30000]\n",
      "current batch loss: -0.194845 KL: 0.633892 Neg-log -0.828737  [12800/30000]\n",
      "current batch loss: 0.102372 KL: 0.633521 Neg-log -0.531148  [25600/30000]\n",
      "avg train loss per batch in training: -0.163984\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.145769 KL: 0.633391 Neg-log -0.779158 MSE 0.09941493719816208\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.148749 KL: 0.633391 Neg-log -0.782138 MSE 0.10197515785694122\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 110\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.411906 KL: 0.633389 Neg-log -0.221484  [    0/30000]\n",
      "current batch loss: -0.204383 KL: 0.633018 Neg-log -0.837402  [12800/30000]\n",
      "current batch loss: -0.294913 KL: 0.632669 Neg-log -0.927582  [25600/30000]\n",
      "avg train loss per batch in training: -0.167097\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.297771 KL: 0.632538 Neg-log -0.930309 MSE 0.08345042169094086\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.319789 KL: 0.632538 Neg-log -0.952328 MSE 0.0802353024482727\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 111\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.426820 KL: 0.632538 Neg-log -1.059358  [    0/30000]\n",
      "current batch loss: -0.372745 KL: 0.632193 Neg-log -1.004938  [12800/30000]\n",
      "current batch loss: 0.231191 KL: 0.631842 Neg-log -0.400651  [25600/30000]\n",
      "avg train loss per batch in training: -0.220019\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.265033 KL: 0.631712 Neg-log -0.896743 MSE 0.08514546602964401\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.273034 KL: 0.631712 Neg-log -0.904744 MSE 0.08287008851766586\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 112\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.311984 KL: 0.631710 Neg-log -0.943694  [    0/30000]\n",
      "current batch loss: -0.569705 KL: 0.631360 Neg-log -1.201065  [12800/30000]\n",
      "current batch loss: 0.185300 KL: 0.631016 Neg-log -0.445716  [25600/30000]\n",
      "avg train loss per batch in training: -0.230250\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.233547 KL: 0.630896 Neg-log -0.864445 MSE 0.09515038132667542\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.250704 KL: 0.630896 Neg-log -0.881602 MSE 0.09359560906887054\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 113\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.018190 KL: 0.630897 Neg-log -0.612707  [    0/30000]\n",
      "current batch loss: -0.459602 KL: 0.630572 Neg-log -1.090174  [12800/30000]\n",
      "current batch loss: -0.451003 KL: 0.630238 Neg-log -1.081240  [25600/30000]\n",
      "avg train loss per batch in training: -0.217971\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.306199 KL: 0.630125 Neg-log -0.936322 MSE 0.08021791279315948\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.302803 KL: 0.630125 Neg-log -0.932927 MSE 0.08075378090143204\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 114\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.341128 KL: 0.630124 Neg-log -0.971251  [    0/30000]\n",
      "current batch loss: -0.442230 KL: 0.629802 Neg-log -1.072032  [12800/30000]\n",
      "current batch loss: -0.379148 KL: 0.629488 Neg-log -1.008636  [25600/30000]\n",
      "avg train loss per batch in training: -0.284333\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.274558 KL: 0.629379 Neg-log -0.903938 MSE 0.08741556853055954\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.274991 KL: 0.629379 Neg-log -0.904370 MSE 0.0872742235660553\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 115\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.426392 KL: 0.629380 Neg-log -1.055771  [    0/30000]\n",
      "current batch loss: -0.556225 KL: 0.629054 Neg-log -1.185280  [12800/30000]\n",
      "current batch loss: -0.392801 KL: 0.628732 Neg-log -1.021534  [25600/30000]\n",
      "avg train loss per batch in training: -0.236478\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.371014 KL: 0.628633 Neg-log -0.999647 MSE 0.07794591784477234\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.317454 KL: 0.628633 Neg-log -0.946087 MSE 0.08669465035200119\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 116\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.578782 KL: 0.628634 Neg-log -1.207416  [    0/30000]\n",
      "current batch loss: -0.366463 KL: 0.628325 Neg-log -0.994788  [12800/30000]\n",
      "current batch loss: 0.121306 KL: 0.628015 Neg-log -0.506710  [25600/30000]\n",
      "avg train loss per batch in training: -0.182690\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.254911 KL: 0.627900 Neg-log -0.882812 MSE 0.08529175817966461\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.252032 KL: 0.627900 Neg-log -0.879933 MSE 0.09107865393161774\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 117\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.372347 KL: 0.627901 Neg-log -1.000249  [    0/30000]\n",
      "current batch loss: 0.007686 KL: 0.627582 Neg-log -0.619896  [12800/30000]\n",
      "current batch loss: -0.477950 KL: 0.627256 Neg-log -1.105206  [25600/30000]\n",
      "avg train loss per batch in training: -0.181775\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.047328 KL: 0.627150 Neg-log -0.674478 MSE 0.11066872626543045\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.053266 KL: 0.627150 Neg-log -0.680415 MSE 0.11197695881128311\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 118\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.742688 KL: 0.627149 Neg-log 0.115539  [    0/30000]\n",
      "current batch loss: -0.359865 KL: 0.626852 Neg-log -0.986717  [12800/30000]\n",
      "current batch loss: -0.563220 KL: 0.626554 Neg-log -1.189773  [25600/30000]\n",
      "avg train loss per batch in training: -0.270112\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.306222 KL: 0.626465 Neg-log -0.932689 MSE 0.09365061670541763\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.311738 KL: 0.626465 Neg-log -0.938204 MSE 0.08889665454626083\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 119\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.504418 KL: 0.626466 Neg-log -1.130884  [    0/30000]\n",
      "current batch loss: -0.412481 KL: 0.626170 Neg-log -1.038651  [12800/30000]\n",
      "current batch loss: -0.356755 KL: 0.625864 Neg-log -0.982618  [25600/30000]\n",
      "avg train loss per batch in training: -0.212284\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.381600 KL: 0.625763 Neg-log -1.007364 MSE 0.07335545122623444\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.328838 KL: 0.625763 Neg-log -0.954601 MSE 0.08126697689294815\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 120\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.550127 KL: 0.625763 Neg-log -1.175890  [    0/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.841313 KL: 0.625491 Neg-log 0.215823  [12800/30000]\n",
      "current batch loss: -0.379210 KL: 0.625209 Neg-log -1.004418  [25600/30000]\n",
      "avg train loss per batch in training: -0.216137\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.338058 KL: 0.625116 Neg-log -0.963173 MSE 0.07325531542301178\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.319704 KL: 0.625116 Neg-log -0.944818 MSE 0.07870987057685852\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 121\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.062245 KL: 0.625115 Neg-log -0.687360  [    0/30000]\n",
      "current batch loss: -0.195562 KL: 0.624818 Neg-log -0.820380  [12800/30000]\n",
      "current batch loss: -0.527574 KL: 0.624533 Neg-log -1.152107  [25600/30000]\n",
      "avg train loss per batch in training: -0.276522\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.325574 KL: 0.624430 Neg-log -0.950004 MSE 0.08351250737905502\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.362870 KL: 0.624430 Neg-log -0.987300 MSE 0.07594479620456696\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 122\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.133374 KL: 0.624431 Neg-log -0.757804  [    0/30000]\n",
      "current batch loss: -0.159859 KL: 0.624147 Neg-log -0.784006  [12800/30000]\n",
      "current batch loss: -0.492790 KL: 0.623863 Neg-log -1.116653  [25600/30000]\n",
      "avg train loss per batch in training: -0.256364\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.339328 KL: 0.623771 Neg-log -0.963100 MSE 0.07570594549179077\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.334719 KL: 0.623771 Neg-log -0.958490 MSE 0.08001871407032013\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 123\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.540912 KL: 0.623771 Neg-log -1.164684  [    0/30000]\n",
      "current batch loss: 0.256894 KL: 0.623481 Neg-log -0.366587  [12800/30000]\n",
      "current batch loss: -0.680086 KL: 0.623199 Neg-log -1.303285  [25600/30000]\n",
      "avg train loss per batch in training: 748.963453\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.430590 KL: 0.623109 Neg-log -0.192520 MSE 0.3098962604999542\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.431490 KL: 0.623109 Neg-log -0.191620 MSE 0.3248146176338196\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 124\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.465291 KL: 0.623110 Neg-log -0.157820  [    0/30000]\n",
      "current batch loss: -0.136371 KL: 0.622795 Neg-log -0.759166  [12800/30000]\n",
      "current batch loss: -0.365538 KL: 0.622491 Neg-log -0.988029  [25600/30000]\n",
      "avg train loss per batch in training: -0.011707\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.146974 KL: 0.622396 Neg-log -0.769370 MSE 0.12828239798545837\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.163188 KL: 0.622396 Neg-log -0.785584 MSE 0.12489224970340729\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 125\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.393120 KL: 0.622396 Neg-log -1.015516  [    0/30000]\n",
      "current batch loss: -0.524938 KL: 0.622130 Neg-log -1.147068  [12800/30000]\n",
      "current batch loss: -0.192986 KL: 0.621871 Neg-log -0.814857  [25600/30000]\n",
      "avg train loss per batch in training: -0.224530\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.309615 KL: 0.621789 Neg-log -0.931404 MSE 0.09003280848264694\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.241461 KL: 0.621789 Neg-log -0.863250 MSE 0.10314534604549408\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 126\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.090336 KL: 0.621789 Neg-log -0.531453  [    0/30000]\n",
      "current batch loss: -0.029534 KL: 0.621527 Neg-log -0.651060  [12800/30000]\n",
      "current batch loss: -0.525427 KL: 0.621262 Neg-log -1.146689  [25600/30000]\n",
      "avg train loss per batch in training: -0.232671\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.344255 KL: 0.621176 Neg-log -0.965431 MSE 0.08346948027610779\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.341600 KL: 0.621176 Neg-log -0.962776 MSE 0.08352755010128021\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 127\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.129158 KL: 0.621176 Neg-log -0.750334  [    0/30000]\n",
      "current batch loss: -0.277371 KL: 0.620927 Neg-log -0.898298  [12800/30000]\n",
      "current batch loss: -0.518659 KL: 0.620677 Neg-log -1.139335  [25600/30000]\n",
      "avg train loss per batch in training: -0.283953\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.304917 KL: 0.620582 Neg-log -0.925499 MSE 0.08855888247489929\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.347390 KL: 0.620582 Neg-log -0.967973 MSE 0.0831223577260971\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 128\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.165494 KL: 0.620582 Neg-log -0.786076  [    0/30000]\n",
      "current batch loss: -0.455104 KL: 0.620348 Neg-log -1.075453  [12800/30000]\n",
      "current batch loss: -0.472661 KL: 0.620071 Neg-log -1.092732  [25600/30000]\n",
      "avg train loss per batch in training: -0.264548\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.322009 KL: 0.619981 Neg-log -0.941992 MSE 0.08501962572336197\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.329519 KL: 0.619981 Neg-log -0.949502 MSE 0.08670136332511902\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 129\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.510468 KL: 0.619983 Neg-log -1.130451  [    0/30000]\n",
      "current batch loss: -0.491734 KL: 0.619710 Neg-log -1.111444  [12800/30000]\n",
      "current batch loss: 0.633456 KL: 0.619445 Neg-log 0.014010  [25600/30000]\n",
      "avg train loss per batch in training: -0.294059\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.360801 KL: 0.619348 Neg-log -0.980148 MSE 0.08607102185487747\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.337090 KL: 0.619348 Neg-log -0.956437 MSE 0.08465636521577835\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 130\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.632667 KL: 0.619347 Neg-log -1.252014  [    0/30000]\n",
      "current batch loss: -0.522438 KL: 0.619091 Neg-log -1.141529  [12800/30000]\n",
      "current batch loss: -0.064349 KL: 0.618833 Neg-log -0.683182  [25600/30000]\n",
      "avg train loss per batch in training: -0.269259\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.011800 KL: 0.618744 Neg-log -0.630543 MSE 0.13039886951446533\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.102963 KL: 0.618744 Neg-log -0.721706 MSE 0.11518817394971848\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 131\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.449383 KL: 0.618743 Neg-log -1.068125  [    0/30000]\n",
      "current batch loss: -0.579302 KL: 0.618474 Neg-log -1.197775  [12800/30000]\n",
      "current batch loss: -0.396921 KL: 0.618211 Neg-log -1.015132  [25600/30000]\n",
      "avg train loss per batch in training: -0.341813\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.389381 KL: 0.618118 Neg-log -1.007497 MSE 0.07652579993009567\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.392216 KL: 0.618118 Neg-log -1.010332 MSE 0.07781078666448593\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 132\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.534572 KL: 0.618116 Neg-log -1.152689  [    0/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: -0.508798 KL: 0.617863 Neg-log -1.126662  [12800/30000]\n",
      "current batch loss: -0.549060 KL: 0.617592 Neg-log -1.166652  [25600/30000]\n",
      "avg train loss per batch in training: -0.310832\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.321296 KL: 0.617494 Neg-log -0.938791 MSE 0.08835895359516144\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.325852 KL: 0.617494 Neg-log -0.943348 MSE 0.0875357910990715\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 133\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.656668 KL: 0.617496 Neg-log -1.274163  [    0/30000]\n",
      "current batch loss: -0.060609 KL: 0.617223 Neg-log -0.677833  [12800/30000]\n",
      "current batch loss: -0.628304 KL: 0.616959 Neg-log -1.245263  [25600/30000]\n",
      "avg train loss per batch in training: -0.334027\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.164180 KL: 0.616873 Neg-log -0.781053 MSE 0.10688482224941254\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.110988 KL: 0.616873 Neg-log -0.727861 MSE 0.12033729255199432\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 134\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.400640 KL: 0.616873 Neg-log -1.017512  [    0/30000]\n",
      "current batch loss: -0.371184 KL: 0.616612 Neg-log -0.987796  [12800/30000]\n",
      "current batch loss: -0.515530 KL: 0.616339 Neg-log -1.131868  [25600/30000]\n",
      "avg train loss per batch in training: -0.311327\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.444090 KL: 0.616241 Neg-log -1.060331 MSE 0.0717613622546196\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.393213 KL: 0.616241 Neg-log -1.009454 MSE 0.07864232361316681\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 135\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.479177 KL: 0.616241 Neg-log -1.095418  [    0/30000]\n",
      "current batch loss: -0.379945 KL: 0.615967 Neg-log -0.995911  [12800/30000]\n",
      "current batch loss: -0.254515 KL: 0.615700 Neg-log -0.870215  [25600/30000]\n",
      "avg train loss per batch in training: -0.314010\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.363269 KL: 0.615609 Neg-log -0.978879 MSE 0.08178193867206573\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.369091 KL: 0.615609 Neg-log -0.984701 MSE 0.07879230380058289\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 136\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.269158 KL: 0.615610 Neg-log -0.884768  [    0/30000]\n",
      "current batch loss: -0.301447 KL: 0.615341 Neg-log -0.916788  [12800/30000]\n",
      "current batch loss: -0.614642 KL: 0.615074 Neg-log -1.229716  [25600/30000]\n",
      "avg train loss per batch in training: -0.339732\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.350976 KL: 0.614977 Neg-log -0.965955 MSE 0.08566584438085556\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.356451 KL: 0.614977 Neg-log -0.971429 MSE 0.08516550064086914\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 137\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.370849 KL: 0.614978 Neg-log -0.985827  [    0/30000]\n",
      "current batch loss: -0.482512 KL: 0.614717 Neg-log -1.097229  [12800/30000]\n",
      "current batch loss: -0.266487 KL: 0.614459 Neg-log -0.880946  [25600/30000]\n",
      "avg train loss per batch in training: -0.323524\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.400758 KL: 0.614364 Neg-log -1.015122 MSE 0.07909808307886124\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.425168 KL: 0.614364 Neg-log -1.039531 MSE 0.07299835979938507\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 138\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.569747 KL: 0.614364 Neg-log -1.184111  [    0/30000]\n",
      "current batch loss: -0.335792 KL: 0.614107 Neg-log -0.949898  [12800/30000]\n",
      "current batch loss: -0.360697 KL: 0.613843 Neg-log -0.974539  [25600/30000]\n",
      "avg train loss per batch in training: -0.326228\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.390815 KL: 0.613749 Neg-log -1.004565 MSE 0.08039955049753189\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.379780 KL: 0.613749 Neg-log -0.993529 MSE 0.08197139203548431\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 139\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.024927 KL: 0.613749 Neg-log -0.638676  [    0/30000]\n",
      "current batch loss: -0.122866 KL: 0.613492 Neg-log -0.736359  [12800/30000]\n",
      "current batch loss: -0.424717 KL: 0.613250 Neg-log -1.037967  [25600/30000]\n",
      "avg train loss per batch in training: -0.403086\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.409188 KL: 0.613161 Neg-log -1.022350 MSE 0.07797098904848099\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.359554 KL: 0.613161 Neg-log -0.972717 MSE 0.08280640840530396\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 140\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.587798 KL: 0.613163 Neg-log -1.200960  [    0/30000]\n",
      "current batch loss: -0.404749 KL: 0.612927 Neg-log -1.017675  [12800/30000]\n",
      "current batch loss: -0.480918 KL: 0.612681 Neg-log -1.093599  [25600/30000]\n",
      "avg train loss per batch in training: -0.313477\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.399085 KL: 0.612594 Neg-log -1.011678 MSE 0.07465748488903046\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.374651 KL: 0.612594 Neg-log -0.987245 MSE 0.07912280410528183\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 141\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.360717 KL: 0.612593 Neg-log -0.973310  [    0/30000]\n",
      "current batch loss: -0.484645 KL: 0.612349 Neg-log -1.096994  [12800/30000]\n",
      "current batch loss: -0.395330 KL: 0.612099 Neg-log -1.007429  [25600/30000]\n",
      "avg train loss per batch in training: -0.361533\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.103505 KL: 0.612008 Neg-log -0.508503 MSE 0.14002704620361328\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.088875 KL: 0.612008 Neg-log -0.523133 MSE 0.1442471593618393\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 142\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.733217 KL: 0.612008 Neg-log 1.121209  [    0/30000]\n",
      "current batch loss: -0.501277 KL: 0.611757 Neg-log -1.113035  [12800/30000]\n",
      "current batch loss: -0.294256 KL: 0.611508 Neg-log -0.905765  [25600/30000]\n",
      "avg train loss per batch in training: -0.263747\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.273982 KL: 0.611433 Neg-log -0.885415 MSE 0.09078212827444077\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.222427 KL: 0.611433 Neg-log -0.833859 MSE 0.10163699835538864\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 143\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.475726 KL: 0.611432 Neg-log -1.087158  [    0/30000]\n",
      "current batch loss: -0.430679 KL: 0.611183 Neg-log -1.041862  [12800/30000]\n",
      "current batch loss: 0.447078 KL: 0.610938 Neg-log -0.163861  [25600/30000]\n",
      "avg train loss per batch in training: -0.333331\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.386443 KL: 0.610855 Neg-log -0.997298 MSE 0.08106766641139984\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.417766 KL: 0.610855 Neg-log -1.028621 MSE 0.07693155109882355\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 144\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.508158 KL: 0.610855 Neg-log -1.119013  [    0/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: -0.587117 KL: 0.610605 Neg-log -1.197722  [12800/30000]\n",
      "current batch loss: -0.600027 KL: 0.610367 Neg-log -1.210394  [25600/30000]\n",
      "avg train loss per batch in training: -0.408380\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.289097 KL: 0.610291 Neg-log -0.899388 MSE 0.09224072098731995\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.331439 KL: 0.610291 Neg-log -0.941730 MSE 0.09021744132041931\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 145\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.496016 KL: 0.610291 Neg-log -1.106307  [    0/30000]\n",
      "current batch loss: -0.522452 KL: 0.610063 Neg-log -1.132515  [12800/30000]\n",
      "current batch loss: -0.477542 KL: 0.609828 Neg-log -1.087370  [25600/30000]\n",
      "avg train loss per batch in training: -0.312030\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.367059 KL: 0.609741 Neg-log -0.976800 MSE 0.08011386543512344\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.379632 KL: 0.609741 Neg-log -0.989373 MSE 0.07912416756153107\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 146\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.318138 KL: 0.609741 Neg-log -0.927879  [    0/30000]\n",
      "current batch loss: -0.618084 KL: 0.609499 Neg-log -1.227583  [12800/30000]\n",
      "current batch loss: -0.354529 KL: 0.609265 Neg-log -0.963794  [25600/30000]\n",
      "avg train loss per batch in training: -0.376533\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.425922 KL: 0.609185 Neg-log -1.035108 MSE 0.0746937021613121\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.468319 KL: 0.609185 Neg-log -1.077504 MSE 0.06895462423563004\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 147\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.491445 KL: 0.609185 Neg-log -1.100631  [    0/30000]\n",
      "current batch loss: -0.654574 KL: 0.608954 Neg-log -1.263528  [12800/30000]\n",
      "current batch loss: 1.487616 KL: 0.608723 Neg-log 0.878893  [25600/30000]\n",
      "avg train loss per batch in training: -0.298450\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.251014 KL: 0.608643 Neg-log -0.859656 MSE 0.09949760884046555\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.238334 KL: 0.608643 Neg-log -0.846976 MSE 0.10210942476987839\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 148\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.251704 KL: 0.608643 Neg-log -0.860346  [    0/30000]\n",
      "current batch loss: -0.487958 KL: 0.608401 Neg-log -1.096359  [12800/30000]\n",
      "current batch loss: -0.250709 KL: 0.608157 Neg-log -0.858866  [25600/30000]\n",
      "avg train loss per batch in training: -0.365621\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.396764 KL: 0.608078 Neg-log -1.004842 MSE 0.0766678974032402\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.402616 KL: 0.608078 Neg-log -1.010694 MSE 0.07793863117694855\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 149\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.203263 KL: 0.608078 Neg-log -0.404815  [    0/30000]\n",
      "current batch loss: -0.674017 KL: 0.607842 Neg-log -1.281860  [12800/30000]\n",
      "current batch loss: -0.350770 KL: 0.607615 Neg-log -0.958385  [25600/30000]\n",
      "avg train loss per batch in training: -0.366461\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.415503 KL: 0.607537 Neg-log -1.023040 MSE 0.07357271015644073\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.391653 KL: 0.607537 Neg-log -0.999190 MSE 0.07835961133241653\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 150\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.630551 KL: 0.607537 Neg-log -1.238089  [    0/30000]\n",
      "current batch loss: -0.567212 KL: 0.607306 Neg-log -1.174518  [12800/30000]\n",
      "current batch loss: -0.515368 KL: 0.607082 Neg-log -1.122450  [25600/30000]\n",
      "avg train loss per batch in training: -0.363049\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.443457 KL: 0.607005 Neg-log -1.050463 MSE 0.07117697596549988\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.439284 KL: 0.607005 Neg-log -1.046291 MSE 0.07214778661727905\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 151\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.799633 KL: 0.607006 Neg-log -1.406639  [    0/30000]\n",
      "current batch loss: -0.688036 KL: 0.606781 Neg-log -1.294817  [12800/30000]\n",
      "current batch loss: -0.221536 KL: 0.606549 Neg-log -0.828085  [25600/30000]\n",
      "avg train loss per batch in training: -0.293290\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.427559 KL: 0.606467 Neg-log -1.034026 MSE 0.0705939456820488\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.387755 KL: 0.606467 Neg-log -0.994223 MSE 0.07946213334798813\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 152\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.264438 KL: 0.606467 Neg-log -0.870905  [    0/30000]\n",
      "current batch loss: -0.513586 KL: 0.606234 Neg-log -1.119820  [12800/30000]\n",
      "current batch loss: -0.397420 KL: 0.606023 Neg-log -1.003443  [25600/30000]\n",
      "avg train loss per batch in training: -0.364996\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.324122 KL: 0.605955 Neg-log -0.930076 MSE 0.07932727783918381\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.343527 KL: 0.605955 Neg-log -0.949481 MSE 0.07920891791582108\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 153\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.028853 KL: 0.605954 Neg-log -0.577101  [    0/30000]\n",
      "current batch loss: -0.665961 KL: 0.605742 Neg-log -1.271704  [12800/30000]\n",
      "current batch loss: -0.048050 KL: 0.605552 Neg-log -0.653602  [25600/30000]\n",
      "avg train loss per batch in training: -0.396098\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.424586 KL: 0.605479 Neg-log -1.030065 MSE 0.07582293450832367\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.391537 KL: 0.605479 Neg-log -0.997016 MSE 0.08229996263980865\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 154\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.632853 KL: 0.605479 Neg-log -1.238332  [    0/30000]\n",
      "current batch loss: -0.679606 KL: 0.605266 Neg-log -1.284872  [12800/30000]\n",
      "current batch loss: -0.685553 KL: 0.605053 Neg-log -1.290606  [25600/30000]\n",
      "avg train loss per batch in training: -0.421377\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.408261 KL: 0.604979 Neg-log -1.013240 MSE 0.080067940056324\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.345165 KL: 0.604979 Neg-log -0.950144 MSE 0.08385112136602402\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 155\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.639753 KL: 0.604979 Neg-log -1.244732  [    0/30000]\n",
      "current batch loss: -0.435150 KL: 0.604789 Neg-log -1.039940  [12800/30000]\n",
      "current batch loss: -0.495214 KL: 0.604600 Neg-log -1.099814  [25600/30000]\n",
      "avg train loss per batch in training: -0.325313\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.391963 KL: 0.604529 Neg-log -0.996492 MSE 0.07404332607984543\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.342987 KL: 0.604529 Neg-log -0.947515 MSE 0.08357741683721542\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 156\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.499803 KL: 0.604529 Neg-log -1.104332  [    0/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: -0.520734 KL: 0.604338 Neg-log -1.125073  [12800/30000]\n",
      "current batch loss: -0.585826 KL: 0.604163 Neg-log -1.189990  [25600/30000]\n",
      "avg train loss per batch in training: -0.379951\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.396125 KL: 0.604094 Neg-log -1.000218 MSE 0.0792267769575119\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.401465 KL: 0.604094 Neg-log -1.005558 MSE 0.07441407442092896\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 157\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.600187 KL: 0.604093 Neg-log -1.204280  [    0/30000]\n",
      "current batch loss: -0.634491 KL: 0.603905 Neg-log -1.238396  [12800/30000]\n",
      "current batch loss: -0.218706 KL: 0.603719 Neg-log -0.822425  [25600/30000]\n",
      "avg train loss per batch in training: -0.368426\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.289368 KL: 0.603658 Neg-log -0.893026 MSE 0.0896693766117096\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.310211 KL: 0.603658 Neg-log -0.913868 MSE 0.0869426429271698\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 158\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.324396 KL: 0.603658 Neg-log -0.928053  [    0/30000]\n",
      "current batch loss: -0.459852 KL: 0.603467 Neg-log -1.063319  [12800/30000]\n",
      "current batch loss: -0.077040 KL: 0.603278 Neg-log -0.680318  [25600/30000]\n",
      "avg train loss per batch in training: -0.360499\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.287134 KL: 0.603209 Neg-log -0.890342 MSE 0.09865181893110275\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.300966 KL: 0.603209 Neg-log -0.904174 MSE 0.09615141153335571\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 159\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.480345 KL: 0.603208 Neg-log -1.083554  [    0/30000]\n",
      "current batch loss: -0.468697 KL: 0.603020 Neg-log -1.071717  [12800/30000]\n",
      "current batch loss: -0.198950 KL: 0.602834 Neg-log -0.801784  [25600/30000]\n",
      "avg train loss per batch in training: -0.366466\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.449565 KL: 0.602768 Neg-log -1.052332 MSE 0.07492751628160477\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.413548 KL: 0.602768 Neg-log -1.016315 MSE 0.07518547773361206\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 160\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.492374 KL: 0.602767 Neg-log -1.095141  [    0/30000]\n",
      "current batch loss: -0.758494 KL: 0.602581 Neg-log -1.361074  [12800/30000]\n",
      "current batch loss: -0.705133 KL: 0.602385 Neg-log -1.307517  [25600/30000]\n",
      "avg train loss per batch in training: -0.368273\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.463011 KL: 0.602317 Neg-log -1.065328 MSE 0.06748899817466736\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.459606 KL: 0.602317 Neg-log -1.061923 MSE 0.06992381811141968\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 161\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.544309 KL: 0.602317 Neg-log -1.146626  [    0/30000]\n",
      "current batch loss: 0.013868 KL: 0.602135 Neg-log -0.588267  [12800/30000]\n",
      "current batch loss: -0.687640 KL: 0.601955 Neg-log -1.289594  [25600/30000]\n",
      "avg train loss per batch in training: -0.439834\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.386348 KL: 0.601897 Neg-log -0.988243 MSE 0.07962051033973694\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.354872 KL: 0.601897 Neg-log -0.956767 MSE 0.08252009749412537\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 162\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.474711 KL: 0.601895 Neg-log -1.076607  [    0/30000]\n",
      "current batch loss: -0.602670 KL: 0.601706 Neg-log -1.204377  [12800/30000]\n",
      "current batch loss: -0.625320 KL: 0.601516 Neg-log -1.226837  [25600/30000]\n",
      "avg train loss per batch in training: -0.337402\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.448615 KL: 0.601450 Neg-log -1.050066 MSE 0.06914553046226501\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.456426 KL: 0.601450 Neg-log -1.057877 MSE 0.06798988580703735\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 163\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.274102 KL: 0.601451 Neg-log -0.875553  [    0/30000]\n",
      "current batch loss: -0.716739 KL: 0.601256 Neg-log -1.317994  [12800/30000]\n",
      "current batch loss: 0.159878 KL: 0.601087 Neg-log -0.441209  [25600/30000]\n",
      "avg train loss per batch in training: -0.396414\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.471129 KL: 0.601029 Neg-log -1.072158 MSE 0.06588488817214966\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.472980 KL: 0.601029 Neg-log -1.074009 MSE 0.06591932475566864\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 164\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.485765 KL: 0.601030 Neg-log -1.086794  [    0/30000]\n",
      "current batch loss: -0.630729 KL: 0.600864 Neg-log -1.231593  [12800/30000]\n",
      "current batch loss: -0.191352 KL: 0.600699 Neg-log -0.792051  [25600/30000]\n",
      "avg train loss per batch in training: -0.320319\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.428176 KL: 0.600637 Neg-log -1.028813 MSE 0.06855196505784988\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.409091 KL: 0.600637 Neg-log -1.009728 MSE 0.07612736523151398\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 165\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.222773 KL: 0.600637 Neg-log -0.377865  [    0/30000]\n",
      "current batch loss: -0.776682 KL: 0.600453 Neg-log -1.377134  [12800/30000]\n",
      "current batch loss: -0.529534 KL: 0.600273 Neg-log -1.129807  [25600/30000]\n",
      "avg train loss per batch in training: -0.371765\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.461763 KL: 0.600211 Neg-log -1.061974 MSE 0.06893090158700943\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.499107 KL: 0.600211 Neg-log -1.099319 MSE 0.0631035789847374\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 166\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.679170 KL: 0.600211 Neg-log -1.279381  [    0/30000]\n",
      "current batch loss: -0.351064 KL: 0.600047 Neg-log -0.951111  [12800/30000]\n",
      "current batch loss: -0.364822 KL: 0.599871 Neg-log -0.964694  [25600/30000]\n",
      "avg train loss per batch in training: 0.064140\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.449934 KL: 0.599808 Neg-log -1.049742 MSE 0.06852973252534866\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.444817 KL: 0.599808 Neg-log -1.044625 MSE 0.07148606330156326\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 167\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.502175 KL: 0.599808 Neg-log -1.101983  [    0/30000]\n",
      "current batch loss: -0.384612 KL: 0.599654 Neg-log -0.984266  [12800/30000]\n",
      "current batch loss: 0.024101 KL: 0.599499 Neg-log -0.575398  [25600/30000]\n",
      "avg train loss per batch in training: -0.416320\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.344199 KL: 0.599450 Neg-log -0.943648 MSE 0.0807875394821167\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.267935 KL: 0.599450 Neg-log -0.867385 MSE 0.0884648934006691\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 168\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.454320 KL: 0.599450 Neg-log -1.053770  [    0/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: -0.292751 KL: 0.599290 Neg-log -0.892041  [12800/30000]\n",
      "current batch loss: -0.678155 KL: 0.599133 Neg-log -1.277288  [25600/30000]\n",
      "avg train loss per batch in training: -0.353377\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.469375 KL: 0.599081 Neg-log -1.068455 MSE 0.06658478081226349\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.486890 KL: 0.599081 Neg-log -1.085970 MSE 0.06684229522943497\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 169\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.605561 KL: 0.599080 Neg-log -1.204641  [    0/30000]\n",
      "current batch loss: -0.510655 KL: 0.598936 Neg-log -1.109591  [12800/30000]\n",
      "current batch loss: -0.490880 KL: 0.598787 Neg-log -1.089667  [25600/30000]\n",
      "avg train loss per batch in training: -0.347350\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.385022 KL: 0.598730 Neg-log -0.983751 MSE 0.07638900727033615\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.391517 KL: 0.598730 Neg-log -0.990246 MSE 0.07943005859851837\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 170\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.649543 KL: 0.598729 Neg-log -1.248272  [    0/30000]\n",
      "current batch loss: 0.001241 KL: 0.598571 Neg-log -0.597330  [12800/30000]\n",
      "current batch loss: -0.433967 KL: 0.598414 Neg-log -1.032381  [25600/30000]\n",
      "avg train loss per batch in training: -0.381661\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.476335 KL: 0.598360 Neg-log -1.074697 MSE 0.06464416533708572\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.468985 KL: 0.598360 Neg-log -1.067346 MSE 0.06942321360111237\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 171\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.631373 KL: 0.598362 Neg-log -1.229735  [    0/30000]\n",
      "current batch loss: -0.701341 KL: 0.598211 Neg-log -1.299552  [12800/30000]\n",
      "current batch loss: -0.661413 KL: 0.598060 Neg-log -1.259473  [25600/30000]\n",
      "avg train loss per batch in training: -0.418225\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.479320 KL: 0.598007 Neg-log -1.077325 MSE 0.0690489113330841\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.498722 KL: 0.598007 Neg-log -1.096728 MSE 0.06549598276615143\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 172\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.596998 KL: 0.598005 Neg-log -1.195004  [    0/30000]\n",
      "current batch loss: -0.400430 KL: 0.597852 Neg-log -0.998283  [12800/30000]\n",
      "current batch loss: -0.600093 KL: 0.597706 Neg-log -1.197799  [25600/30000]\n",
      "avg train loss per batch in training: -0.356166\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.261249 KL: 0.597648 Neg-log -0.858896 MSE 0.09131208062171936\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.321180 KL: 0.597648 Neg-log -0.918827 MSE 0.08391071110963821\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 173\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.482388 KL: 0.597648 Neg-log -1.080036  [    0/30000]\n",
      "current batch loss: -0.560458 KL: 0.597495 Neg-log -1.157953  [12800/30000]\n",
      "current batch loss: -0.035393 KL: 0.597334 Neg-log -0.632727  [25600/30000]\n",
      "avg train loss per batch in training: -0.384574\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.016857 KL: 0.597281 Neg-log -0.580425 MSE 0.11947158724069595\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.024298 KL: 0.597281 Neg-log -0.572983 MSE 0.12203527987003326\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 174\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.508654 KL: 0.597281 Neg-log -1.105936  [    0/30000]\n",
      "current batch loss: -0.080638 KL: 0.597122 Neg-log -0.677760  [12800/30000]\n",
      "current batch loss: -0.580116 KL: 0.596972 Neg-log -1.177089  [25600/30000]\n",
      "avg train loss per batch in training: -0.458435\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.478831 KL: 0.596924 Neg-log -1.075754 MSE 0.07038760930299759\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.523823 KL: 0.596924 Neg-log -1.120746 MSE 0.06433361768722534\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 175\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.623606 KL: 0.596923 Neg-log -1.220529  [    0/30000]\n",
      "current batch loss: -0.581801 KL: 0.596770 Neg-log -1.178571  [12800/30000]\n",
      "current batch loss: -0.558124 KL: 0.596613 Neg-log -1.154736  [25600/30000]\n",
      "avg train loss per batch in training: -0.435508\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.523139 KL: 0.596557 Neg-log -1.119696 MSE 0.06092269346117973\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.491946 KL: 0.596557 Neg-log -1.088503 MSE 0.06557255983352661\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 176\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.536052 KL: 0.596557 Neg-log -1.132609  [    0/30000]\n",
      "current batch loss: -0.163495 KL: 0.596415 Neg-log -0.759911  [12800/30000]\n",
      "current batch loss: -0.726060 KL: 0.596265 Neg-log -1.322324  [25600/30000]\n",
      "avg train loss per batch in training: -0.412200\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.489274 KL: 0.596207 Neg-log -1.085482 MSE 0.0704241544008255\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.438263 KL: 0.596207 Neg-log -1.034471 MSE 0.07972432672977448\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 177\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.622010 KL: 0.596208 Neg-log -1.218218  [    0/30000]\n",
      "current batch loss: -0.776190 KL: 0.596064 Neg-log -1.372255  [12800/30000]\n",
      "current batch loss: -0.634447 KL: 0.595939 Neg-log -1.230386  [25600/30000]\n",
      "avg train loss per batch in training: -0.448311\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.437134 KL: 0.595893 Neg-log -1.033027 MSE 0.0688808336853981\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.408444 KL: 0.595893 Neg-log -1.004336 MSE 0.07068202644586563\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 178\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.551142 KL: 0.595893 Neg-log -1.147035  [    0/30000]\n",
      "current batch loss: 2.144372 KL: 0.595761 Neg-log 1.548611  [12800/30000]\n",
      "current batch loss: 0.231070 KL: 0.595628 Neg-log -0.364557  [25600/30000]\n",
      "avg train loss per batch in training: -0.446276\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.556264 KL: 0.595581 Neg-log -1.151844 MSE 0.05933065339922905\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.547848 KL: 0.595581 Neg-log -1.143429 MSE 0.060289595276117325\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 179\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.832485 KL: 0.595580 Neg-log -1.428065  [    0/30000]\n",
      "current batch loss: -0.528215 KL: 0.595448 Neg-log -1.123663  [12800/30000]\n",
      "current batch loss: -0.360218 KL: 0.595313 Neg-log -0.955532  [25600/30000]\n",
      "avg train loss per batch in training: -0.456059\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.158560 KL: 0.595266 Neg-log -0.753826 MSE 0.09611449390649796\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.205916 KL: 0.595266 Neg-log -0.801182 MSE 0.08997100591659546\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 180\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.650573 KL: 0.595266 Neg-log -1.245839  [    0/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: -0.511880 KL: 0.595130 Neg-log -1.107010  [12800/30000]\n",
      "current batch loss: 0.015788 KL: 0.594984 Neg-log -0.579197  [25600/30000]\n",
      "avg train loss per batch in training: -0.380563\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.395099 KL: 0.594939 Neg-log -0.990037 MSE 0.07832326740026474\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.464647 KL: 0.594939 Neg-log -1.059585 MSE 0.06875615566968918\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 181\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.090242 KL: 0.594938 Neg-log -0.685179  [    0/30000]\n",
      "current batch loss: -0.606153 KL: 0.594802 Neg-log -1.200955  [12800/30000]\n",
      "current batch loss: -0.538568 KL: 0.594661 Neg-log -1.133230  [25600/30000]\n",
      "avg train loss per batch in training: -0.453072\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.445038 KL: 0.594610 Neg-log -1.039648 MSE 0.06902121752500534\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.469245 KL: 0.594610 Neg-log -1.063855 MSE 0.06693971902132034\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 182\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.792659 KL: 0.594610 Neg-log -1.387269  [    0/30000]\n",
      "current batch loss: 0.168835 KL: 0.594479 Neg-log -0.425645  [12800/30000]\n",
      "current batch loss: -0.426867 KL: 0.594338 Neg-log -1.021205  [25600/30000]\n",
      "avg train loss per batch in training: -0.419345\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.405745 KL: 0.594286 Neg-log -1.000032 MSE 0.07352004945278168\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.376924 KL: 0.594286 Neg-log -0.971212 MSE 0.07882356643676758\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 183\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.786558 KL: 0.594288 Neg-log -1.380846  [    0/30000]\n",
      "current batch loss: -0.465455 KL: 0.594142 Neg-log -1.059598  [12800/30000]\n",
      "current batch loss: -0.557836 KL: 0.593997 Neg-log -1.151834  [25600/30000]\n",
      "avg train loss per batch in training: -0.441592\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.537605 KL: 0.593947 Neg-log -1.131551 MSE 0.06337939947843552\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.546946 KL: 0.593947 Neg-log -1.140891 MSE 0.06389573216438293\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 184\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.221852 KL: 0.593946 Neg-log -0.815798  [    0/30000]\n",
      "current batch loss: -0.489066 KL: 0.593804 Neg-log -1.082869  [12800/30000]\n",
      "current batch loss: -0.495105 KL: 0.593652 Neg-log -1.088757  [25600/30000]\n",
      "avg train loss per batch in training: -0.402195\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.513700 KL: 0.593599 Neg-log -1.107301 MSE 0.06499464809894562\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.449176 KL: 0.593599 Neg-log -1.042776 MSE 0.06830096244812012\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 185\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.255043 KL: 0.593600 Neg-log -0.338557  [    0/30000]\n",
      "current batch loss: -0.596411 KL: 0.593453 Neg-log -1.189864  [12800/30000]\n",
      "current batch loss: -0.684119 KL: 0.593314 Neg-log -1.277433  [25600/30000]\n",
      "avg train loss per batch in training: -0.419991\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.537088 KL: 0.593262 Neg-log -1.130350 MSE 0.05808563902974129\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.512647 KL: 0.593262 Neg-log -1.105909 MSE 0.0637311339378357\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 186\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.656744 KL: 0.593262 Neg-log -1.250007  [    0/30000]\n",
      "current batch loss: -0.655316 KL: 0.593130 Neg-log -1.248446  [12800/30000]\n",
      "current batch loss: 0.251370 KL: 0.592990 Neg-log -0.341620  [25600/30000]\n",
      "avg train loss per batch in training: -0.423925\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.470626 KL: 0.592948 Neg-log -1.063574 MSE 0.07338137924671173\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.496679 KL: 0.592948 Neg-log -1.089627 MSE 0.06575706601142883\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 187\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.389807 KL: 0.592948 Neg-log -0.982756  [    0/30000]\n",
      "current batch loss: -0.505474 KL: 0.592821 Neg-log -1.098295  [12800/30000]\n",
      "current batch loss: -0.621430 KL: 0.592704 Neg-log -1.214134  [25600/30000]\n",
      "avg train loss per batch in training: -0.429150\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.463327 KL: 0.592667 Neg-log -1.055993 MSE 0.06746464967727661\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.406092 KL: 0.592667 Neg-log -0.998758 MSE 0.07510732859373093\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 188\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.428244 KL: 0.592666 Neg-log -1.020910  [    0/30000]\n",
      "current batch loss: -0.760640 KL: 0.592542 Neg-log -1.353183  [12800/30000]\n",
      "current batch loss: -0.646708 KL: 0.592411 Neg-log -1.239119  [25600/30000]\n",
      "avg train loss per batch in training: -0.474121\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.393034 KL: 0.592368 Neg-log -0.985402 MSE 0.0826738029718399\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.377401 KL: 0.592368 Neg-log -0.969769 MSE 0.08386936783790588\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 189\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.541204 KL: 0.592368 Neg-log -1.133572  [    0/30000]\n",
      "current batch loss: 0.133448 KL: 0.592251 Neg-log -0.458803  [12800/30000]\n",
      "current batch loss: 0.072520 KL: 0.592140 Neg-log -0.519620  [25600/30000]\n",
      "avg train loss per batch in training: -0.504310\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.564943 KL: 0.592103 Neg-log -1.157048 MSE 0.05887618288397789\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.567088 KL: 0.592103 Neg-log -1.159193 MSE 0.0592065192759037\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 190\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.662258 KL: 0.592105 Neg-log -1.254363  [    0/30000]\n",
      "current batch loss: -0.525049 KL: 0.591988 Neg-log -1.117037  [12800/30000]\n",
      "current batch loss: -0.694379 KL: 0.591871 Neg-log -1.286250  [25600/30000]\n",
      "avg train loss per batch in training: -0.497681\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.495895 KL: 0.591834 Neg-log -1.087729 MSE 0.0660179853439331\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.484893 KL: 0.591834 Neg-log -1.076726 MSE 0.06152515485882759\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 191\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.407886 KL: 0.591833 Neg-log -0.183947  [    0/30000]\n",
      "current batch loss: -0.621201 KL: 0.591730 Neg-log -1.212931  [12800/30000]\n",
      "current batch loss: -0.596143 KL: 0.591611 Neg-log -1.187754  [25600/30000]\n",
      "avg train loss per batch in training: -0.413599\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.507686 KL: 0.591568 Neg-log -1.099254 MSE 0.06019069254398346\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.474586 KL: 0.591568 Neg-log -1.066154 MSE 0.06377626210451126\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 192\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.427646 KL: 0.591568 Neg-log -0.163922  [    0/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: -0.620251 KL: 0.591437 Neg-log -1.211688  [12800/30000]\n",
      "current batch loss: -0.314866 KL: 0.591332 Neg-log -0.906198  [25600/30000]\n",
      "avg train loss per batch in training: -0.496306\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.556553 KL: 0.591299 Neg-log -1.147850 MSE 0.06250960379838943\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.546086 KL: 0.591299 Neg-log -1.137383 MSE 0.06100661680102348\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 193\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.480988 KL: 0.591297 Neg-log -1.072285  [    0/30000]\n",
      "current batch loss: -0.330516 KL: 0.591182 Neg-log -0.921699  [12800/30000]\n",
      "current batch loss: -0.111566 KL: 0.591077 Neg-log -0.702643  [25600/30000]\n",
      "avg train loss per batch in training: -0.481751\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.317623 KL: 0.591041 Neg-log -0.908664 MSE 0.07966408133506775\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.333382 KL: 0.591041 Neg-log -0.924423 MSE 0.078260637819767\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 194\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.687688 KL: 0.591041 Neg-log -1.278729  [    0/30000]\n",
      "current batch loss: -0.000437 KL: 0.590945 Neg-log -0.591381  [12800/30000]\n",
      "current batch loss: 0.286092 KL: 0.590842 Neg-log -0.304750  [25600/30000]\n",
      "avg train loss per batch in training: -0.481517\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.473368 KL: 0.590806 Neg-log -1.064175 MSE 0.06358271837234497\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.492161 KL: 0.590806 Neg-log -1.082969 MSE 0.06407023221254349\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 195\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.026587 KL: 0.590807 Neg-log -0.617394  [    0/30000]\n",
      "current batch loss: -0.437832 KL: 0.590698 Neg-log -1.028529  [12800/30000]\n",
      "current batch loss: -0.597840 KL: 0.590597 Neg-log -1.188437  [25600/30000]\n",
      "avg train loss per batch in training: -0.444264\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.579312 KL: 0.590554 Neg-log -1.169866 MSE 0.052393484860658646\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.605685 KL: 0.590554 Neg-log -1.196239 MSE 0.05217518284916878\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 196\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.608086 KL: 0.590554 Neg-log -1.198640  [    0/30000]\n",
      "current batch loss: 0.166369 KL: 0.590436 Neg-log -0.424067  [12800/30000]\n",
      "current batch loss: -0.649427 KL: 0.590339 Neg-log -1.239766  [25600/30000]\n",
      "avg train loss per batch in training: -0.470861\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.474085 KL: 0.590302 Neg-log -1.064388 MSE 0.06255684047937393\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.427153 KL: 0.590302 Neg-log -1.017456 MSE 0.06949741393327713\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 197\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.476452 KL: 0.590303 Neg-log -1.066756  [    0/30000]\n",
      "current batch loss: -0.774392 KL: 0.590199 Neg-log -1.364591  [12800/30000]\n",
      "current batch loss: -0.506992 KL: 0.590094 Neg-log -1.097086  [25600/30000]\n",
      "avg train loss per batch in training: -0.520137\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.612715 KL: 0.590056 Neg-log -1.202770 MSE 0.05643367022275925\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.601715 KL: 0.590056 Neg-log -1.191770 MSE 0.05643349140882492\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 198\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.761083 KL: 0.590055 Neg-log -1.351138  [    0/30000]\n",
      "current batch loss: 0.308703 KL: 0.589961 Neg-log -0.281258  [12800/30000]\n",
      "current batch loss: -0.560093 KL: 0.589854 Neg-log -1.149947  [25600/30000]\n",
      "avg train loss per batch in training: -0.416567\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.368374 KL: 0.589815 Neg-log -0.958189 MSE 0.07273094356060028\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.357362 KL: 0.589815 Neg-log -0.947177 MSE 0.07488897442817688\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 199\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.532052 KL: 0.589815 Neg-log -1.121867  [    0/30000]\n",
      "current batch loss: -0.567171 KL: 0.589702 Neg-log -1.156873  [12800/30000]\n",
      "current batch loss: -0.790070 KL: 0.589590 Neg-log -1.379660  [25600/30000]\n",
      "avg train loss per batch in training: -0.497054\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.620233 KL: 0.589552 Neg-log -1.209784 MSE 0.050241779536008835\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.591895 KL: 0.589552 Neg-log -1.181445 MSE 0.057099077850580215\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 200\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.588658 KL: 0.589551 Neg-log -1.178209  [    0/30000]\n",
      "current batch loss: -0.739608 KL: 0.589439 Neg-log -1.329047  [12800/30000]\n",
      "current batch loss: -0.790841 KL: 0.589342 Neg-log -1.380183  [25600/30000]\n",
      "avg train loss per batch in training: -0.541374\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.588651 KL: 0.589310 Neg-log -1.177961 MSE 0.056698791682720184\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.594823 KL: 0.589310 Neg-log -1.184133 MSE 0.05567891150712967\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 201\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.350547 KL: 0.589310 Neg-log -0.939857  [    0/30000]\n",
      "current batch loss: -0.002525 KL: 0.589214 Neg-log -0.591738  [12800/30000]\n",
      "current batch loss: -0.741879 KL: 0.589112 Neg-log -1.330991  [25600/30000]\n",
      "avg train loss per batch in training: -0.438807\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.576136 KL: 0.589072 Neg-log -1.165209 MSE 0.05183415114879608\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.564128 KL: 0.589072 Neg-log -1.153201 MSE 0.05282837152481079\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 202\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.677350 KL: 0.589072 Neg-log -1.266422  [    0/30000]\n",
      "current batch loss: -0.341201 KL: 0.588970 Neg-log -0.930171  [12800/30000]\n",
      "current batch loss: -0.750798 KL: 0.588859 Neg-log -1.339657  [25600/30000]\n",
      "avg train loss per batch in training: -0.530042\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.589495 KL: 0.588827 Neg-log -1.178321 MSE 0.05545596405863762\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.577386 KL: 0.588827 Neg-log -1.166212 MSE 0.056465305387973785\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 203\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.801599 KL: 0.588825 Neg-log -1.390424  [    0/30000]\n",
      "current batch loss: -0.792549 KL: 0.588717 Neg-log -1.381266  [12800/30000]\n",
      "current batch loss: -0.589500 KL: 0.588616 Neg-log -1.178117  [25600/30000]\n",
      "avg train loss per batch in training: -0.526050\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.614382 KL: 0.588586 Neg-log -1.202969 MSE 0.050283320248126984\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.615836 KL: 0.588586 Neg-log -1.204423 MSE 0.05035945400595665\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 204\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.577025 KL: 0.588587 Neg-log -1.165612  [    0/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: -0.498714 KL: 0.588482 Neg-log -1.087196  [12800/30000]\n",
      "current batch loss: -0.754669 KL: 0.588389 Neg-log -1.343058  [25600/30000]\n",
      "avg train loss per batch in training: -0.566529\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.515708 KL: 0.588355 Neg-log -1.104062 MSE 0.05816115811467171\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.558905 KL: 0.588355 Neg-log -1.147259 MSE 0.0548514686524868\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 205\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.494935 KL: 0.588354 Neg-log -1.083289  [    0/30000]\n",
      "current batch loss: -0.610028 KL: 0.588264 Neg-log -1.198292  [12800/30000]\n",
      "current batch loss: -0.613455 KL: 0.588173 Neg-log -1.201628  [25600/30000]\n",
      "avg train loss per batch in training: -0.490504\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.398086 KL: 0.588136 Neg-log -0.986223 MSE 0.07021326571702957\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.345647 KL: 0.588136 Neg-log -0.933784 MSE 0.07761465758085251\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 206\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.357018 KL: 0.588138 Neg-log -0.945155  [    0/30000]\n",
      "current batch loss: -0.570698 KL: 0.588038 Neg-log -1.158736  [12800/30000]\n",
      "current batch loss: -0.666406 KL: 0.587947 Neg-log -1.254354  [25600/30000]\n",
      "avg train loss per batch in training: -0.520751\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.099875 KL: 0.587927 Neg-log -0.488051 MSE 0.12048400193452835\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.077165 KL: 0.587927 Neg-log -0.510760 MSE 0.1200513243675232\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 207\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.265715 KL: 0.587926 Neg-log -0.322211  [    0/30000]\n",
      "current batch loss: -0.414106 KL: 0.587802 Neg-log -1.001908  [12800/30000]\n",
      "current batch loss: -0.397826 KL: 0.587707 Neg-log -0.985533  [25600/30000]\n",
      "avg train loss per batch in training: -0.483798\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.643107 KL: 0.587675 Neg-log -1.230780 MSE 0.048107948154211044\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.612684 KL: 0.587675 Neg-log -1.200359 MSE 0.05130644515156746\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 208\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.734614 KL: 0.587674 Neg-log -1.322288  [    0/30000]\n",
      "current batch loss: -0.685139 KL: 0.587581 Neg-log -1.272719  [12800/30000]\n",
      "current batch loss: -0.736546 KL: 0.587471 Neg-log -1.324018  [25600/30000]\n",
      "avg train loss per batch in training: -0.548097\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.620457 KL: 0.587433 Neg-log -1.207892 MSE 0.05257805064320564\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.592525 KL: 0.587433 Neg-log -1.179960 MSE 0.05515308305621147\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 209\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.581760 KL: 0.587435 Neg-log -1.169194  [    0/30000]\n",
      "current batch loss: -0.451315 KL: 0.587333 Neg-log -1.038648  [12800/30000]\n",
      "current batch loss: -0.725836 KL: 0.587237 Neg-log -1.313073  [25600/30000]\n",
      "avg train loss per batch in training: -0.555053\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.199679 KL: 0.587204 Neg-log -0.786883 MSE 0.08633740246295929\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.207992 KL: 0.587204 Neg-log -0.795196 MSE 0.08838056027889252\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 210\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.228835 KL: 0.587204 Neg-log -0.816039  [    0/30000]\n",
      "current batch loss: -0.823112 KL: 0.587105 Neg-log -1.410217  [12800/30000]\n",
      "current batch loss: -0.683218 KL: 0.587023 Neg-log -1.270241  [25600/30000]\n",
      "avg train loss per batch in training: -0.584346\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.635798 KL: 0.586992 Neg-log -1.222790 MSE 0.049530141055583954\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.654393 KL: 0.586992 Neg-log -1.241386 MSE 0.049574583768844604\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 211\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.811649 KL: 0.586993 Neg-log -1.398641  [    0/30000]\n",
      "current batch loss: -0.622234 KL: 0.586897 Neg-log -1.209131  [12800/30000]\n",
      "current batch loss: -0.834904 KL: 0.586795 Neg-log -1.421699  [25600/30000]\n",
      "avg train loss per batch in training: -0.543938\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.625774 KL: 0.586753 Neg-log -1.212527 MSE 0.049436721950769424\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.630821 KL: 0.586753 Neg-log -1.217575 MSE 0.051074180752038956\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 212\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.866355 KL: 0.586754 Neg-log -1.453109  [    0/30000]\n",
      "current batch loss: 0.370239 KL: 0.586671 Neg-log -0.216432  [12800/30000]\n",
      "current batch loss: -0.871828 KL: 0.586580 Neg-log -1.458407  [25600/30000]\n",
      "avg train loss per batch in training: -0.590003\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.523661 KL: 0.586548 Neg-log -1.110211 MSE 0.05598943680524826\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.438130 KL: 0.586548 Neg-log -1.024679 MSE 0.06048281490802765\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 213\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.111799 KL: 0.586549 Neg-log -0.698348  [    0/30000]\n",
      "current batch loss: -0.532648 KL: 0.586455 Neg-log -1.119103  [12800/30000]\n",
      "current batch loss: 0.670007 KL: 0.586364 Neg-log 0.083643  [25600/30000]\n",
      "avg train loss per batch in training: -0.571096\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.692841 KL: 0.586327 Neg-log -1.279169 MSE 0.046433594077825546\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.658389 KL: 0.586327 Neg-log -1.244717 MSE 0.05001239478588104\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 214\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.801233 KL: 0.586328 Neg-log -1.387560  [    0/30000]\n",
      "current batch loss: -0.810985 KL: 0.586251 Neg-log -1.397236  [12800/30000]\n",
      "current batch loss: 0.141802 KL: 0.586180 Neg-log -0.444378  [25600/30000]\n",
      "avg train loss per batch in training: -0.643099\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.676977 KL: 0.586146 Neg-log -1.263124 MSE 0.046435557305812836\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.643840 KL: 0.586146 Neg-log -1.229987 MSE 0.049450427293777466\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 215\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.829803 KL: 0.586147 Neg-log -1.415950  [    0/30000]\n",
      "current batch loss: -0.734532 KL: 0.586073 Neg-log -1.320605  [12800/30000]\n",
      "current batch loss: -0.659900 KL: 0.585987 Neg-log -1.245887  [25600/30000]\n",
      "avg train loss per batch in training: -0.585246\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.605593 KL: 0.585963 Neg-log -1.191557 MSE 0.04914399981498718\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.615942 KL: 0.585963 Neg-log -1.201906 MSE 0.0511440746486187\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 216\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.531789 KL: 0.585964 Neg-log -1.117752  [    0/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: -0.295846 KL: 0.585885 Neg-log -0.881731  [12800/30000]\n",
      "current batch loss: -0.605116 KL: 0.585787 Neg-log -1.190903  [25600/30000]\n",
      "avg train loss per batch in training: -0.513715\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.394058 KL: 0.585761 Neg-log -0.979818 MSE 0.061763960868120193\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.413439 KL: 0.585761 Neg-log -0.999200 MSE 0.0654449537396431\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 217\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.123685 KL: 0.585761 Neg-log -0.462076  [    0/30000]\n",
      "current batch loss: -0.576416 KL: 0.585657 Neg-log -1.162072  [12800/30000]\n",
      "current batch loss: 0.357669 KL: 0.585571 Neg-log -0.227902  [25600/30000]\n",
      "avg train loss per batch in training: -0.573949\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.675059 KL: 0.585540 Neg-log -1.260598 MSE 0.04304139316082001\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.667212 KL: 0.585540 Neg-log -1.252751 MSE 0.047285296022892\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 218\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.577843 KL: 0.585540 Neg-log -1.163383  [    0/30000]\n",
      "current batch loss: -0.860926 KL: 0.585466 Neg-log -1.446392  [12800/30000]\n",
      "current batch loss: -0.854723 KL: 0.585368 Neg-log -1.440092  [25600/30000]\n",
      "avg train loss per batch in training: -0.598520\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.683605 KL: 0.585334 Neg-log -1.268939 MSE 0.04507854953408241\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.652376 KL: 0.585334 Neg-log -1.237709 MSE 0.048144903033971786\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 219\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.844942 KL: 0.585333 Neg-log -1.430276  [    0/30000]\n",
      "current batch loss: -0.767891 KL: 0.585241 Neg-log -1.353132  [12800/30000]\n",
      "current batch loss: -0.815824 KL: 0.585162 Neg-log -1.400986  [25600/30000]\n",
      "avg train loss per batch in training: -0.581468\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.570428 KL: 0.585134 Neg-log -1.155562 MSE 0.06039389967918396\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.620059 KL: 0.585134 Neg-log -1.205191 MSE 0.057406116276979446\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 220\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.823559 KL: 0.585133 Neg-log -1.408692  [    0/30000]\n",
      "current batch loss: -0.742004 KL: 0.585032 Neg-log -1.327036  [12800/30000]\n",
      "current batch loss: -0.506808 KL: 0.584952 Neg-log -1.091761  [25600/30000]\n",
      "avg train loss per batch in training: -0.577641\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.659743 KL: 0.584931 Neg-log -1.244674 MSE 0.05025096237659454\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.643722 KL: 0.584931 Neg-log -1.228654 MSE 0.051829345524311066\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 221\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.438191 KL: 0.584931 Neg-log -1.023123  [    0/30000]\n",
      "current batch loss: -0.600854 KL: 0.584846 Neg-log -1.185700  [12800/30000]\n",
      "current batch loss: -0.501686 KL: 0.584743 Neg-log -1.086428  [25600/30000]\n",
      "avg train loss per batch in training: -0.559189\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.433962 KL: 0.584715 Neg-log -1.018674 MSE 0.07139735668897629\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.401382 KL: 0.584715 Neg-log -0.986095 MSE 0.07547198981046677\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 222\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.297095 KL: 0.584713 Neg-log -0.881808  [    0/30000]\n",
      "current batch loss: -0.805822 KL: 0.584620 Neg-log -1.390442  [12800/30000]\n",
      "current batch loss: -0.936661 KL: 0.584527 Neg-log -1.521188  [25600/30000]\n",
      "avg train loss per batch in training: -0.627395\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.573671 KL: 0.584501 Neg-log -1.158171 MSE 0.052914686501026154\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.596314 KL: 0.584501 Neg-log -1.180814 MSE 0.05133280158042908\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 223\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.845585 KL: 0.584500 Neg-log -1.430085  [    0/30000]\n",
      "current batch loss: -0.694449 KL: 0.584396 Neg-log -1.278845  [12800/30000]\n",
      "current batch loss: -0.778521 KL: 0.584295 Neg-log -1.362815  [25600/30000]\n",
      "avg train loss per batch in training: -0.521736\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.545736 KL: 0.584258 Neg-log -1.129993 MSE 0.054506704211235046\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.515323 KL: 0.584258 Neg-log -1.099579 MSE 0.056294798851013184\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 224\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.658930 KL: 0.584256 Neg-log -1.243186  [    0/30000]\n",
      "current batch loss: -0.420407 KL: 0.584161 Neg-log -1.004568  [12800/30000]\n",
      "current batch loss: -0.782789 KL: 0.584055 Neg-log -1.366844  [25600/30000]\n",
      "avg train loss per batch in training: -0.614296\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.651406 KL: 0.584030 Neg-log -1.235436 MSE 0.04564998298883438\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.655947 KL: 0.584030 Neg-log -1.239976 MSE 0.04630894586443901\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 225\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.899227 KL: 0.584030 Neg-log -1.483257  [    0/30000]\n",
      "current batch loss: -0.612882 KL: 0.583944 Neg-log -1.196827  [12800/30000]\n",
      "current batch loss: -0.639099 KL: 0.583851 Neg-log -1.222950  [25600/30000]\n",
      "avg train loss per batch in training: -0.593191\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.368143 KL: 0.583826 Neg-log -0.951969 MSE 0.07052349299192429\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.362036 KL: 0.583826 Neg-log -0.945862 MSE 0.07246634364128113\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 226\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.907382 KL: 0.583826 Neg-log -1.491208  [    0/30000]\n",
      "current batch loss: -0.817765 KL: 0.583746 Neg-log -1.401511  [12800/30000]\n",
      "current batch loss: -0.635062 KL: 0.583660 Neg-log -1.218723  [25600/30000]\n",
      "avg train loss per batch in training: -0.635134\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.618996 KL: 0.583618 Neg-log -1.202614 MSE 0.052077386528253555\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.590377 KL: 0.583618 Neg-log -1.173995 MSE 0.05178212746977806\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 227\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.652900 KL: 0.583618 Neg-log -1.236517  [    0/30000]\n",
      "current batch loss: -0.174196 KL: 0.583533 Neg-log -0.757728  [12800/30000]\n",
      "current batch loss: -0.737567 KL: 0.583431 Neg-log -1.320998  [25600/30000]\n",
      "avg train loss per batch in training: -0.633072\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.738893 KL: 0.583404 Neg-log -1.322297 MSE 0.04143153876066208\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.716791 KL: 0.583404 Neg-log -1.300193 MSE 0.04382231831550598\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 228\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.884853 KL: 0.583403 Neg-log -1.468255  [    0/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: -0.933598 KL: 0.583322 Neg-log -1.516919  [12800/30000]\n",
      "current batch loss: -0.554976 KL: 0.583216 Neg-log -1.138192  [25600/30000]\n",
      "avg train loss per batch in training: -0.623101\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.723789 KL: 0.583185 Neg-log -1.306975 MSE 0.042448267340660095\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.721325 KL: 0.583185 Neg-log -1.304510 MSE 0.04156852886080742\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 229\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.853692 KL: 0.583186 Neg-log -1.436878  [    0/30000]\n",
      "current batch loss: -0.872516 KL: 0.583073 Neg-log -1.455589  [12800/30000]\n",
      "current batch loss: -0.887125 KL: 0.582984 Neg-log -1.470109  [25600/30000]\n",
      "avg train loss per batch in training: -0.633764\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.600182 KL: 0.582945 Neg-log -1.183127 MSE 0.05115572735667229\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.613799 KL: 0.582945 Neg-log -1.196743 MSE 0.050518523901700974\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 230\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.895938 KL: 0.582944 Neg-log 0.312994  [    0/30000]\n",
      "current batch loss: -0.656797 KL: 0.582846 Neg-log -1.239643  [12800/30000]\n",
      "current batch loss: -0.324757 KL: 0.582751 Neg-log -0.907508  [25600/30000]\n",
      "avg train loss per batch in training: -0.639704\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.697302 KL: 0.582711 Neg-log -1.280014 MSE 0.0444599874317646\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.707674 KL: 0.582711 Neg-log -1.290386 MSE 0.04421466588973999\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 231\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.897008 KL: 0.582712 Neg-log -1.479720  [    0/30000]\n",
      "current batch loss: -0.707152 KL: 0.582615 Neg-log -1.289767  [12800/30000]\n",
      "current batch loss: -0.839179 KL: 0.582529 Neg-log -1.421707  [25600/30000]\n",
      "avg train loss per batch in training: -0.673662\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.528771 KL: 0.582489 Neg-log -1.111258 MSE 0.05416426062583923\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.559150 KL: 0.582489 Neg-log -1.141637 MSE 0.05235850438475609\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 232\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.795302 KL: 0.582487 Neg-log -1.377789  [    0/30000]\n",
      "current batch loss: -0.607937 KL: 0.582390 Neg-log -1.190327  [12800/30000]\n",
      "current batch loss: 0.151780 KL: 0.582295 Neg-log -0.430514  [25600/30000]\n",
      "avg train loss per batch in training: -0.600593\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.555254 KL: 0.582252 Neg-log -1.137507 MSE 0.05770391598343849\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.525527 KL: 0.582252 Neg-log -1.107779 MSE 0.06077037379145622\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 233\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.747674 KL: 0.582252 Neg-log -1.329926  [    0/30000]\n",
      "current batch loss: -0.938581 KL: 0.582130 Neg-log -1.520711  [12800/30000]\n",
      "current batch loss: -0.874864 KL: 0.582026 Neg-log -1.456890  [25600/30000]\n",
      "avg train loss per batch in training: -0.664129\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.266447 KL: 0.581991 Neg-log -0.848436 MSE 0.07580221444368362\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.277180 KL: 0.581991 Neg-log -0.859170 MSE 0.07540857046842575\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 234\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.535226 KL: 0.581990 Neg-log -1.117215  [    0/30000]\n",
      "current batch loss: -0.662387 KL: 0.581887 Neg-log -1.244274  [12800/30000]\n",
      "current batch loss: -0.806250 KL: 0.581769 Neg-log -1.388019  [25600/30000]\n",
      "avg train loss per batch in training: -0.714012\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.678947 KL: 0.581728 Neg-log -1.260676 MSE 0.04791761562228203\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.597193 KL: 0.581728 Neg-log -1.178921 MSE 0.05261528864502907\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 235\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.970400 KL: 0.581729 Neg-log -1.552130  [    0/30000]\n",
      "current batch loss: -0.842960 KL: 0.581621 Neg-log -1.424581  [12800/30000]\n",
      "current batch loss: -0.592189 KL: 0.581506 Neg-log -1.173695  [25600/30000]\n",
      "avg train loss per batch in training: -0.640685\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.730890 KL: 0.581466 Neg-log -1.312355 MSE 0.04032798483967781\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.723010 KL: 0.581466 Neg-log -1.304475 MSE 0.040328025817871094\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 236\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.774565 KL: 0.581465 Neg-log -1.356030  [    0/30000]\n",
      "current batch loss: -0.648106 KL: 0.581339 Neg-log -1.229445  [12800/30000]\n",
      "current batch loss: -0.460029 KL: 0.581222 Neg-log -1.041251  [25600/30000]\n",
      "avg train loss per batch in training: -0.636949\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.705249 KL: 0.581184 Neg-log -1.286432 MSE 0.042123351246118546\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.678939 KL: 0.581184 Neg-log -1.260123 MSE 0.045005425810813904\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 237\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.762189 KL: 0.581184 Neg-log -1.343373  [    0/30000]\n",
      "current batch loss: -0.774560 KL: 0.581065 Neg-log -1.355625  [12800/30000]\n",
      "current batch loss: -0.841262 KL: 0.580970 Neg-log -1.422232  [25600/30000]\n",
      "avg train loss per batch in training: -0.700264\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.704912 KL: 0.580938 Neg-log -1.285849 MSE 0.048709236085414886\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.659903 KL: 0.580938 Neg-log -1.240841 MSE 0.05102971941232681\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 238\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.745744 KL: 0.580938 Neg-log -1.326681  [    0/30000]\n",
      "current batch loss: -0.877409 KL: 0.580838 Neg-log -1.458246  [12800/30000]\n",
      "current batch loss: -0.905527 KL: 0.580710 Neg-log -1.486237  [25600/30000]\n",
      "avg train loss per batch in training: -0.615836\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.778693 KL: 0.580666 Neg-log -1.359360 MSE 0.04009493440389633\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.745743 KL: 0.580666 Neg-log -1.326411 MSE 0.04168957471847534\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 239\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.845471 KL: 0.580667 Neg-log -1.426138  [    0/30000]\n",
      "current batch loss: -0.859505 KL: 0.580546 Neg-log -1.440051  [12800/30000]\n",
      "current batch loss: -0.959220 KL: 0.580416 Neg-log -1.539636  [25600/30000]\n",
      "avg train loss per batch in training: -0.648318\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.821818 KL: 0.580383 Neg-log -1.402200 MSE 0.036087699234485626\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.779562 KL: 0.580383 Neg-log -1.359943 MSE 0.03974583372473717\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 240\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.753920 KL: 0.580382 Neg-log -1.334302  [    0/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: -0.714600 KL: 0.580278 Neg-log -1.294878  [12800/30000]\n",
      "current batch loss: -0.805680 KL: 0.580146 Neg-log -1.385826  [25600/30000]\n",
      "avg train loss per batch in training: -0.691666\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.808584 KL: 0.580100 Neg-log -1.388684 MSE 0.0386396124958992\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.779683 KL: 0.580100 Neg-log -1.359784 MSE 0.03927262872457504\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 241\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.891089 KL: 0.580100 Neg-log -1.471189  [    0/30000]\n",
      "current batch loss: -0.788011 KL: 0.579977 Neg-log -1.367987  [12800/30000]\n",
      "current batch loss: -0.537035 KL: 0.579864 Neg-log -1.116899  [25600/30000]\n",
      "avg train loss per batch in training: -0.650537\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.760873 KL: 0.579819 Neg-log -1.340692 MSE 0.03952525928616524\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.735066 KL: 0.579819 Neg-log -1.314886 MSE 0.04366747662425041\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 242\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.750833 KL: 0.579819 Neg-log -1.330652  [    0/30000]\n",
      "current batch loss: -0.362325 KL: 0.579705 Neg-log -0.942029  [12800/30000]\n",
      "current batch loss: -0.721653 KL: 0.579593 Neg-log -1.301246  [25600/30000]\n",
      "avg train loss per batch in training: -0.678804\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.576167 KL: 0.579546 Neg-log -1.155714 MSE 0.05057530105113983\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.563479 KL: 0.579546 Neg-log -1.143026 MSE 0.05260634794831276\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 243\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.552836 KL: 0.579547 Neg-log -1.132383  [    0/30000]\n",
      "current batch loss: -0.786193 KL: 0.579426 Neg-log -1.365619  [12800/30000]\n",
      "current batch loss: -0.841615 KL: 0.579307 Neg-log -1.420922  [25600/30000]\n",
      "avg train loss per batch in training: -0.692188\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.441943 KL: 0.579260 Neg-log -1.021202 MSE 0.06130942702293396\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.438872 KL: 0.579260 Neg-log -1.018132 MSE 0.06023351475596428\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 244\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.886066 KL: 0.579259 Neg-log -1.465325  [    0/30000]\n",
      "current batch loss: -0.932731 KL: 0.579129 Neg-log -1.511860  [12800/30000]\n",
      "current batch loss: -0.882279 KL: 0.579009 Neg-log -1.461288  [25600/30000]\n",
      "avg train loss per batch in training: -0.670949\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.315223 KL: 0.578966 Neg-log -0.894190 MSE 0.07318980246782303\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.282265 KL: 0.578966 Neg-log -0.861232 MSE 0.0766989067196846\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 245\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.841853 KL: 0.578967 Neg-log -1.420820  [    0/30000]\n",
      "current batch loss: -0.923803 KL: 0.578854 Neg-log -1.502656  [12800/30000]\n",
      "current batch loss: -0.787763 KL: 0.578723 Neg-log -1.366486  [25600/30000]\n",
      "avg train loss per batch in training: -0.696314\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.798702 KL: 0.578674 Neg-log -1.377376 MSE 0.03634561970829964\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.737552 KL: 0.578674 Neg-log -1.316226 MSE 0.03974660113453865\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 246\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.978231 KL: 0.578674 Neg-log -1.556905  [    0/30000]\n",
      "current batch loss: 4.913252 KL: 0.578533 Neg-log 4.334720  [12800/30000]\n",
      "current batch loss: -0.109907 KL: 0.578360 Neg-log -0.688267  [25600/30000]\n",
      "avg train loss per batch in training: 2739.373521\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.315233 KL: 0.578293 Neg-log -0.893528 MSE 0.09127646684646606\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.308288 KL: 0.578293 Neg-log -0.886582 MSE 0.09162448346614838\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 247\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.292332 KL: 0.578294 Neg-log -0.870626  [    0/30000]\n",
      "current batch loss: -0.448797 KL: 0.578120 Neg-log -1.026917  [12800/30000]\n",
      "current batch loss: -0.579289 KL: 0.577968 Neg-log -1.157257  [25600/30000]\n",
      "avg train loss per batch in training: -0.470630\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 638825.532289 KL: 0.577918 Neg-log 638825.062500 MSE 0.11836400628089905\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.610027 KL: 0.577918 Neg-log -1.187944 MSE 0.053522489964962006\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 248\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.653808 KL: 0.577917 Neg-log -1.231726  [    0/30000]\n",
      "current batch loss: -0.775099 KL: 0.577789 Neg-log -1.352888  [12800/30000]\n",
      "current batch loss: -0.689991 KL: 0.577658 Neg-log -1.267649  [25600/30000]\n",
      "avg train loss per batch in training: -0.590502\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.624964 KL: 0.577613 Neg-log -1.202578 MSE 0.05715198069810867\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.639127 KL: 0.577613 Neg-log -1.216740 MSE 0.05407522991299629\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 249\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.744736 KL: 0.577613 Neg-log -1.322350  [    0/30000]\n",
      "current batch loss: -0.754536 KL: 0.577487 Neg-log -1.332023  [12800/30000]\n",
      "current batch loss: -0.816711 KL: 0.577364 Neg-log -1.394076  [25600/30000]\n",
      "avg train loss per batch in training: -0.630368\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.625848 KL: 0.577332 Neg-log -1.203180 MSE 0.06118810921907425\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.646178 KL: 0.577332 Neg-log -1.223510 MSE 0.05066072940826416\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 250\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.543432 KL: 0.577332 Neg-log -1.120764  [    0/30000]\n",
      "current batch loss: -1.000536 KL: 0.577224 Neg-log -1.577760  [12800/30000]\n",
      "current batch loss: -0.712772 KL: 0.577107 Neg-log -1.289880  [25600/30000]\n",
      "avg train loss per batch in training: -0.647755\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.526053 KL: 0.577070 Neg-log -1.103122 MSE 0.05720249190926552\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.493155 KL: 0.577070 Neg-log -1.070224 MSE 0.062036313116550446\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 251\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.701096 KL: 0.577069 Neg-log -1.278165  [    0/30000]\n",
      "current batch loss: -0.509776 KL: 0.576944 Neg-log -1.086720  [12800/30000]\n",
      "current batch loss: -0.746931 KL: 0.576815 Neg-log -1.323747  [25600/30000]\n",
      "avg train loss per batch in training: -0.627229\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.626554 KL: 0.576766 Neg-log -1.203319 MSE 0.05226777121424675\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.655686 KL: 0.576766 Neg-log -1.232451 MSE 0.05134349688887596\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 252\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.422891 KL: 0.576765 Neg-log -0.999656  [    0/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: -0.670979 KL: 0.576639 Neg-log -1.247617  [12800/30000]\n",
      "current batch loss: -0.803490 KL: 0.576509 Neg-log -1.379999  [25600/30000]\n",
      "avg train loss per batch in training: -0.645285\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.715711 KL: 0.576463 Neg-log -1.292176 MSE 0.04575061425566673\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.671835 KL: 0.576463 Neg-log -1.248299 MSE 0.05107222497463226\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 253\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.801537 KL: 0.576464 Neg-log -1.378002  [    0/30000]\n",
      "current batch loss: -0.940329 KL: 0.576338 Neg-log -1.516667  [12800/30000]\n",
      "current batch loss: 0.057357 KL: 0.576215 Neg-log -0.518857  [25600/30000]\n",
      "avg train loss per batch in training: -0.625333\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.691278 KL: 0.576165 Neg-log -1.267443 MSE 0.04522358998656273\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.676459 KL: 0.576165 Neg-log -1.252625 MSE 0.04757024720311165\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 254\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.800511 KL: 0.576165 Neg-log -1.376677  [    0/30000]\n",
      "current batch loss: -0.825885 KL: 0.576035 Neg-log -1.401920  [12800/30000]\n",
      "current batch loss: -0.521329 KL: 0.575905 Neg-log -1.097234  [25600/30000]\n",
      "avg train loss per batch in training: -0.612646\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.713494 KL: 0.575852 Neg-log -1.289348 MSE 0.04650122672319412\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.697647 KL: 0.575852 Neg-log -1.273500 MSE 0.04538901895284653\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 255\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.781711 KL: 0.575853 Neg-log -1.357564  [    0/30000]\n",
      "current batch loss: -0.822019 KL: 0.575721 Neg-log -1.397740  [12800/30000]\n",
      "current batch loss: -0.244544 KL: 0.575590 Neg-log -0.820134  [25600/30000]\n",
      "avg train loss per batch in training: -0.696199\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.741022 KL: 0.575546 Neg-log -1.316568 MSE 0.04314209520816803\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.715442 KL: 0.575546 Neg-log -1.290987 MSE 0.04601321369409561\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 256\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.736470 KL: 0.575546 Neg-log -1.312016  [    0/30000]\n",
      "current batch loss: -0.703056 KL: 0.575409 Neg-log -1.278466  [12800/30000]\n",
      "current batch loss: -0.989236 KL: 0.575259 Neg-log -1.564495  [25600/30000]\n",
      "avg train loss per batch in training: -0.658767\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.346556 KL: 0.575210 Neg-log -0.921764 MSE 0.0794191062450409\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.319052 KL: 0.575210 Neg-log -0.894261 MSE 0.08662280440330505\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 257\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.033430 KL: 0.575209 Neg-log -0.541779  [    0/30000]\n",
      "current batch loss: -0.634327 KL: 0.575070 Neg-log -1.209397  [12800/30000]\n",
      "current batch loss: -0.710961 KL: 0.574930 Neg-log -1.285891  [25600/30000]\n",
      "avg train loss per batch in training: -0.689223\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 10.054970 KL: 0.574890 Neg-log 9.480081 MSE 0.0633491650223732\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.575256 KL: 0.574890 Neg-log -1.150146 MSE 0.054421234875917435\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 258\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.437486 KL: 0.574889 Neg-log -0.137403  [    0/30000]\n",
      "current batch loss: -0.585375 KL: 0.574762 Neg-log -1.160137  [12800/30000]\n",
      "current batch loss: -0.783986 KL: 0.574630 Neg-log -1.358616  [25600/30000]\n",
      "avg train loss per batch in training: -0.698915\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.717681 KL: 0.574584 Neg-log -1.292264 MSE 0.04733743891119957\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.748951 KL: 0.574584 Neg-log -1.323534 MSE 0.043562985956668854\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 259\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.901415 KL: 0.574583 Neg-log -1.475998  [    0/30000]\n",
      "current batch loss: -0.378216 KL: 0.574442 Neg-log -0.952658  [12800/30000]\n",
      "current batch loss: -0.539152 KL: 0.574298 Neg-log -1.113450  [25600/30000]\n",
      "avg train loss per batch in training: -0.656256\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.621163 KL: 0.574244 Neg-log -1.195408 MSE 0.051777567714452744\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.605876 KL: 0.574244 Neg-log -1.180121 MSE 0.05489865690469742\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 260\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.164370 KL: 0.574245 Neg-log -0.409875  [    0/30000]\n",
      "current batch loss: -0.541538 KL: 0.574084 Neg-log -1.115622  [12800/30000]\n",
      "current batch loss: -0.663785 KL: 0.573942 Neg-log -1.237727  [25600/30000]\n",
      "avg train loss per batch in training: -0.149152\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.695846 KL: 0.573891 Neg-log -1.269738 MSE 0.04794391989707947\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.707457 KL: 0.573891 Neg-log -1.281349 MSE 0.04946909099817276\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 261\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.740901 KL: 0.573892 Neg-log -1.314793  [    0/30000]\n",
      "current batch loss: -0.834096 KL: 0.573762 Neg-log -1.407858  [12800/30000]\n",
      "current batch loss: -0.831892 KL: 0.573622 Neg-log -1.405513  [25600/30000]\n",
      "avg train loss per batch in training: -0.668571\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.485568 KL: 0.573572 Neg-log -1.059140 MSE 0.06731635332107544\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.446300 KL: 0.573572 Neg-log -1.019872 MSE 0.06386920064687729\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 262\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.302302 KL: 0.573572 Neg-log -0.875874  [    0/30000]\n",
      "current batch loss: -0.789860 KL: 0.573446 Neg-log -1.363306  [12800/30000]\n",
      "current batch loss: -0.639167 KL: 0.573309 Neg-log -1.212476  [25600/30000]\n",
      "avg train loss per batch in training: -0.501854\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.730271 KL: 0.573259 Neg-log -1.303532 MSE 0.04440119490027428\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.724324 KL: 0.573259 Neg-log -1.297584 MSE 0.04508822783827782\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 263\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.952026 KL: 0.573260 Neg-log -1.525286  [    0/30000]\n",
      "current batch loss: -0.879503 KL: 0.573117 Neg-log -1.452620  [12800/30000]\n",
      "current batch loss: -0.235303 KL: 0.572993 Neg-log -0.808295  [25600/30000]\n",
      "avg train loss per batch in training: -0.691144\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.740403 KL: 0.572951 Neg-log -1.313353 MSE 0.042447347193956375\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.728912 KL: 0.572951 Neg-log -1.301862 MSE 0.04511187970638275\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 264\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.818472 KL: 0.572950 Neg-log -1.391422  [    0/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: -0.868593 KL: 0.572808 Neg-log -1.441401  [12800/30000]\n",
      "current batch loss: 0.320279 KL: 0.572685 Neg-log -0.252406  [25600/30000]\n",
      "avg train loss per batch in training: -0.675465\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.678475 KL: 0.572640 Neg-log -1.251115 MSE 0.04784083366394043\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.692114 KL: 0.572640 Neg-log -1.264754 MSE 0.04743904992938042\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 265\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.521481 KL: 0.572640 Neg-log -1.094121  [    0/30000]\n",
      "current batch loss: -0.825066 KL: 0.572512 Neg-log -1.397578  [12800/30000]\n",
      "current batch loss: -0.841622 KL: 0.572379 Neg-log -1.414001  [25600/30000]\n",
      "avg train loss per batch in training: -0.723274\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.757498 KL: 0.572328 Neg-log -1.329827 MSE 0.04185810312628746\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.734085 KL: 0.572328 Neg-log -1.306413 MSE 0.044918522238731384\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 266\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.171393 KL: 0.572329 Neg-log -0.743722  [    0/30000]\n",
      "current batch loss: -0.800170 KL: 0.572196 Neg-log -1.372366  [12800/30000]\n",
      "current batch loss: -0.846717 KL: 0.572058 Neg-log -1.418775  [25600/30000]\n",
      "avg train loss per batch in training: -0.709215\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.743601 KL: 0.572013 Neg-log -1.315614 MSE 0.04302072897553444\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.720670 KL: 0.572013 Neg-log -1.292683 MSE 0.045711785554885864\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 267\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.000353 KL: 0.572013 Neg-log -1.572367  [    0/30000]\n",
      "current batch loss: -0.847460 KL: 0.571877 Neg-log -1.419337  [12800/30000]\n",
      "current batch loss: -0.742937 KL: 0.571738 Neg-log -1.314675  [25600/30000]\n",
      "avg train loss per batch in training: -0.682924\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.739553 KL: 0.571686 Neg-log -1.311239 MSE 0.042993247509002686\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.734854 KL: 0.571686 Neg-log -1.306540 MSE 0.043564364314079285\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 268\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.750678 KL: 0.571686 Neg-log -1.322364  [    0/30000]\n",
      "current batch loss: -0.609571 KL: 0.571540 Neg-log -1.181111  [12800/30000]\n",
      "current batch loss: -0.476266 KL: 0.571418 Neg-log -1.047684  [25600/30000]\n",
      "avg train loss per batch in training: -0.700472\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.452186 KL: 0.571367 Neg-log -1.023554 MSE 0.061304256319999695\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.378907 KL: 0.571367 Neg-log -0.950276 MSE 0.0650840550661087\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 269\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.540823 KL: 0.571368 Neg-log -0.030545  [    0/30000]\n",
      "current batch loss: -0.717112 KL: 0.571220 Neg-log -1.288331  [12800/30000]\n",
      "current batch loss: -0.926751 KL: 0.571069 Neg-log -1.497821  [25600/30000]\n",
      "avg train loss per batch in training: -0.692258\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.689105 KL: 0.571020 Neg-log -1.260123 MSE 0.0463438555598259\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.596270 KL: 0.571020 Neg-log -1.167289 MSE 0.05354128032922745\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 270\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.039873 KL: 0.571019 Neg-log -0.531145  [    0/30000]\n",
      "current batch loss: -0.643522 KL: 0.570870 Neg-log -1.214391  [12800/30000]\n",
      "current batch loss: -0.672230 KL: 0.570724 Neg-log -1.242953  [25600/30000]\n",
      "avg train loss per batch in training: -0.694813\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.764187 KL: 0.570665 Neg-log -1.334854 MSE 0.043109674006700516\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.754850 KL: 0.570665 Neg-log -1.325517 MSE 0.04270987585186958\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 271\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.829040 KL: 0.570667 Neg-log -1.399707  [    0/30000]\n",
      "current batch loss: -0.739204 KL: 0.570508 Neg-log -1.309712  [12800/30000]\n",
      "current batch loss: -0.913887 KL: 0.570355 Neg-log -1.484243  [25600/30000]\n",
      "avg train loss per batch in training: -0.739997\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.749511 KL: 0.570297 Neg-log -1.319808 MSE 0.042462605983018875\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.727083 KL: 0.570297 Neg-log -1.297380 MSE 0.046037688851356506\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 272\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.996347 KL: 0.570297 Neg-log -1.566644  [    0/30000]\n",
      "current batch loss: -0.948459 KL: 0.570161 Neg-log -1.518620  [12800/30000]\n",
      "current batch loss: -0.492536 KL: 0.570026 Neg-log -1.062562  [25600/30000]\n",
      "avg train loss per batch in training: -0.730996\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.765860 KL: 0.569979 Neg-log -1.335841 MSE 0.04146619886159897\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.772007 KL: 0.569979 Neg-log -1.341986 MSE 0.041161078959703445\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 273\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.759726 KL: 0.569980 Neg-log -1.329706  [    0/30000]\n",
      "current batch loss: -0.861419 KL: 0.569834 Neg-log -1.431253  [12800/30000]\n",
      "current batch loss: -0.946783 KL: 0.569677 Neg-log -1.516459  [25600/30000]\n",
      "avg train loss per batch in training: -0.743778\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.717433 KL: 0.569627 Neg-log -1.287061 MSE 0.04483930394053459\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.712001 KL: 0.569627 Neg-log -1.281629 MSE 0.046500541269779205\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 274\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.865630 KL: 0.569629 Neg-log -1.435259  [    0/30000]\n",
      "current batch loss: -0.771110 KL: 0.569471 Neg-log -1.340581  [12800/30000]\n",
      "current batch loss: -0.899876 KL: 0.569326 Neg-log -1.469202  [25600/30000]\n",
      "avg train loss per batch in training: -0.751542\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.754407 KL: 0.569280 Neg-log -1.323686 MSE 0.04416540265083313\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.730260 KL: 0.569280 Neg-log -1.299540 MSE 0.04287510737776756\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 275\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.712702 KL: 0.569279 Neg-log -1.281981  [    0/30000]\n",
      "current batch loss: -0.863554 KL: 0.569150 Neg-log -1.432704  [12800/30000]\n",
      "current batch loss: -0.702821 KL: 0.568995 Neg-log -1.271816  [25600/30000]\n",
      "avg train loss per batch in training: -0.670978\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.667376 KL: 0.568941 Neg-log -1.236318 MSE 0.05201798677444458\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.652610 KL: 0.568941 Neg-log -1.221552 MSE 0.053257543593645096\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 276\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.478636 KL: 0.568941 Neg-log -1.047578  [    0/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: -0.728287 KL: 0.568793 Neg-log -1.297080  [12800/30000]\n",
      "current batch loss: -1.084363 KL: 0.568643 Neg-log -1.653006  [25600/30000]\n",
      "avg train loss per batch in training: -0.677287\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.792408 KL: 0.568589 Neg-log -1.360997 MSE 0.0403677336871624\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.787551 KL: 0.568589 Neg-log -1.356141 MSE 0.04196435213088989\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 277\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.791472 KL: 0.568590 Neg-log -1.360062  [    0/30000]\n",
      "current batch loss: -0.303254 KL: 0.568446 Neg-log -0.871700  [12800/30000]\n",
      "current batch loss: -0.949480 KL: 0.568296 Neg-log -1.517776  [25600/30000]\n",
      "avg train loss per batch in training: -0.721845\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.723659 KL: 0.568243 Neg-log -1.291902 MSE 0.04453611746430397\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.676536 KL: 0.568243 Neg-log -1.244779 MSE 0.04977435991168022\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 278\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.960783 KL: 0.568243 Neg-log -1.529025  [    0/30000]\n",
      "current batch loss: -0.868597 KL: 0.568092 Neg-log -1.436689  [12800/30000]\n",
      "current batch loss: -0.983285 KL: 0.567951 Neg-log -1.551236  [25600/30000]\n",
      "avg train loss per batch in training: -0.751038\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.799284 KL: 0.567902 Neg-log -1.367186 MSE 0.03975362703204155\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.776217 KL: 0.567902 Neg-log -1.344119 MSE 0.04277878627181053\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 279\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.788042 KL: 0.567901 Neg-log -1.355943  [    0/30000]\n",
      "current batch loss: -0.875027 KL: 0.567768 Neg-log -1.442794  [12800/30000]\n",
      "current batch loss: -0.363783 KL: 0.567633 Neg-log -0.931416  [25600/30000]\n",
      "avg train loss per batch in training: -0.695808\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.199959 KL: 0.567595 Neg-log -0.367635 MSE 0.10007604211568832\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.200728 KL: 0.567595 Neg-log -0.366866 MSE 0.10255707055330276\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 280\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.499125 KL: 0.567594 Neg-log -0.068469  [    0/30000]\n",
      "current batch loss: -0.858319 KL: 0.567448 Neg-log -1.425767  [12800/30000]\n",
      "current batch loss: -0.336548 KL: 0.567302 Neg-log -0.903849  [25600/30000]\n",
      "avg train loss per batch in training: -0.653825\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.789678 KL: 0.567255 Neg-log -1.356933 MSE 0.040279749780893326\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.755916 KL: 0.567255 Neg-log -1.323171 MSE 0.04366325959563255\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 281\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.196037 KL: 0.567255 Neg-log -0.763292  [    0/30000]\n",
      "current batch loss: -0.818553 KL: 0.567118 Neg-log -1.385671  [12800/30000]\n",
      "current batch loss: 0.770336 KL: 0.566978 Neg-log 0.203357  [25600/30000]\n",
      "avg train loss per batch in training: -0.713295\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.773394 KL: 0.566934 Neg-log -1.340329 MSE 0.039929598569869995\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.794254 KL: 0.566934 Neg-log -1.361188 MSE 0.04110050946474075\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 282\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.996194 KL: 0.566935 Neg-log -1.563129  [    0/30000]\n",
      "current batch loss: -0.612855 KL: 0.566791 Neg-log -1.179646  [12800/30000]\n",
      "current batch loss: -0.588237 KL: 0.566644 Neg-log -1.154882  [25600/30000]\n",
      "avg train loss per batch in training: -0.689119\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.727998 KL: 0.566590 Neg-log -1.294588 MSE 0.043011195957660675\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.721721 KL: 0.566590 Neg-log -1.288311 MSE 0.045070748776197433\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 283\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.933150 KL: 0.566591 Neg-log -1.499740  [    0/30000]\n",
      "current batch loss: -0.736987 KL: 0.566449 Neg-log -1.303436  [12800/30000]\n",
      "current batch loss: -0.901010 KL: 0.566316 Neg-log -1.467327  [25600/30000]\n",
      "avg train loss per batch in training: -0.733060\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.794023 KL: 0.566268 Neg-log -1.360291 MSE 0.03877608850598335\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.753177 KL: 0.566268 Neg-log -1.319443 MSE 0.042024075984954834\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 284\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.891474 KL: 0.566267 Neg-log -1.457741  [    0/30000]\n",
      "current batch loss: -0.933196 KL: 0.566140 Neg-log -1.499336  [12800/30000]\n",
      "current batch loss: -0.632643 KL: 0.566005 Neg-log -1.198648  [25600/30000]\n",
      "avg train loss per batch in training: -0.718779\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.126043 KL: 0.565958 Neg-log -0.692002 MSE 0.07927333563566208\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.039017 KL: 0.565958 Neg-log -0.604976 MSE 0.08186280727386475\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 285\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.534458 KL: 0.565959 Neg-log -1.100417  [    0/30000]\n",
      "current batch loss: -0.858484 KL: 0.565823 Neg-log -1.424306  [12800/30000]\n",
      "current batch loss: -0.796893 KL: 0.565699 Neg-log -1.362592  [25600/30000]\n",
      "avg train loss per batch in training: -0.713810\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.808919 KL: 0.565657 Neg-log -1.374576 MSE 0.04103682190179825\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.792786 KL: 0.565657 Neg-log -1.358442 MSE 0.04047997668385506\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 286\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.926833 KL: 0.565657 Neg-log -1.492490  [    0/30000]\n",
      "current batch loss: -0.790822 KL: 0.565532 Neg-log -1.356353  [12800/30000]\n",
      "current batch loss: -0.714131 KL: 0.565400 Neg-log -1.279530  [25600/30000]\n",
      "avg train loss per batch in training: -0.745089\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.764126 KL: 0.565359 Neg-log -1.329484 MSE 0.043070804327726364\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.770740 KL: 0.565359 Neg-log -1.336098 MSE 0.04212869703769684\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 287\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.793359 KL: 0.565358 Neg-log -1.358718  [    0/30000]\n",
      "current batch loss: -0.086267 KL: 0.565245 Neg-log -0.651512  [12800/30000]\n",
      "current batch loss: -0.729417 KL: 0.565109 Neg-log -1.294525  [25600/30000]\n",
      "avg train loss per batch in training: -0.749457\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.772090 KL: 0.565069 Neg-log -1.337159 MSE 0.04155287891626358\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.749911 KL: 0.565069 Neg-log -1.314979 MSE 0.043438803404569626\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 288\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.888849 KL: 0.565068 Neg-log -1.453918  [    0/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: -0.666160 KL: 0.564945 Neg-log -1.231105  [12800/30000]\n",
      "current batch loss: -0.137803 KL: 0.564809 Neg-log -0.702611  [25600/30000]\n",
      "avg train loss per batch in training: -0.733027\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.767541 KL: 0.564764 Neg-log -1.332303 MSE 0.04215613380074501\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.786651 KL: 0.564764 Neg-log -1.351413 MSE 0.040776513516902924\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 289\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.061350 KL: 0.564762 Neg-log -1.626112  [    0/30000]\n",
      "current batch loss: 0.311958 KL: 0.564637 Neg-log -0.252679  [12800/30000]\n",
      "current batch loss: -1.003492 KL: 0.564510 Neg-log -1.568002  [25600/30000]\n",
      "avg train loss per batch in training: -0.716855\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.827143 KL: 0.564461 Neg-log -1.391604 MSE 0.03675468638539314\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.781398 KL: 0.564461 Neg-log -1.345858 MSE 0.042224910110235214\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 290\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.852204 KL: 0.564461 Neg-log -1.416665  [    0/30000]\n",
      "current batch loss: 0.408179 KL: 0.564343 Neg-log -0.156164  [12800/30000]\n",
      "current batch loss: -0.187143 KL: 0.564227 Neg-log -0.751371  [25600/30000]\n",
      "avg train loss per batch in training: -0.701182\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.803106 KL: 0.564186 Neg-log -1.367291 MSE 0.03960701823234558\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.778893 KL: 0.564186 Neg-log -1.343078 MSE 0.04042857885360718\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 291\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.941648 KL: 0.564185 Neg-log -1.505833  [    0/30000]\n",
      "current batch loss: -0.702412 KL: 0.564063 Neg-log -1.266475  [12800/30000]\n",
      "current batch loss: -0.720975 KL: 0.563948 Neg-log -1.284923  [25600/30000]\n",
      "avg train loss per batch in training: -0.736641\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.705834 KL: 0.563909 Neg-log -1.269743 MSE 0.050143226981163025\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.667730 KL: 0.563909 Neg-log -1.231638 MSE 0.051186151802539825\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 292\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.915129 KL: 0.563908 Neg-log -1.479037  [    0/30000]\n",
      "current batch loss: -1.005584 KL: 0.563807 Neg-log -1.569391  [12800/30000]\n",
      "current batch loss: 0.343854 KL: 0.563696 Neg-log -0.219842  [25600/30000]\n",
      "avg train loss per batch in training: -0.713421\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.815665 KL: 0.563652 Neg-log -1.379317 MSE 0.04173407331109047\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.802962 KL: 0.563652 Neg-log -1.366614 MSE 0.04144403710961342\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 293\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.963462 KL: 0.563653 Neg-log -1.527115  [    0/30000]\n",
      "current batch loss: -0.619083 KL: 0.563531 Neg-log -1.182614  [12800/30000]\n",
      "current batch loss: -0.774513 KL: 0.563418 Neg-log -1.337931  [25600/30000]\n",
      "avg train loss per batch in training: -0.722205\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.752994 KL: 0.563379 Neg-log -1.316375 MSE 0.0437602773308754\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.733092 KL: 0.563379 Neg-log -1.296473 MSE 0.045992035418748856\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 294\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.981530 KL: 0.563381 Neg-log -1.544912  [    0/30000]\n",
      "current batch loss: -1.061415 KL: 0.563270 Neg-log -1.624686  [12800/30000]\n",
      "current batch loss: -0.912470 KL: 0.563173 Neg-log -1.475643  [25600/30000]\n",
      "avg train loss per batch in training: -0.801426\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.811588 KL: 0.563141 Neg-log -1.374730 MSE 0.03848056122660637\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.803001 KL: 0.563141 Neg-log -1.366143 MSE 0.040339574217796326\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 295\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.961088 KL: 0.563141 Neg-log -1.524229  [    0/30000]\n",
      "current batch loss: -0.977172 KL: 0.563046 Neg-log -1.540218  [12800/30000]\n",
      "current batch loss: -0.644842 KL: 0.562924 Neg-log -1.207766  [25600/30000]\n",
      "avg train loss per batch in training: -0.720634\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.820869 KL: 0.562881 Neg-log -1.383749 MSE 0.03692750260233879\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.826783 KL: 0.562881 Neg-log -1.389664 MSE 0.038772258907556534\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 296\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.948801 KL: 0.562881 Neg-log -1.511682  [    0/30000]\n",
      "current batch loss: -0.822869 KL: 0.562780 Neg-log -1.385649  [12800/30000]\n",
      "current batch loss: -0.514117 KL: 0.562682 Neg-log -1.076800  [25600/30000]\n",
      "avg train loss per batch in training: -0.741250\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.826092 KL: 0.562651 Neg-log -1.388741 MSE 0.03706249222159386\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.798470 KL: 0.562651 Neg-log -1.361120 MSE 0.038734495639801025\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 297\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.993847 KL: 0.562650 Neg-log -1.556497  [    0/30000]\n",
      "current batch loss: -0.602471 KL: 0.562558 Neg-log -1.165029  [12800/30000]\n",
      "current batch loss: -1.032106 KL: 0.562470 Neg-log -1.594577  [25600/30000]\n",
      "avg train loss per batch in training: -0.745759\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.840113 KL: 0.562432 Neg-log -1.402545 MSE 0.038053564727306366\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.816308 KL: 0.562432 Neg-log -1.378740 MSE 0.03902589529752731\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 298\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.988017 KL: 0.562432 Neg-log -1.550449  [    0/30000]\n",
      "current batch loss: -0.822481 KL: 0.562352 Neg-log -1.384833  [12800/30000]\n",
      "current batch loss: -0.955738 KL: 0.562248 Neg-log -1.517986  [25600/30000]\n",
      "avg train loss per batch in training: -0.731092\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.530296 KL: 0.562218 Neg-log -1.092515 MSE 0.052418969571590424\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.485036 KL: 0.562218 Neg-log -1.047254 MSE 0.05577952414751053\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 299\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.704812 KL: 0.562218 Neg-log -1.267030  [    0/30000]\n",
      "current batch loss: -0.051426 KL: 0.562153 Neg-log -0.613580  [12800/30000]\n",
      "current batch loss: -0.778636 KL: 0.562061 Neg-log -1.340697  [25600/30000]\n",
      "avg train loss per batch in training: -0.763235\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.375066 KL: 0.562029 Neg-log -0.937097 MSE 0.038768548518419266\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.794827 KL: 0.562029 Neg-log -1.356858 MSE 0.03978373482823372\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 300\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.968656 KL: 0.562030 Neg-log -1.530686  [    0/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: -0.813445 KL: 0.561945 Neg-log -1.375390  [12800/30000]\n",
      "current batch loss: -0.580735 KL: 0.561857 Neg-log -1.142592  [25600/30000]\n",
      "avg train loss per batch in training: 263378551.169694\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.563719 KL: 0.561814 Neg-log -1.125533 MSE 0.06310909986495972\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.575631 KL: 0.561814 Neg-log -1.137446 MSE 0.0625176653265953\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 301\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.571183 KL: 0.561815 Neg-log -1.132998  [    0/30000]\n",
      "current batch loss: -0.644853 KL: 0.561709 Neg-log -1.206562  [12800/30000]\n",
      "current batch loss: -0.941400 KL: 0.561618 Neg-log -1.503019  [25600/30000]\n",
      "avg train loss per batch in training: -0.659956\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.493862 KL: 0.561590 Neg-log -1.055450 MSE 0.06191887706518173\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.427061 KL: 0.561590 Neg-log -0.988651 MSE 0.0697375237941742\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 302\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.769000 KL: 0.561589 Neg-log -1.330589  [    0/30000]\n",
      "current batch loss: -0.647715 KL: 0.561506 Neg-log -1.209221  [12800/30000]\n",
      "current batch loss: -0.750350 KL: 0.561422 Neg-log -1.311771  [25600/30000]\n",
      "avg train loss per batch in training: -0.693361\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.806601 KL: 0.561392 Neg-log -1.367990 MSE 0.044098127633333206\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.725145 KL: 0.561392 Neg-log -1.286534 MSE 0.0514000728726387\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 303\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.823452 KL: 0.561390 Neg-log -1.384842  [    0/30000]\n",
      "current batch loss: -0.717726 KL: 0.561305 Neg-log -1.279031  [12800/30000]\n",
      "current batch loss: -0.811653 KL: 0.561217 Neg-log -1.372869  [25600/30000]\n",
      "avg train loss per batch in training: -0.737025\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.741571 KL: 0.561188 Neg-log -1.302758 MSE 0.046159788966178894\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.742311 KL: 0.561188 Neg-log -1.303498 MSE 0.04679809510707855\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 304\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.461343 KL: 0.561188 Neg-log -1.022531  [    0/30000]\n",
      "current batch loss: -0.640564 KL: 0.561095 Neg-log -1.201658  [12800/30000]\n",
      "current batch loss: 0.206128 KL: 0.561004 Neg-log -0.354876  [25600/30000]\n",
      "avg train loss per batch in training: -0.438223\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.705645 KL: 0.560974 Neg-log -1.266619 MSE 0.04676060751080513\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.692982 KL: 0.560974 Neg-log -1.253957 MSE 0.04975011199712753\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 305\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.701685 KL: 0.560975 Neg-log -1.262660  [    0/30000]\n",
      "current batch loss: -0.842176 KL: 0.560893 Neg-log -1.403069  [12800/30000]\n",
      "current batch loss: -0.840600 KL: 0.560795 Neg-log -1.401395  [25600/30000]\n",
      "avg train loss per batch in training: -0.714636\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.408192 KL: 0.560766 Neg-log 0.847427 MSE 0.04601107910275459\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.724775 KL: 0.560766 Neg-log -1.285539 MSE 0.04808465763926506\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 306\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.987447 KL: 0.560764 Neg-log -1.548211  [    0/30000]\n",
      "current batch loss: -0.506873 KL: 0.560681 Neg-log -1.067554  [12800/30000]\n",
      "current batch loss: -0.943544 KL: 0.560591 Neg-log -1.504135  [25600/30000]\n",
      "avg train loss per batch in training: -0.711165\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.809023 KL: 0.560562 Neg-log -1.369583 MSE 0.040463536977767944\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.784690 KL: 0.560562 Neg-log -1.345251 MSE 0.04338658228516579\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 307\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.850497 KL: 0.560560 Neg-log -1.411058  [    0/30000]\n",
      "current batch loss: -0.682050 KL: 0.560471 Neg-log -1.242521  [12800/30000]\n",
      "current batch loss: -0.019181 KL: 0.560384 Neg-log -0.579564  [25600/30000]\n",
      "avg train loss per batch in training: -0.706885\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.713222 KL: 0.560358 Neg-log -1.273581 MSE 0.044820524752140045\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.756496 KL: 0.560358 Neg-log -1.316855 MSE 0.04484519734978676\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 308\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.743344 KL: 0.560359 Neg-log -1.303703  [    0/30000]\n",
      "current batch loss: -0.947045 KL: 0.560273 Neg-log -1.507318  [12800/30000]\n",
      "current batch loss: -0.783633 KL: 0.560186 Neg-log -1.343819  [25600/30000]\n",
      "avg train loss per batch in training: -0.773721\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.824185 KL: 0.560159 Neg-log -1.384344 MSE 0.05086182802915573\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.798957 KL: 0.560159 Neg-log -1.359118 MSE 0.043224748224020004\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 309\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.561724 KL: 0.560160 Neg-log -1.121885  [    0/30000]\n",
      "current batch loss: -0.989810 KL: 0.560074 Neg-log -1.549885  [12800/30000]\n",
      "current batch loss: -0.865196 KL: 0.559988 Neg-log -1.425184  [25600/30000]\n",
      "avg train loss per batch in training: -0.771730\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.629985 KL: 0.559957 Neg-log -1.189941 MSE 0.05131057649850845\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.635984 KL: 0.559957 Neg-log -1.195940 MSE 0.05136922374367714\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 310\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.564447 KL: 0.559956 Neg-log -1.124403  [    0/30000]\n",
      "current batch loss: -1.003918 KL: 0.559885 Neg-log -1.563803  [12800/30000]\n",
      "current batch loss: -0.251937 KL: 0.559794 Neg-log -0.811731  [25600/30000]\n",
      "avg train loss per batch in training: -0.802651\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.815402 KL: 0.559769 Neg-log -1.375170 MSE 0.039225004613399506\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.794756 KL: 0.559769 Neg-log -1.354524 MSE 0.042311158031225204\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 311\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.654396 KL: 0.559768 Neg-log -1.214163  [    0/30000]\n",
      "current batch loss: -0.801731 KL: 0.559682 Neg-log -1.361414  [12800/30000]\n",
      "current batch loss: -1.088562 KL: 0.559605 Neg-log -1.648167  [25600/30000]\n",
      "avg train loss per batch in training: -0.797568\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.682114 KL: 0.559578 Neg-log -1.241692 MSE 0.04492945969104767\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.690932 KL: 0.559578 Neg-log -1.250510 MSE 0.04972704499959946\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 312\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.688611 KL: 0.559578 Neg-log -1.248189  [    0/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: -0.839618 KL: 0.559503 Neg-log -1.399121  [12800/30000]\n",
      "current batch loss: 1.000155 KL: 0.559422 Neg-log 0.440733  [25600/30000]\n",
      "avg train loss per batch in training: -0.767605\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.842694 KL: 0.559388 Neg-log -1.402082 MSE 0.038046855479478836\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.814457 KL: 0.559388 Neg-log -1.373845 MSE 0.04128007963299751\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 313\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.629035 KL: 0.559388 Neg-log -1.188423  [    0/30000]\n",
      "current batch loss: -0.266536 KL: 0.559302 Neg-log -0.825837  [12800/30000]\n",
      "current batch loss: -0.623231 KL: 0.559219 Neg-log -1.182450  [25600/30000]\n",
      "avg train loss per batch in training: -0.749462\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.835590 KL: 0.559189 Neg-log -1.394778 MSE 0.04160945117473602\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.828807 KL: 0.559189 Neg-log -1.387995 MSE 0.0415225550532341\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 314\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.783400 KL: 0.559189 Neg-log -1.342588  [    0/30000]\n",
      "current batch loss: -0.921278 KL: 0.559096 Neg-log -1.480374  [12800/30000]\n",
      "current batch loss: -1.048538 KL: 0.559008 Neg-log -1.607546  [25600/30000]\n",
      "avg train loss per batch in training: -0.820983\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.835316 KL: 0.558984 Neg-log -1.394301 MSE 0.044941384345293045\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.840392 KL: 0.558984 Neg-log -1.399377 MSE 0.03927701711654663\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 315\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.908907 KL: 0.558985 Neg-log -1.467892  [    0/30000]\n",
      "current batch loss: -0.261422 KL: 0.558901 Neg-log -0.820323  [12800/30000]\n",
      "current batch loss: -1.049166 KL: 0.558816 Neg-log -1.607982  [25600/30000]\n",
      "avg train loss per batch in training: -0.721959\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.812550 KL: 0.558786 Neg-log -1.371338 MSE 0.04084392264485359\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.819045 KL: 0.558786 Neg-log -1.377833 MSE 0.03989824652671814\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 316\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.003440 KL: 0.558787 Neg-log -1.562228  [    0/30000]\n",
      "current batch loss: -0.910818 KL: 0.558708 Neg-log -1.469526  [12800/30000]\n",
      "current batch loss: -0.734925 KL: 0.558629 Neg-log -1.293555  [25600/30000]\n",
      "avg train loss per batch in training: -0.735207\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.841575 KL: 0.558595 Neg-log -1.400170 MSE 0.03825724497437477\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.817247 KL: 0.558595 Neg-log -1.375843 MSE 0.040820736438035965\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 317\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.615286 KL: 0.558596 Neg-log 0.056690  [    0/30000]\n",
      "current batch loss: -0.976084 KL: 0.558506 Neg-log -1.534590  [12800/30000]\n",
      "current batch loss: -1.035608 KL: 0.558419 Neg-log -1.594027  [25600/30000]\n",
      "avg train loss per batch in training: -0.781606\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.630751 KL: 0.558382 Neg-log -1.189134 MSE 0.04735514149069786\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.643883 KL: 0.558382 Neg-log -1.202265 MSE 0.04947996884584427\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 318\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.461957 KL: 0.558382 Neg-log -1.020339  [    0/30000]\n",
      "current batch loss: -0.442118 KL: 0.558307 Neg-log -1.000425  [12800/30000]\n",
      "current batch loss: -0.931220 KL: 0.558218 Neg-log -1.489438  [25600/30000]\n",
      "avg train loss per batch in training: -0.763283\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.834846 KL: 0.558196 Neg-log -1.393040 MSE 0.04106668755412102\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.790262 KL: 0.558196 Neg-log -1.348455 MSE 0.042144566774368286\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 319\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.965546 KL: 0.558194 Neg-log -1.523741  [    0/30000]\n",
      "current batch loss: -0.299376 KL: 0.558104 Neg-log -0.857481  [12800/30000]\n",
      "current batch loss: -0.700516 KL: 0.558020 Neg-log -1.258536  [25600/30000]\n",
      "avg train loss per batch in training: -0.719530\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.801263 KL: 0.557997 Neg-log -1.359259 MSE 0.03884798660874367\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.782215 KL: 0.557997 Neg-log -1.340211 MSE 0.04190998896956444\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 320\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.780954 KL: 0.557997 Neg-log -1.338950  [    0/30000]\n",
      "current batch loss: -0.924877 KL: 0.557916 Neg-log -1.482793  [12800/30000]\n",
      "current batch loss: -0.848117 KL: 0.557838 Neg-log -1.405954  [25600/30000]\n",
      "avg train loss per batch in training: -0.770522\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.873375 KL: 0.557806 Neg-log -1.431180 MSE 0.04050225764513016\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.837136 KL: 0.557806 Neg-log -1.394941 MSE 0.038877274841070175\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 321\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.934563 KL: 0.557805 Neg-log -1.492368  [    0/30000]\n",
      "current batch loss: -0.735933 KL: 0.557715 Neg-log -1.293649  [12800/30000]\n",
      "current batch loss: -0.971277 KL: 0.557635 Neg-log -1.528911  [25600/30000]\n",
      "avg train loss per batch in training: -0.827792\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.834857 KL: 0.557609 Neg-log -1.392465 MSE 0.03819180652499199\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.859551 KL: 0.557609 Neg-log -1.417160 MSE 0.03870423510670662\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 322\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.887160 KL: 0.557608 Neg-log -1.444768  [    0/30000]\n",
      "current batch loss: -1.037510 KL: 0.557531 Neg-log -1.595042  [12800/30000]\n",
      "current batch loss: -0.995477 KL: 0.557455 Neg-log -1.552932  [25600/30000]\n",
      "avg train loss per batch in training: -0.780697\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.825647 KL: 0.557428 Neg-log -1.383076 MSE 0.04332083463668823\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.782433 KL: 0.557428 Neg-log -1.339862 MSE 0.04178436100482941\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 323\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.987454 KL: 0.557429 Neg-log -1.544884  [    0/30000]\n",
      "current batch loss: -0.092191 KL: 0.557349 Neg-log -0.649540  [12800/30000]\n",
      "current batch loss: -0.517338 KL: 0.557265 Neg-log -1.074602  [25600/30000]\n",
      "avg train loss per batch in training: -0.781409\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.678462 KL: 0.557237 Neg-log -1.235700 MSE 0.04478836432099342\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.672152 KL: 0.557237 Neg-log -1.229389 MSE 0.04847562685608864\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 324\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.960477 KL: 0.557238 Neg-log -1.517715  [    0/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: -0.597944 KL: 0.557156 Neg-log -1.155101  [12800/30000]\n",
      "current batch loss: -0.815032 KL: 0.557072 Neg-log -1.372104  [25600/30000]\n",
      "avg train loss per batch in training: -0.808942\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.786978 KL: 0.557045 Neg-log -1.344023 MSE 0.041918687522411346\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.788458 KL: 0.557045 Neg-log -1.345504 MSE 0.040881745517253876\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 325\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.027219 KL: 0.557046 Neg-log -1.584265  [    0/30000]\n",
      "current batch loss: -0.690678 KL: 0.556976 Neg-log -1.247654  [12800/30000]\n",
      "current batch loss: -0.980962 KL: 0.556907 Neg-log -1.537869  [25600/30000]\n",
      "avg train loss per batch in training: -0.812037\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.863224 KL: 0.556879 Neg-log -1.420105 MSE 0.03848176449537277\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.879660 KL: 0.556879 Neg-log -1.436540 MSE 0.03756779059767723\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 326\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.868929 KL: 0.556880 Neg-log -1.425809  [    0/30000]\n",
      "current batch loss: -0.923149 KL: 0.556812 Neg-log -1.479961  [12800/30000]\n",
      "current batch loss: -0.449301 KL: 0.556735 Neg-log -1.006036  [25600/30000]\n",
      "avg train loss per batch in training: -0.799953\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.863181 KL: 0.556709 Neg-log -1.419890 MSE 0.039435334503650665\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.852214 KL: 0.556709 Neg-log -1.408924 MSE 0.03911759331822395\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 327\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.403976 KL: 0.556709 Neg-log -0.960685  [    0/30000]\n",
      "current batch loss: -0.670284 KL: 0.556633 Neg-log -1.226917  [12800/30000]\n",
      "current batch loss: -0.926512 KL: 0.556569 Neg-log -1.483081  [25600/30000]\n",
      "avg train loss per batch in training: -0.788506\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.165603 KL: 0.556547 Neg-log -0.722149 MSE 0.07363692671060562\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.070988 KL: 0.556547 Neg-log -0.627533 MSE 0.07763883471488953\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 328\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.408652 KL: 0.556546 Neg-log -0.965198  [    0/30000]\n",
      "current batch loss: -1.090236 KL: 0.556481 Neg-log -1.646717  [12800/30000]\n",
      "current batch loss: -0.617214 KL: 0.556404 Neg-log -1.173618  [25600/30000]\n",
      "avg train loss per batch in training: -0.784615\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.808793 KL: 0.556387 Neg-log -1.365179 MSE 0.0385030135512352\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.767066 KL: 0.556387 Neg-log -1.323451 MSE 0.04258473217487335\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 329\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.673784 KL: 0.556386 Neg-log -1.230169  [    0/30000]\n",
      "current batch loss: -0.932919 KL: 0.556313 Neg-log -1.489232  [12800/30000]\n",
      "current batch loss: -1.144990 KL: 0.556237 Neg-log -1.701226  [25600/30000]\n",
      "avg train loss per batch in training: -0.794333\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.892694 KL: 0.556206 Neg-log -1.448901 MSE 0.03478941693902016\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.879098 KL: 0.556206 Neg-log -1.435305 MSE 0.03875358775258064\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 330\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.684313 KL: 0.556207 Neg-log -1.240521  [    0/30000]\n",
      "current batch loss: -0.745025 KL: 0.556145 Neg-log -1.301170  [12800/30000]\n",
      "current batch loss: -1.055973 KL: 0.556071 Neg-log -1.612044  [25600/30000]\n",
      "avg train loss per batch in training: -0.770137\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.732518 KL: 0.556044 Neg-log -1.288561 MSE 0.043413642793893814\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.694099 KL: 0.556044 Neg-log -1.250143 MSE 0.04762287437915802\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 331\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.579429 KL: 0.556043 Neg-log -1.135473  [    0/30000]\n",
      "current batch loss: -1.027999 KL: 0.555976 Neg-log -1.583975  [12800/30000]\n",
      "current batch loss: -0.911042 KL: 0.555906 Neg-log -1.466948  [25600/30000]\n",
      "avg train loss per batch in training: -0.773905\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.741850 KL: 0.555883 Neg-log -1.297732 MSE 0.04767962545156479\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.702715 KL: 0.555883 Neg-log -1.258597 MSE 0.052775099873542786\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 332\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.933352 KL: 0.555882 Neg-log -1.489233  [    0/30000]\n",
      "current batch loss: -1.010011 KL: 0.555817 Neg-log -1.565828  [12800/30000]\n",
      "current batch loss: -1.020687 KL: 0.555754 Neg-log -1.576442  [25600/30000]\n",
      "avg train loss per batch in training: -0.819244\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.573345 KL: 0.555732 Neg-log -1.129077 MSE 0.05094451084733009\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.551382 KL: 0.555732 Neg-log -1.107114 MSE 0.053346410393714905\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 333\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.516961 KL: 0.555732 Neg-log -1.072693  [    0/30000]\n",
      "current batch loss: -0.619982 KL: 0.555662 Neg-log -1.175644  [12800/30000]\n",
      "current batch loss: -1.059902 KL: 0.555589 Neg-log -1.615491  [25600/30000]\n",
      "avg train loss per batch in training: -0.787218\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.866525 KL: 0.555566 Neg-log -1.422092 MSE 0.03738807514309883\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.841055 KL: 0.555566 Neg-log -1.396622 MSE 0.038751233369112015\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 334\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.074533 KL: 0.555568 Neg-log -1.630100  [    0/30000]\n",
      "current batch loss: -0.907312 KL: 0.555506 Neg-log -1.462818  [12800/30000]\n",
      "current batch loss: -0.952224 KL: 0.555453 Neg-log -1.507677  [25600/30000]\n",
      "avg train loss per batch in training: -0.795634\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.873686 KL: 0.555428 Neg-log -1.429115 MSE 0.03538002446293831\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.860000 KL: 0.555428 Neg-log -1.415429 MSE 0.03677794709801674\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 335\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.960981 KL: 0.555429 Neg-log -1.516410  [    0/30000]\n",
      "current batch loss: -0.977890 KL: 0.555376 Neg-log -1.533266  [12800/30000]\n",
      "current batch loss: -0.914659 KL: 0.555311 Neg-log -1.469971  [25600/30000]\n",
      "avg train loss per batch in training: -0.866120\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.895558 KL: 0.555291 Neg-log -1.450849 MSE 0.03489150106906891\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.592864 KL: 0.555291 Neg-log -1.148155 MSE 0.04048522189259529\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 336\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.062960 KL: 0.555292 Neg-log -1.618252  [    0/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: -0.616405 KL: 0.555237 Neg-log -1.171641  [12800/30000]\n",
      "current batch loss: -0.816012 KL: 0.555164 Neg-log -1.371176  [25600/30000]\n",
      "avg train loss per batch in training: -0.785383\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.821185 KL: 0.555143 Neg-log -1.376328 MSE 0.04078301414847374\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.827672 KL: 0.555143 Neg-log -1.382814 MSE 0.03958793357014656\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 337\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.871069 KL: 0.555142 Neg-log -1.426212  [    0/30000]\n",
      "current batch loss: -0.971046 KL: 0.555087 Neg-log -1.526133  [12800/30000]\n",
      "current batch loss: -0.653543 KL: 0.555014 Neg-log -1.208557  [25600/30000]\n",
      "avg train loss per batch in training: -0.808156\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.854596 KL: 0.554994 Neg-log -1.409591 MSE 0.038693442940711975\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.839421 KL: 0.554994 Neg-log -1.394416 MSE 0.03876204416155815\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 338\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.026006 KL: 0.554995 Neg-log -1.581001  [    0/30000]\n",
      "current batch loss: -1.131498 KL: 0.554942 Neg-log -1.686440  [12800/30000]\n",
      "current batch loss: -1.114149 KL: 0.554872 Neg-log -1.669021  [25600/30000]\n",
      "avg train loss per batch in training: -0.859873\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.897143 KL: 0.554848 Neg-log -1.451991 MSE 0.0337773934006691\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.903745 KL: 0.554848 Neg-log -1.458593 MSE 0.035522107034921646\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 339\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.091051 KL: 0.554848 Neg-log -1.645899  [    0/30000]\n",
      "current batch loss: -1.095108 KL: 0.554783 Neg-log -1.649890  [12800/30000]\n",
      "current batch loss: -0.819758 KL: 0.554719 Neg-log -1.374478  [25600/30000]\n",
      "avg train loss per batch in training: -0.849163\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.679914 KL: 0.554703 Neg-log -1.234618 MSE 0.04881015419960022\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.729496 KL: 0.554703 Neg-log -1.284199 MSE 0.04394536837935448\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 340\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.236898 KL: 0.554703 Neg-log -0.317805  [    0/30000]\n",
      "current batch loss: -0.704795 KL: 0.554643 Neg-log -1.259438  [12800/30000]\n",
      "current batch loss: -1.061289 KL: 0.554586 Neg-log -1.615874  [25600/30000]\n",
      "avg train loss per batch in training: -0.792641\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.853376 KL: 0.554566 Neg-log -1.407942 MSE 0.036625321954488754\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.842464 KL: 0.554566 Neg-log -1.397031 MSE 0.03911758214235306\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 341\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.013683 KL: 0.554567 Neg-log -1.568249  [    0/30000]\n",
      "current batch loss: -0.993673 KL: 0.554509 Neg-log -1.548182  [12800/30000]\n",
      "current batch loss: -0.756310 KL: 0.554438 Neg-log -1.310748  [25600/30000]\n",
      "avg train loss per batch in training: -0.825052\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.851642 KL: 0.554415 Neg-log -1.406058 MSE 0.036221977323293686\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.879538 KL: 0.554415 Neg-log -1.433953 MSE 0.037696730345487595\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 342\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.985489 KL: 0.554416 Neg-log -1.539905  [    0/30000]\n",
      "current batch loss: -1.122076 KL: 0.554359 Neg-log -1.676435  [12800/30000]\n",
      "current batch loss: -0.984014 KL: 0.554294 Neg-log -1.538308  [25600/30000]\n",
      "avg train loss per batch in training: -0.828405\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.907353 KL: 0.554274 Neg-log -1.461626 MSE 0.035345934331417084\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.889810 KL: 0.554274 Neg-log -1.444082 MSE 0.036825962364673615\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 343\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.981387 KL: 0.554273 Neg-log -1.535659  [    0/30000]\n",
      "current batch loss: -0.842347 KL: 0.554211 Neg-log -1.396558  [12800/30000]\n",
      "current batch loss: -0.565572 KL: 0.554153 Neg-log -1.119726  [25600/30000]\n",
      "avg train loss per batch in training: -0.868960\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.918543 KL: 0.554138 Neg-log -1.472681 MSE 0.035350386053323746\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.894694 KL: 0.554138 Neg-log -1.448831 MSE 0.03538624942302704\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 344\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.736133 KL: 0.554138 Neg-log -1.290271  [    0/30000]\n",
      "current batch loss: -0.993751 KL: 0.554073 Neg-log -1.547823  [12800/30000]\n",
      "current batch loss: -0.351269 KL: 0.554008 Neg-log -0.905278  [25600/30000]\n",
      "avg train loss per batch in training: -0.763704\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.883182 KL: 0.553984 Neg-log -1.437164 MSE 0.03519000858068466\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.875259 KL: 0.553984 Neg-log -1.429241 MSE 0.03678840398788452\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 345\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.880414 KL: 0.553982 Neg-log -1.434397  [    0/30000]\n",
      "current batch loss: -0.134832 KL: 0.553913 Neg-log -0.688745  [12800/30000]\n",
      "current batch loss: 0.239745 KL: 0.553867 Neg-log -0.314122  [25600/30000]\n",
      "avg train loss per batch in training: -0.842669\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.797145 KL: 0.553850 Neg-log -1.350996 MSE 0.04124882072210312\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.764214 KL: 0.553850 Neg-log -1.318065 MSE 0.044096916913986206\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 346\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.987569 KL: 0.553851 Neg-log -1.541420  [    0/30000]\n",
      "current batch loss: -1.199290 KL: 0.553801 Neg-log -1.753091  [12800/30000]\n",
      "current batch loss: -0.666538 KL: 0.553747 Neg-log -1.220284  [25600/30000]\n",
      "avg train loss per batch in training: -0.806286\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.864782 KL: 0.553719 Neg-log -1.418502 MSE 0.03724360838532448\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.868019 KL: 0.553719 Neg-log -1.421739 MSE 0.03703799843788147\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 347\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.755731 KL: 0.553720 Neg-log -1.309451  [    0/30000]\n",
      "current batch loss: -1.096085 KL: 0.553666 Neg-log -1.649751  [12800/30000]\n",
      "current batch loss: -0.256680 KL: 0.553603 Neg-log -0.810283  [25600/30000]\n",
      "avg train loss per batch in training: -0.817085\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.879624 KL: 0.553581 Neg-log -1.433204 MSE 0.036074936389923096\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.880450 KL: 0.553581 Neg-log -1.434031 MSE 0.03427496924996376\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 348\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.076877 KL: 0.553581 Neg-log -1.630457  [    0/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: -0.792932 KL: 0.553509 Neg-log -1.346441  [12800/30000]\n",
      "current batch loss: -1.018845 KL: 0.553453 Neg-log -1.572298  [25600/30000]\n",
      "avg train loss per batch in training: -0.839897\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.896785 KL: 0.553431 Neg-log -1.450217 MSE 0.034954044967889786\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.827670 KL: 0.553431 Neg-log -1.381102 MSE 0.038315724581480026\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 349\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.688799 KL: 0.553432 Neg-log -1.242231  [    0/30000]\n",
      "current batch loss: -0.886562 KL: 0.553369 Neg-log -1.439930  [12800/30000]\n",
      "current batch loss: -0.741287 KL: 0.553317 Neg-log -1.294604  [25600/30000]\n",
      "avg train loss per batch in training: -0.803802\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.838942 KL: 0.553299 Neg-log -1.392242 MSE 0.03942213952541351\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.829443 KL: 0.553299 Neg-log -1.382743 MSE 0.039714422076940536\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 350\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.862638 KL: 0.553301 Neg-log -1.415939  [    0/30000]\n",
      "current batch loss: -0.828790 KL: 0.553244 Neg-log -1.382034  [12800/30000]\n",
      "current batch loss: -0.163021 KL: 0.553187 Neg-log -0.716209  [25600/30000]\n",
      "avg train loss per batch in training: -0.845094\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.879330 KL: 0.553162 Neg-log -1.432493 MSE 0.03981182724237442\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.838242 KL: 0.553162 Neg-log -1.391404 MSE 0.03741179779171944\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 351\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.997717 KL: 0.553162 Neg-log -1.550879  [    0/30000]\n",
      "current batch loss: -0.713220 KL: 0.553120 Neg-log -1.266340  [12800/30000]\n",
      "current batch loss: 0.812255 KL: 0.553066 Neg-log 0.259189  [25600/30000]\n",
      "avg train loss per batch in training: -0.802053\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.892427 KL: 0.553041 Neg-log -1.445470 MSE 0.03598998114466667\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.881854 KL: 0.553041 Neg-log -1.434897 MSE 0.037211306393146515\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 352\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.835473 KL: 0.553043 Neg-log -1.388516  [    0/30000]\n",
      "current batch loss: -0.840814 KL: 0.552989 Neg-log -1.393803  [12800/30000]\n",
      "current batch loss: -1.041311 KL: 0.552937 Neg-log -1.594248  [25600/30000]\n",
      "avg train loss per batch in training: -0.825133\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.551036 KL: 0.552912 Neg-log -1.103949 MSE 0.049975812435150146\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.573881 KL: 0.552912 Neg-log -1.126795 MSE 0.049264125525951385\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 353\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.304563 KL: 0.552914 Neg-log -0.857477  [    0/30000]\n",
      "current batch loss: -1.081341 KL: 0.552843 Neg-log -1.634184  [12800/30000]\n",
      "current batch loss: -1.126846 KL: 0.552799 Neg-log -1.679645  [25600/30000]\n",
      "avg train loss per batch in training: -0.817879\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.923831 KL: 0.552773 Neg-log -1.476604 MSE 0.03235017880797386\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.880121 KL: 0.552773 Neg-log -1.432894 MSE 0.0367705300450325\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 354\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.948782 KL: 0.552773 Neg-log -1.501555  [    0/30000]\n",
      "current batch loss: -0.990246 KL: 0.552714 Neg-log -1.542960  [12800/30000]\n",
      "current batch loss: -1.066314 KL: 0.552661 Neg-log -1.618975  [25600/30000]\n",
      "avg train loss per batch in training: -0.881136\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.852531 KL: 0.552637 Neg-log -1.405168 MSE 0.038027916103601456\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.879368 KL: 0.552637 Neg-log -1.432006 MSE 0.038728244602680206\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 355\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.537885 KL: 0.552637 Neg-log -0.014752  [    0/30000]\n",
      "current batch loss: -0.543099 KL: 0.552589 Neg-log -1.095688  [12800/30000]\n",
      "current batch loss: -1.094873 KL: 0.552532 Neg-log -1.647405  [25600/30000]\n",
      "avg train loss per batch in training: -0.824743\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.867584 KL: 0.552513 Neg-log -1.420094 MSE 0.04124238342046738\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.832243 KL: 0.552513 Neg-log -1.384754 MSE 0.04498685151338577\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 356\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.807235 KL: 0.552511 Neg-log -1.359746  [    0/30000]\n",
      "current batch loss: -0.522945 KL: 0.552451 Neg-log -1.075396  [12800/30000]\n",
      "current batch loss: -0.955839 KL: 0.552388 Neg-log -1.508227  [25600/30000]\n",
      "avg train loss per batch in training: -0.822389\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.897971 KL: 0.552370 Neg-log -1.450343 MSE 0.037410784512758255\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.909722 KL: 0.552370 Neg-log -1.462094 MSE 0.03591451421380043\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 357\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.326364 KL: 0.552371 Neg-log -0.878736  [    0/30000]\n",
      "current batch loss: -0.550332 KL: 0.552316 Neg-log -1.102648  [12800/30000]\n",
      "current batch loss: -0.947545 KL: 0.552241 Neg-log -1.499786  [25600/30000]\n",
      "avg train loss per batch in training: -0.865685\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.910556 KL: 0.552214 Neg-log -1.462770 MSE 0.03336017206311226\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.925028 KL: 0.552214 Neg-log -1.477242 MSE 0.03419522941112518\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 358\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.054064 KL: 0.552213 Neg-log -1.606277  [    0/30000]\n",
      "current batch loss: -0.623752 KL: 0.552168 Neg-log -1.175920  [12800/30000]\n",
      "current batch loss: -0.980785 KL: 0.552114 Neg-log -1.532900  [25600/30000]\n",
      "avg train loss per batch in training: -0.893727\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.758433 KL: 0.552095 Neg-log -1.310530 MSE 0.040507007390260696\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.700055 KL: 0.552095 Neg-log -1.252152 MSE 0.04627859592437744\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 359\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.734398 KL: 0.552097 Neg-log -1.286495  [    0/30000]\n",
      "current batch loss: -0.994350 KL: 0.552041 Neg-log -1.546392  [12800/30000]\n",
      "current batch loss: -0.960203 KL: 0.551985 Neg-log -1.512188  [25600/30000]\n",
      "avg train loss per batch in training: -0.832843\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.809277 KL: 0.551962 Neg-log -1.361237 MSE 0.04024612903594971\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.836185 KL: 0.551962 Neg-log -1.388146 MSE 0.037050455808639526\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 360\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.016052 KL: 0.551960 Neg-log -1.568012  [    0/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: -1.034887 KL: 0.551889 Neg-log -1.586775  [12800/30000]\n",
      "current batch loss: -0.120372 KL: 0.551820 Neg-log -0.672192  [25600/30000]\n",
      "avg train loss per batch in training: -0.858478\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.857625 KL: 0.551798 Neg-log -1.409425 MSE 0.03748182952404022\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.824057 KL: 0.551798 Neg-log -1.375856 MSE 0.04010710492730141\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 361\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.975349 KL: 0.551799 Neg-log -1.527148  [    0/30000]\n",
      "current batch loss: -0.458798 KL: 0.551715 Neg-log -1.010513  [12800/30000]\n",
      "current batch loss: -1.103491 KL: 0.551641 Neg-log -1.655132  [25600/30000]\n",
      "avg train loss per batch in training: -0.817947\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.910393 KL: 0.551614 Neg-log -1.462009 MSE 0.03425954282283783\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.896092 KL: 0.551614 Neg-log -1.447708 MSE 0.03412405773997307\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 362\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.016305 KL: 0.551616 Neg-log -1.567921  [    0/30000]\n",
      "current batch loss: -0.887532 KL: 0.551552 Neg-log -1.439084  [12800/30000]\n",
      "current batch loss: -0.901815 KL: 0.551469 Neg-log -1.453284  [25600/30000]\n",
      "avg train loss per batch in training: -0.876091\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.852790 KL: 0.551447 Neg-log -1.404239 MSE 0.03706703707575798\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.841867 KL: 0.551447 Neg-log -1.393315 MSE 0.03773827850818634\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 363\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.089796 KL: 0.551449 Neg-log -1.641245  [    0/30000]\n",
      "current batch loss: -0.785093 KL: 0.551384 Neg-log -1.336477  [12800/30000]\n",
      "current batch loss: -0.963450 KL: 0.551332 Neg-log -1.514782  [25600/30000]\n",
      "avg train loss per batch in training: -0.864839\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.898620 KL: 0.551306 Neg-log -1.449925 MSE 0.033405549824237823\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.913627 KL: 0.551306 Neg-log -1.464933 MSE 0.03459128737449646\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 364\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.952854 KL: 0.551306 Neg-log -1.504160  [    0/30000]\n",
      "current batch loss: -1.012719 KL: 0.551246 Neg-log -1.563965  [12800/30000]\n",
      "current batch loss: -1.204858 KL: 0.551194 Neg-log -1.756052  [25600/30000]\n",
      "avg train loss per batch in training: -0.903867\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.700818 KL: 0.551178 Neg-log -1.251996 MSE 0.04155215993523598\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.738271 KL: 0.551178 Neg-log -1.289448 MSE 0.03933965042233467\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 365\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.002126 KL: 0.551178 Neg-log -1.553303  [    0/30000]\n",
      "current batch loss: -0.536526 KL: 0.551116 Neg-log -1.087641  [12800/30000]\n",
      "current batch loss: -1.163238 KL: 0.551060 Neg-log -1.714298  [25600/30000]\n",
      "avg train loss per batch in training: -0.877157\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.873458 KL: 0.551042 Neg-log -1.424502 MSE 0.03594791144132614\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.861833 KL: 0.551042 Neg-log -1.412877 MSE 0.03742707148194313\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 366\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.000380 KL: 0.551044 Neg-log -1.551424  [    0/30000]\n",
      "current batch loss: -0.833568 KL: 0.550992 Neg-log -1.384560  [12800/30000]\n",
      "current batch loss: -0.501758 KL: 0.550931 Neg-log -1.052689  [25600/30000]\n",
      "avg train loss per batch in training: -0.850923\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.905116 KL: 0.550911 Neg-log -1.456028 MSE 0.03253153711557388\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.896889 KL: 0.550911 Neg-log -1.447801 MSE 0.03519902378320694\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 367\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.916874 KL: 0.550912 Neg-log -1.467786  [    0/30000]\n",
      "current batch loss: -1.040060 KL: 0.550852 Neg-log -1.590912  [12800/30000]\n",
      "current batch loss: -0.939651 KL: 0.550790 Neg-log -1.490441  [25600/30000]\n",
      "avg train loss per batch in training: -0.856504\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.905052 KL: 0.550767 Neg-log -1.455821 MSE 0.03336133807897568\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.914990 KL: 0.550767 Neg-log -1.465758 MSE 0.03325518220663071\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 368\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.284404 KL: 0.550768 Neg-log -1.835173  [    0/30000]\n",
      "current batch loss: -1.033645 KL: 0.550701 Neg-log -1.584347  [12800/30000]\n",
      "current batch loss: -0.691383 KL: 0.550632 Neg-log -1.242015  [25600/30000]\n",
      "avg train loss per batch in training: -0.855207\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.570059 KL: 0.550613 Neg-log -1.120671 MSE 0.05174989625811577\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.551246 KL: 0.550613 Neg-log -1.101858 MSE 0.05461985990405083\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 369\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.947180 KL: 0.550612 Neg-log -1.497792  [    0/30000]\n",
      "current batch loss: -1.100708 KL: 0.550550 Neg-log -1.651259  [12800/30000]\n",
      "current batch loss: -1.148661 KL: 0.550483 Neg-log -1.699144  [25600/30000]\n",
      "avg train loss per batch in training: -0.860959\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1080.788804 KL: 0.550462 Neg-log 1080.238281 MSE 0.05911487713456154\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.707915 KL: 0.550462 Neg-log -1.258378 MSE 0.04355161264538765\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 370\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.916298 KL: 0.550463 Neg-log -1.466761  [    0/30000]\n",
      "current batch loss: -1.001705 KL: 0.550402 Neg-log -1.552107  [12800/30000]\n",
      "current batch loss: -1.033399 KL: 0.550335 Neg-log -1.583733  [25600/30000]\n",
      "avg train loss per batch in training: -0.813993\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.962900 KL: 0.550314 Neg-log -1.513213 MSE 0.031069444492459297\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.933322 KL: 0.550314 Neg-log -1.483635 MSE 0.03440435603260994\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 371\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.109460 KL: 0.550313 Neg-log -1.659773  [    0/30000]\n",
      "current batch loss: -1.182909 KL: 0.550243 Neg-log -1.733153  [12800/30000]\n",
      "current batch loss: -1.109439 KL: 0.550189 Neg-log -1.659628  [25600/30000]\n",
      "avg train loss per batch in training: -0.867574\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.876345 KL: 0.550171 Neg-log -1.426515 MSE 0.03497007116675377\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.870382 KL: 0.550171 Neg-log -1.420553 MSE 0.03856688737869263\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 372\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.858026 KL: 0.550171 Neg-log -1.408197  [    0/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: -0.871265 KL: 0.550110 Neg-log -1.421375  [12800/30000]\n",
      "current batch loss: -0.611053 KL: 0.550054 Neg-log -1.161106  [25600/30000]\n",
      "avg train loss per batch in training: -0.909154\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.941286 KL: 0.550041 Neg-log -1.491325 MSE 0.03216443210840225\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.925203 KL: 0.550041 Neg-log -1.475243 MSE 0.03479775786399841\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 373\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.060078 KL: 0.550039 Neg-log -1.610117  [    0/30000]\n",
      "current batch loss: 0.378849 KL: 0.549991 Neg-log -0.171142  [12800/30000]\n",
      "current batch loss: -0.783977 KL: 0.549942 Neg-log -1.333919  [25600/30000]\n",
      "avg train loss per batch in training: -0.890386\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.968850 KL: 0.549919 Neg-log -1.518768 MSE 0.030300728976726532\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.933420 KL: 0.549919 Neg-log -1.483339 MSE 0.033017922192811966\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 374\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.968345 KL: 0.549919 Neg-log -1.518264  [    0/30000]\n",
      "current batch loss: -0.807775 KL: 0.549860 Neg-log -1.357635  [12800/30000]\n",
      "current batch loss: -0.865441 KL: 0.549807 Neg-log -1.415247  [25600/30000]\n",
      "avg train loss per batch in training: -0.893753\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.834472 KL: 0.549789 Neg-log -1.384260 MSE 0.03801575303077698\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.757026 KL: 0.549789 Neg-log -1.306814 MSE 0.04283133149147034\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 375\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.532999 KL: 0.549788 Neg-log -1.082787  [    0/30000]\n",
      "current batch loss: -1.211034 KL: 0.549715 Neg-log -1.760750  [12800/30000]\n",
      "current batch loss: -1.043813 KL: 0.549652 Neg-log -1.593465  [25600/30000]\n",
      "avg train loss per batch in training: -0.922111\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.868356 KL: 0.549638 Neg-log -1.417994 MSE 0.03819474205374718\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.856272 KL: 0.549638 Neg-log -1.405911 MSE 0.03708124905824661\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 376\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.057591 KL: 0.549639 Neg-log -1.607231  [    0/30000]\n",
      "current batch loss: -1.078993 KL: 0.549573 Neg-log -1.628566  [12800/30000]\n",
      "current batch loss: -1.086771 KL: 0.549501 Neg-log -1.636271  [25600/30000]\n",
      "avg train loss per batch in training: -0.695645\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.903964 KL: 0.549484 Neg-log -1.453448 MSE 0.03311842679977417\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.875964 KL: 0.549484 Neg-log -1.425447 MSE 0.036088455468416214\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 377\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.026007 KL: 0.549484 Neg-log -1.575491  [    0/30000]\n",
      "current batch loss: -1.123149 KL: 0.549420 Neg-log -1.672569  [12800/30000]\n",
      "current batch loss: -0.831773 KL: 0.549375 Neg-log -1.381148  [25600/30000]\n",
      "avg train loss per batch in training: -0.867362\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.914337 KL: 0.549362 Neg-log -1.463699 MSE 0.0331808403134346\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.873519 KL: 0.549362 Neg-log -1.422881 MSE 0.035515353083610535\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 378\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.687622 KL: 0.549362 Neg-log -1.236984  [    0/30000]\n",
      "current batch loss: -1.146443 KL: 0.549313 Neg-log -1.695756  [12800/30000]\n",
      "current batch loss: -1.115596 KL: 0.549268 Neg-log -1.664864  [25600/30000]\n",
      "avg train loss per batch in training: -0.870364\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.881111 KL: 0.549248 Neg-log -1.430360 MSE 0.03658360615372658\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.842270 KL: 0.549248 Neg-log -1.391519 MSE 0.039486803114414215\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 379\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.040725 KL: 0.549249 Neg-log -1.589974  [    0/30000]\n",
      "current batch loss: -1.122939 KL: 0.549188 Neg-log -1.672127  [12800/30000]\n",
      "current batch loss: -0.728255 KL: 0.549139 Neg-log -1.277394  [25600/30000]\n",
      "avg train loss per batch in training: -0.865990\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.940034 KL: 0.549120 Neg-log -1.489155 MSE 0.032854992896318436\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.919605 KL: 0.549120 Neg-log -1.468726 MSE 0.03474182263016701\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 380\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.447189 KL: 0.549121 Neg-log -0.996310  [    0/30000]\n",
      "current batch loss: -1.132862 KL: 0.549070 Neg-log -1.681932  [12800/30000]\n",
      "current batch loss: -1.076354 KL: 0.549015 Neg-log -1.625369  [25600/30000]\n",
      "avg train loss per batch in training: -0.856119\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.882468 KL: 0.548996 Neg-log -1.431465 MSE 0.03493184223771095\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.825544 KL: 0.548996 Neg-log -1.374541 MSE 0.03737326338887215\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 381\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.789194 KL: 0.548996 Neg-log -1.338190  [    0/30000]\n",
      "current batch loss: -0.715852 KL: 0.548943 Neg-log -1.264795  [12800/30000]\n",
      "current batch loss: -0.806512 KL: 0.548882 Neg-log -1.355394  [25600/30000]\n",
      "avg train loss per batch in training: -0.823963\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.874420 KL: 0.548858 Neg-log -1.423278 MSE 0.03559299185872078\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.880397 KL: 0.548858 Neg-log -1.429256 MSE 0.03626042976975441\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 382\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.091241 KL: 0.548858 Neg-log -1.640099  [    0/30000]\n",
      "current batch loss: -1.149798 KL: 0.548795 Neg-log -1.698593  [12800/30000]\n",
      "current batch loss: -0.116945 KL: 0.548742 Neg-log -0.665687  [25600/30000]\n",
      "avg train loss per batch in training: -0.893338\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.949694 KL: 0.548721 Neg-log -1.498416 MSE 0.03119094856083393\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.926756 KL: 0.548721 Neg-log -1.475478 MSE 0.03429637849330902\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 383\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.110396 KL: 0.548722 Neg-log -1.659118  [    0/30000]\n",
      "current batch loss: -1.182553 KL: 0.548660 Neg-log -1.731213  [12800/30000]\n",
      "current batch loss: -1.138068 KL: 0.548591 Neg-log -1.686659  [25600/30000]\n",
      "avg train loss per batch in training: -0.866816\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.939190 KL: 0.548569 Neg-log -1.487760 MSE 0.033749449998140335\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.954240 KL: 0.548569 Neg-log -1.502809 MSE 0.03284529596567154\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 384\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.051157 KL: 0.548570 Neg-log -1.599727  [    0/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: -0.856890 KL: 0.548508 Neg-log -1.405398  [12800/30000]\n",
      "current batch loss: -0.950082 KL: 0.548453 Neg-log -1.498535  [25600/30000]\n",
      "avg train loss per batch in training: -0.912469\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.982271 KL: 0.548433 Neg-log -1.530706 MSE 0.03008853644132614\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.968625 KL: 0.548433 Neg-log -1.517060 MSE 0.031092628836631775\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 385\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.893357 KL: 0.548435 Neg-log -1.441792  [    0/30000]\n",
      "current batch loss: -1.054397 KL: 0.548374 Neg-log -1.602771  [12800/30000]\n",
      "current batch loss: -0.539509 KL: 0.548325 Neg-log -1.087834  [25600/30000]\n",
      "avg train loss per batch in training: -0.910218\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.916202 KL: 0.548302 Neg-log -1.464506 MSE 0.034626126289367676\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.906133 KL: 0.548302 Neg-log -1.454436 MSE 0.03672667592763901\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 386\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.294701 KL: 0.548304 Neg-log -0.843004  [    0/30000]\n",
      "current batch loss: -1.059389 KL: 0.548261 Neg-log -1.607649  [12800/30000]\n",
      "current batch loss: -0.860283 KL: 0.548217 Neg-log -1.408500  [25600/30000]\n",
      "avg train loss per batch in training: -0.895296\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.871626 KL: 0.548203 Neg-log -1.419829 MSE 0.03538432717323303\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.887497 KL: 0.548203 Neg-log -1.435699 MSE 0.035463977605104446\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 387\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.847985 KL: 0.548203 Neg-log -1.396188  [    0/30000]\n",
      "current batch loss: -0.926450 KL: 0.548151 Neg-log -1.474601  [12800/30000]\n",
      "current batch loss: -1.023454 KL: 0.548086 Neg-log -1.571540  [25600/30000]\n",
      "avg train loss per batch in training: -0.885822\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.876638 KL: 0.548066 Neg-log -1.424704 MSE 0.03704565390944481\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.860988 KL: 0.548066 Neg-log -1.409055 MSE 0.03927350789308548\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 388\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.994419 KL: 0.548067 Neg-log -1.542485  [    0/30000]\n",
      "current batch loss: -1.277101 KL: 0.548009 Neg-log -1.825110  [12800/30000]\n",
      "current batch loss: -1.076103 KL: 0.547944 Neg-log -1.624048  [25600/30000]\n",
      "avg train loss per batch in training: -0.883658\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.604381 KL: 0.547927 Neg-log -1.152307 MSE 0.04422810301184654\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.604680 KL: 0.547927 Neg-log -1.152607 MSE 0.045601535588502884\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 389\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.695750 KL: 0.547927 Neg-log -1.243677  [    0/30000]\n",
      "current batch loss: -1.106131 KL: 0.547864 Neg-log -1.653995  [12800/30000]\n",
      "current batch loss: -0.155936 KL: 0.547822 Neg-log -0.703758  [25600/30000]\n",
      "avg train loss per batch in training: -0.907224\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.972904 KL: 0.547804 Neg-log -1.520708 MSE 0.031164394691586494\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.984510 KL: 0.547804 Neg-log -1.532314 MSE 0.03127947822213173\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 390\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.991179 KL: 0.547804 Neg-log -1.538983  [    0/30000]\n",
      "current batch loss: -0.877181 KL: 0.547767 Neg-log -1.424948  [12800/30000]\n",
      "current batch loss: -1.106635 KL: 0.547725 Neg-log -1.654359  [25600/30000]\n",
      "avg train loss per batch in training: -0.898043\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.995250 KL: 0.547706 Neg-log -1.542954 MSE 0.030143434181809425\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.962949 KL: 0.547706 Neg-log -1.510654 MSE 0.031750574707984924\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 391\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.908998 KL: 0.547705 Neg-log -1.456703  [    0/30000]\n",
      "current batch loss: -1.143205 KL: 0.547646 Neg-log -1.690851  [12800/30000]\n",
      "current batch loss: -0.912645 KL: 0.547582 Neg-log -1.460227  [25600/30000]\n",
      "avg train loss per batch in training: -0.889231\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.856917 KL: 0.547561 Neg-log -1.404478 MSE 0.03796956315636635\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.875699 KL: 0.547561 Neg-log -1.423259 MSE 0.03689365088939667\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 392\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.879364 KL: 0.547561 Neg-log -1.426924  [    0/30000]\n",
      "current batch loss: -0.417886 KL: 0.547490 Neg-log -0.965376  [12800/30000]\n",
      "current batch loss: -1.061037 KL: 0.547420 Neg-log -1.608457  [25600/30000]\n",
      "avg train loss per batch in training: -0.894555\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.960509 KL: 0.547401 Neg-log -1.507910 MSE 0.03177826106548309\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.968978 KL: 0.547401 Neg-log -1.516379 MSE 0.03323584422469139\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 393\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.090710 KL: 0.547401 Neg-log -1.638111  [    0/30000]\n",
      "current batch loss: -1.089924 KL: 0.547358 Neg-log -1.637282  [12800/30000]\n",
      "current batch loss: -1.061750 KL: 0.547316 Neg-log -1.609066  [25600/30000]\n",
      "avg train loss per batch in training: -0.932568\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.994619 KL: 0.547294 Neg-log -1.541913 MSE 0.03775748237967491\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.960568 KL: 0.547294 Neg-log -1.507862 MSE 0.032746586948633194\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 394\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.102316 KL: 0.547295 Neg-log -1.649611  [    0/30000]\n",
      "current batch loss: -1.140205 KL: 0.547240 Neg-log -1.687445  [12800/30000]\n",
      "current batch loss: -0.993917 KL: 0.547199 Neg-log -1.541116  [25600/30000]\n",
      "avg train loss per batch in training: -0.900073\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.968587 KL: 0.547180 Neg-log -1.515767 MSE 0.03172732889652252\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.923703 KL: 0.547180 Neg-log -1.470883 MSE 0.03464995324611664\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 395\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.255674 KL: 0.547180 Neg-log -1.802854  [    0/30000]\n",
      "current batch loss: -1.072919 KL: 0.547133 Neg-log -1.620053  [12800/30000]\n",
      "current batch loss: -0.672845 KL: 0.547095 Neg-log -1.219940  [25600/30000]\n",
      "avg train loss per batch in training: -0.918489\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.966876 KL: 0.547081 Neg-log -1.513956 MSE 0.03190578892827034\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.960169 KL: 0.547081 Neg-log -1.507250 MSE 0.033666934818029404\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 396\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.995201 KL: 0.547081 Neg-log -1.542282  [    0/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: -0.714661 KL: 0.547029 Neg-log -1.261690  [12800/30000]\n",
      "current batch loss: -0.838105 KL: 0.546961 Neg-log -1.385066  [25600/30000]\n",
      "avg train loss per batch in training: -0.905476\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.983954 KL: 0.546937 Neg-log -1.530892 MSE 0.03389354795217514\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.980774 KL: 0.546937 Neg-log -1.527712 MSE 0.03206717595458031\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 397\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.243466 KL: 0.546938 Neg-log -1.790404  [    0/30000]\n",
      "current batch loss: -1.004710 KL: 0.546889 Neg-log -1.551600  [12800/30000]\n",
      "current batch loss: -1.053379 KL: 0.546845 Neg-log -1.600224  [25600/30000]\n",
      "avg train loss per batch in training: -0.857382\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.875468 KL: 0.546822 Neg-log -1.422290 MSE 0.03649018704891205\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.839353 KL: 0.546822 Neg-log -1.386175 MSE 0.03859240561723709\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 398\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.863329 KL: 0.546822 Neg-log -1.410151  [    0/30000]\n",
      "current batch loss: -0.970607 KL: 0.546747 Neg-log -1.517354  [12800/30000]\n",
      "current batch loss: -0.887192 KL: 0.546690 Neg-log -1.433882  [25600/30000]\n",
      "avg train loss per batch in training: -0.883206\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.962305 KL: 0.546669 Neg-log -1.508974 MSE 0.033018216490745544\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.952082 KL: 0.546669 Neg-log -1.498751 MSE 0.03408268839120865\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 399\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.117642 KL: 0.546669 Neg-log -1.664310  [    0/30000]\n",
      "current batch loss: -0.682247 KL: 0.546614 Neg-log -1.228861  [12800/30000]\n",
      "current batch loss: -0.839403 KL: 0.546558 Neg-log -1.385961  [25600/30000]\n",
      "avg train loss per batch in training: -0.884954\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.984839 KL: 0.546538 Neg-log -1.531376 MSE 0.030407385900616646\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.981740 KL: 0.546538 Neg-log -1.528277 MSE 0.03019910492002964\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 400\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.967268 KL: 0.546537 Neg-log -1.513805  [    0/30000]\n",
      "current batch loss: -1.071524 KL: 0.546477 Neg-log -1.618001  [12800/30000]\n",
      "current batch loss: -0.697589 KL: 0.546426 Neg-log -1.244015  [25600/30000]\n",
      "avg train loss per batch in training: -0.962471\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.954572 KL: 0.546410 Neg-log -1.500983 MSE 0.03188253194093704\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.929143 KL: 0.546410 Neg-log -1.475554 MSE 0.03490680083632469\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 401\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.772235 KL: 0.546411 Neg-log -1.318646  [    0/30000]\n",
      "current batch loss: -0.866272 KL: 0.546363 Neg-log -1.412635  [12800/30000]\n",
      "current batch loss: -1.253558 KL: 0.546312 Neg-log -1.799870  [25600/30000]\n",
      "avg train loss per batch in training: -0.907286\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.937281 KL: 0.546295 Neg-log -1.483577 MSE 0.03589048236608505\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.944522 KL: 0.546295 Neg-log -1.490818 MSE 0.03308810293674469\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 402\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.002822 KL: 0.546296 Neg-log -1.549118  [    0/30000]\n",
      "current batch loss: -0.720267 KL: 0.546252 Neg-log -1.266519  [12800/30000]\n",
      "current batch loss: -0.941766 KL: 0.546196 Neg-log -1.487963  [25600/30000]\n",
      "avg train loss per batch in training: -0.883328\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -1.012178 KL: 0.546179 Neg-log -1.558356 MSE 0.02780662290751934\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.971140 KL: 0.546179 Neg-log -1.517318 MSE 0.03125761076807976\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 403\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.256063 KL: 0.546178 Neg-log -1.802241  [    0/30000]\n",
      "current batch loss: -1.012889 KL: 0.546140 Neg-log -1.559029  [12800/30000]\n",
      "current batch loss: -0.802704 KL: 0.546089 Neg-log -1.348793  [25600/30000]\n",
      "avg train loss per batch in training: -0.885710\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.858767 KL: 0.546074 Neg-log -1.404840 MSE 0.04364458844065666\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.873963 KL: 0.546074 Neg-log -1.420036 MSE 0.03789970278739929\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 404\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.009380 KL: 0.546073 Neg-log -1.555453  [    0/30000]\n",
      "current batch loss: -0.863258 KL: 0.546017 Neg-log -1.409275  [12800/30000]\n",
      "current batch loss: -1.048718 KL: 0.545961 Neg-log -1.594679  [25600/30000]\n",
      "avg train loss per batch in training: -0.891782\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.983221 KL: 0.545942 Neg-log -1.529162 MSE 0.029556171968579292\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.983928 KL: 0.545942 Neg-log -1.529870 MSE 0.03262327238917351\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 405\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.853433 KL: 0.545941 Neg-log -1.399374  [    0/30000]\n",
      "current batch loss: -0.905018 KL: 0.545904 Neg-log -1.450922  [12800/30000]\n",
      "current batch loss: -0.807527 KL: 0.545846 Neg-log -1.353373  [25600/30000]\n",
      "avg train loss per batch in training: -0.896472\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.969581 KL: 0.545828 Neg-log -1.515409 MSE 0.029302533715963364\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.973965 KL: 0.545828 Neg-log -1.519794 MSE 0.031649623066186905\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 406\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.974243 KL: 0.545828 Neg-log -1.520070  [    0/30000]\n",
      "current batch loss: -0.745558 KL: 0.545780 Neg-log -1.291338  [12800/30000]\n",
      "current batch loss: -1.122467 KL: 0.545743 Neg-log -1.668210  [25600/30000]\n",
      "avg train loss per batch in training: -0.939799\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -1.009772 KL: 0.545729 Neg-log -1.555499 MSE 0.03014425002038479\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.976694 KL: 0.545729 Neg-log -1.522421 MSE 0.031093912199139595\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 407\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.218613 KL: 0.545727 Neg-log -1.764340  [    0/30000]\n",
      "current batch loss: -1.118582 KL: 0.545683 Neg-log -1.664265  [12800/30000]\n",
      "current batch loss: -1.091664 KL: 0.545631 Neg-log -1.637295  [25600/30000]\n",
      "avg train loss per batch in training: -0.936420\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -1.015418 KL: 0.545616 Neg-log -1.561035 MSE 0.031614288687705994\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.983642 KL: 0.545616 Neg-log -1.529259 MSE 0.03155531361699104\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 408\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.130885 KL: 0.545617 Neg-log -1.676501  [    0/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: -1.002937 KL: 0.545581 Neg-log -1.548518  [12800/30000]\n",
      "current batch loss: -1.153445 KL: 0.545549 Neg-log -1.698993  [25600/30000]\n",
      "avg train loss per batch in training: -0.950736\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.942302 KL: 0.545531 Neg-log -1.487832 MSE 0.03323632478713989\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.951949 KL: 0.545531 Neg-log -1.497479 MSE 0.032799750566482544\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 409\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.153079 KL: 0.545530 Neg-log -1.698610  [    0/30000]\n",
      "current batch loss: -0.446182 KL: 0.545479 Neg-log -0.991662  [12800/30000]\n",
      "current batch loss: -1.099273 KL: 0.545434 Neg-log -1.644707  [25600/30000]\n",
      "avg train loss per batch in training: -0.917653\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.943005 KL: 0.545416 Neg-log -1.488420 MSE 0.03373197466135025\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.936532 KL: 0.545416 Neg-log -1.481946 MSE 0.034854304045438766\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 410\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.042861 KL: 0.545414 Neg-log -1.588275  [    0/30000]\n",
      "current batch loss: -1.094917 KL: 0.545373 Neg-log -1.640291  [12800/30000]\n",
      "current batch loss: -0.987645 KL: 0.545337 Neg-log -1.532982  [25600/30000]\n",
      "avg train loss per batch in training: -0.950635\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.992056 KL: 0.545326 Neg-log -1.537381 MSE 0.03591478243470192\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.982705 KL: 0.545326 Neg-log -1.528030 MSE 0.03226470202207565\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 411\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.845036 KL: 0.545326 Neg-log -1.390362  [    0/30000]\n",
      "current batch loss: -0.991190 KL: 0.545271 Neg-log -1.536461  [12800/30000]\n",
      "current batch loss: -1.070194 KL: 0.545236 Neg-log -1.615430  [25600/30000]\n",
      "avg train loss per batch in training: -0.886789\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -1.011271 KL: 0.545221 Neg-log -1.556492 MSE 0.030014527961611748\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -1.001631 KL: 0.545221 Neg-log -1.546853 MSE 0.03076898120343685\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 412\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.077974 KL: 0.545222 Neg-log -1.623196  [    0/30000]\n",
      "current batch loss: -1.117307 KL: 0.545170 Neg-log -1.662477  [12800/30000]\n",
      "current batch loss: -1.020036 KL: 0.545123 Neg-log -1.565159  [25600/30000]\n",
      "avg train loss per batch in training: -0.957059\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -1.025344 KL: 0.545099 Neg-log -1.570444 MSE 0.0303217601031065\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.959760 KL: 0.545099 Neg-log -1.504860 MSE 0.03345407173037529\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 413\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.104778 KL: 0.545100 Neg-log -1.649878  [    0/30000]\n",
      "current batch loss: -0.835622 KL: 0.545047 Neg-log -1.380669  [12800/30000]\n",
      "current batch loss: -1.153013 KL: 0.544997 Neg-log -1.698010  [25600/30000]\n",
      "avg train loss per batch in training: -0.934513\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.979474 KL: 0.544981 Neg-log -1.524454 MSE 0.029971161857247353\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.963376 KL: 0.544981 Neg-log -1.508355 MSE 0.03207143768668175\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 414\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.928689 KL: 0.544980 Neg-log -1.473669  [    0/30000]\n",
      "current batch loss: -1.106594 KL: 0.544957 Neg-log -1.651550  [12800/30000]\n",
      "current batch loss: -0.236999 KL: 0.544899 Neg-log -0.781898  [25600/30000]\n",
      "avg train loss per batch in training: -0.907581\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.995941 KL: 0.544886 Neg-log -1.540829 MSE 0.02995675429701805\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.978756 KL: 0.544886 Neg-log -1.523643 MSE 0.03070564940571785\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 415\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.772321 KL: 0.544887 Neg-log -1.317208  [    0/30000]\n",
      "current batch loss: -0.380726 KL: 0.544829 Neg-log -0.925555  [12800/30000]\n",
      "current batch loss: -0.787825 KL: 0.544796 Neg-log -1.332621  [25600/30000]\n",
      "avg train loss per batch in training: -0.913726\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.996642 KL: 0.544779 Neg-log -1.541423 MSE 0.0297204852104187\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.983511 KL: 0.544779 Neg-log -1.528291 MSE 0.03206241875886917\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 416\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.041120 KL: 0.544781 Neg-log -0.585900  [    0/30000]\n",
      "current batch loss: -0.421677 KL: 0.544733 Neg-log -0.966410  [12800/30000]\n",
      "current batch loss: -1.108303 KL: 0.544695 Neg-log -1.652998  [25600/30000]\n",
      "avg train loss per batch in training: -0.930555\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.920150 KL: 0.544677 Neg-log -1.464827 MSE 0.033078745007514954\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.941228 KL: 0.544677 Neg-log -1.485904 MSE 0.03172420337796211\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 417\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.185982 KL: 0.544676 Neg-log -1.730658  [    0/30000]\n",
      "current batch loss: -0.831828 KL: 0.544636 Neg-log -1.376464  [12800/30000]\n",
      "current batch loss: -1.051925 KL: 0.544587 Neg-log -1.596512  [25600/30000]\n",
      "avg train loss per batch in training: -0.884088\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 1.126999 KL: 0.544565 Neg-log 0.582432 MSE 0.04011561721563339\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.875277 KL: 0.544565 Neg-log -1.419843 MSE 0.034237638115882874\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 418\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.675108 KL: 0.544566 Neg-log -1.219674  [    0/30000]\n",
      "current batch loss: -1.173670 KL: 0.544507 Neg-log -1.718177  [12800/30000]\n",
      "current batch loss: -0.823074 KL: 0.544456 Neg-log -1.367530  [25600/30000]\n",
      "avg train loss per batch in training: -0.881075\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.965895 KL: 0.544435 Neg-log -1.510331 MSE 0.03213518485426903\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.956914 KL: 0.544435 Neg-log -1.501350 MSE 0.03303129971027374\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 419\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.796529 KL: 0.544436 Neg-log -1.340965  [    0/30000]\n",
      "current batch loss: -1.088768 KL: 0.544395 Neg-log -1.633163  [12800/30000]\n",
      "current batch loss: -1.160178 KL: 0.544350 Neg-log -1.704528  [25600/30000]\n",
      "avg train loss per batch in training: -0.952701\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.966842 KL: 0.544340 Neg-log -1.511181 MSE 0.0319414958357811\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.915124 KL: 0.544340 Neg-log -1.459462 MSE 0.03545956686139107\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 420\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.104860 KL: 0.544338 Neg-log -1.649198  [    0/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: -1.232896 KL: 0.544299 Neg-log -1.777195  [12800/30000]\n",
      "current batch loss: -1.020273 KL: 0.544254 Neg-log -1.564527  [25600/30000]\n",
      "avg train loss per batch in training: -0.944026\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.892182 KL: 0.544237 Neg-log -1.436421 MSE 0.039849113672971725\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.856839 KL: 0.544237 Neg-log -1.401078 MSE 0.03715463727712631\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 421\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.874190 KL: 0.544239 Neg-log -1.418429  [    0/30000]\n",
      "current batch loss: -1.071854 KL: 0.544185 Neg-log -1.616039  [12800/30000]\n",
      "current batch loss: -1.128885 KL: 0.544131 Neg-log -1.673016  [25600/30000]\n",
      "avg train loss per batch in training: -0.895437\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.928438 KL: 0.544115 Neg-log -1.472554 MSE 0.03519203141331673\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.907016 KL: 0.544115 Neg-log -1.451131 MSE 0.035710133612155914\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 422\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.970524 KL: 0.544116 Neg-log -1.514640  [    0/30000]\n",
      "current batch loss: -1.011368 KL: 0.544063 Neg-log -1.555431  [12800/30000]\n",
      "current batch loss: -1.092967 KL: 0.544001 Neg-log -1.636968  [25600/30000]\n",
      "avg train loss per batch in training: -0.923704\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.816756 KL: 0.543991 Neg-log -1.360749 MSE 0.03551679104566574\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.794040 KL: 0.543991 Neg-log -1.338032 MSE 0.037193071097135544\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 423\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.874491 KL: 0.543992 Neg-log -1.418483  [    0/30000]\n",
      "current batch loss: -0.927857 KL: 0.543936 Neg-log -1.471793  [12800/30000]\n",
      "current batch loss: -1.221782 KL: 0.543891 Neg-log -1.765673  [25600/30000]\n",
      "avg train loss per batch in training: -0.918113\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.962329 KL: 0.543877 Neg-log -1.506207 MSE 0.03201742842793465\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.957252 KL: 0.543877 Neg-log -1.501130 MSE 0.03081061691045761\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 424\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.209701 KL: 0.543878 Neg-log -1.753579  [    0/30000]\n",
      "current batch loss: -0.593252 KL: 0.543839 Neg-log -1.137091  [12800/30000]\n",
      "current batch loss: -1.171076 KL: 0.543801 Neg-log -1.714877  [25600/30000]\n",
      "avg train loss per batch in training: -0.964882\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.906141 KL: 0.543785 Neg-log -1.449925 MSE 0.03614906966686249\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.873116 KL: 0.543785 Neg-log -1.416899 MSE 0.03884247690439224\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 425\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.277847 KL: 0.543784 Neg-log -1.821631  [    0/30000]\n",
      "current batch loss: -0.556871 KL: 0.543743 Neg-log -1.100614  [12800/30000]\n",
      "current batch loss: -1.199446 KL: 0.543683 Neg-log -1.743129  [25600/30000]\n",
      "avg train loss per batch in training: -0.899547\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.871208 KL: 0.543669 Neg-log -1.414876 MSE 0.034409452229738235\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.910476 KL: 0.543669 Neg-log -1.454142 MSE 0.035753894597291946\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 426\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.973256 KL: 0.543667 Neg-log -1.516924  [    0/30000]\n",
      "current batch loss: -0.917240 KL: 0.543625 Neg-log -1.460865  [12800/30000]\n",
      "current batch loss: -1.121897 KL: 0.543584 Neg-log -1.665481  [25600/30000]\n",
      "avg train loss per batch in training: -0.931375\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -1.037201 KL: 0.543570 Neg-log -1.580769 MSE 0.02774639241397381\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -1.006968 KL: 0.543570 Neg-log -1.550537 MSE 0.03069758042693138\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 427\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.128711 KL: 0.543568 Neg-log -1.672280  [    0/30000]\n",
      "current batch loss: -0.968452 KL: 0.543526 Neg-log -1.511978  [12800/30000]\n",
      "current batch loss: -1.200320 KL: 0.543471 Neg-log -1.743791  [25600/30000]\n",
      "avg train loss per batch in training: -0.944658\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -1.059167 KL: 0.543451 Neg-log -1.602620 MSE 0.0268181711435318\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -1.010011 KL: 0.543451 Neg-log -1.553464 MSE 0.030579088255763054\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 428\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.152359 KL: 0.543453 Neg-log -1.695812  [    0/30000]\n",
      "current batch loss: -1.103819 KL: 0.543400 Neg-log -1.647219  [12800/30000]\n",
      "current batch loss: -1.173774 KL: 0.543366 Neg-log -1.717140  [25600/30000]\n",
      "avg train loss per batch in training: -0.936569\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.984921 KL: 0.543350 Neg-log -1.528269 MSE 0.040043219923973083\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.789505 KL: 0.543350 Neg-log -1.332853 MSE 0.031087704002857208\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 429\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.194787 KL: 0.543349 Neg-log -1.738136  [    0/30000]\n",
      "current batch loss: -0.879456 KL: 0.543312 Neg-log -1.422769  [12800/30000]\n",
      "current batch loss: -0.879507 KL: 0.543273 Neg-log -1.422780  [25600/30000]\n",
      "avg train loss per batch in training: -0.932370\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.955941 KL: 0.543257 Neg-log -1.499195 MSE 0.030424129217863083\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.960531 KL: 0.543257 Neg-log -1.503786 MSE 0.032644327729940414\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 430\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.863788 KL: 0.543255 Neg-log -1.407043  [    0/30000]\n",
      "current batch loss: -1.162913 KL: 0.543221 Neg-log -1.706134  [12800/30000]\n",
      "current batch loss: -0.968250 KL: 0.543170 Neg-log -1.511420  [25600/30000]\n",
      "avg train loss per batch in training: -0.949276\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.993391 KL: 0.543152 Neg-log -1.536543 MSE 0.03020978532731533\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.956498 KL: 0.543152 Neg-log -1.499650 MSE 0.03392907604575157\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 431\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.152476 KL: 0.543152 Neg-log -1.695627  [    0/30000]\n",
      "current batch loss: -0.872204 KL: 0.543117 Neg-log -1.415321  [12800/30000]\n",
      "current batch loss: -0.720620 KL: 0.543084 Neg-log -1.263704  [25600/30000]\n",
      "avg train loss per batch in training: -0.958315\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.995249 KL: 0.543075 Neg-log -1.538323 MSE 0.029602065682411194\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.998457 KL: 0.543075 Neg-log -1.541531 MSE 0.029560957103967667\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 432\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.387022 KL: 0.543074 Neg-log -0.930097  [    0/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: -1.120913 KL: 0.543034 Neg-log -1.663946  [12800/30000]\n",
      "current batch loss: -0.838869 KL: 0.542990 Neg-log -1.381859  [25600/30000]\n",
      "avg train loss per batch in training: -0.916076\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.649057 KL: 0.542983 Neg-log -1.192039 MSE 0.03844263404607773\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.691301 KL: 0.542983 Neg-log -1.234283 MSE 0.03845786675810814\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 433\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.574956 KL: 0.542982 Neg-log -1.117938  [    0/30000]\n",
      "current batch loss: -0.840372 KL: 0.542938 Neg-log -1.383309  [12800/30000]\n",
      "current batch loss: -1.064213 KL: 0.542896 Neg-log -1.607109  [25600/30000]\n",
      "avg train loss per batch in training: -0.977296\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.888266 KL: 0.542877 Neg-log -1.431144 MSE 0.03927937150001526\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.860849 KL: 0.542877 Neg-log -1.403726 MSE 0.03835040330886841\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 434\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.924604 KL: 0.542877 Neg-log -1.467481  [    0/30000]\n",
      "current batch loss: -0.956388 KL: 0.542823 Neg-log -1.499212  [12800/30000]\n",
      "current batch loss: -1.255311 KL: 0.542789 Neg-log -1.798100  [25600/30000]\n",
      "avg train loss per batch in training: -0.963037\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -1.020665 KL: 0.542771 Neg-log -1.563437 MSE 0.028493208810687065\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -1.038440 KL: 0.542771 Neg-log -1.581212 MSE 0.028903549537062645\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 435\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.087470 KL: 0.542772 Neg-log -1.630242  [    0/30000]\n",
      "current batch loss: -1.198962 KL: 0.542728 Neg-log -1.741690  [12800/30000]\n",
      "current batch loss: -1.124439 KL: 0.542690 Neg-log -1.667129  [25600/30000]\n",
      "avg train loss per batch in training: -0.970999\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -1.005561 KL: 0.542677 Neg-log -1.548236 MSE 0.03157085180282593\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -1.022928 KL: 0.542677 Neg-log -1.565603 MSE 0.029038161039352417\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 436\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.588605 KL: 0.542675 Neg-log -1.131280  [    0/30000]\n",
      "current batch loss: -1.209754 KL: 0.542633 Neg-log -1.752387  [12800/30000]\n",
      "current batch loss: -1.104130 KL: 0.542595 Neg-log -1.646726  [25600/30000]\n",
      "avg train loss per batch in training: -0.926729\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.988752 KL: 0.542578 Neg-log -1.531330 MSE 0.038527730852365494\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.996403 KL: 0.542578 Neg-log -1.538981 MSE 0.03248492628335953\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 437\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.147369 KL: 0.542578 Neg-log -1.689946  [    0/30000]\n",
      "current batch loss: -1.199041 KL: 0.542537 Neg-log -1.741578  [12800/30000]\n",
      "current batch loss: -1.078887 KL: 0.542490 Neg-log -1.621377  [25600/30000]\n",
      "avg train loss per batch in training: -0.916086\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.114764 KL: 0.542472 Neg-log -0.427708 MSE 0.027574237436056137\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -1.029731 KL: 0.542472 Neg-log -1.572203 MSE 0.029555976390838623\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 438\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.137538 KL: 0.542471 Neg-log -1.680009  [    0/30000]\n",
      "current batch loss: -1.193861 KL: 0.542428 Neg-log -1.736289  [12800/30000]\n",
      "current batch loss: -1.133790 KL: 0.542386 Neg-log -1.676176  [25600/30000]\n",
      "avg train loss per batch in training: -0.988396\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -1.036675 KL: 0.542368 Neg-log -1.579045 MSE 0.027918804436922073\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.988744 KL: 0.542368 Neg-log -1.531113 MSE 0.03311130777001381\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 439\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.379904 KL: 0.542369 Neg-log -0.922273  [    0/30000]\n",
      "current batch loss: -1.199827 KL: 0.542334 Neg-log -1.742161  [12800/30000]\n",
      "current batch loss: -0.863533 KL: 0.542288 Neg-log -1.405821  [25600/30000]\n",
      "avg train loss per batch in training: -0.996335\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.988600 KL: 0.542273 Neg-log -1.530871 MSE 0.03199189528822899\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -1.005889 KL: 0.542273 Neg-log -1.548160 MSE 0.032407160848379135\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 440\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.559934 KL: 0.542271 Neg-log -1.102205  [    0/30000]\n",
      "current batch loss: -1.093886 KL: 0.542229 Neg-log -1.636115  [12800/30000]\n",
      "current batch loss: -0.902188 KL: 0.542178 Neg-log -1.444366  [25600/30000]\n",
      "avg train loss per batch in training: -0.933928\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -1.048159 KL: 0.542158 Neg-log -1.590317 MSE 0.026039281859993935\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -1.030846 KL: 0.542158 Neg-log -1.573003 MSE 0.028766445815563202\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 441\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.160106 KL: 0.542157 Neg-log -1.702263  [    0/30000]\n",
      "current batch loss: -1.168019 KL: 0.542108 Neg-log -1.710128  [12800/30000]\n",
      "current batch loss: -1.134593 KL: 0.542068 Neg-log -1.676661  [25600/30000]\n",
      "avg train loss per batch in training: -0.961290\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.932013 KL: 0.542055 Neg-log -1.474068 MSE 0.03524696081876755\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.944759 KL: 0.542055 Neg-log -1.486814 MSE 0.032760120928287506\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 442\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.224362 KL: 0.542055 Neg-log -0.766418  [    0/30000]\n",
      "current batch loss: -0.754737 KL: 0.542014 Neg-log -1.296751  [12800/30000]\n",
      "current batch loss: -0.736229 KL: 0.541984 Neg-log -1.278213  [25600/30000]\n",
      "avg train loss per batch in training: -0.947509\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -1.001921 KL: 0.541969 Neg-log -1.543890 MSE 0.027950823307037354\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -1.001119 KL: 0.541969 Neg-log -1.543087 MSE 0.029786305502057076\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 443\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.269885 KL: 0.541968 Neg-log -1.811853  [    0/30000]\n",
      "current batch loss: -1.203810 KL: 0.541923 Neg-log -1.745734  [12800/30000]\n",
      "current batch loss: -1.095816 KL: 0.541870 Neg-log -1.637685  [25600/30000]\n",
      "avg train loss per batch in training: -0.969641\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -1.068650 KL: 0.541849 Neg-log -1.610501 MSE 0.028032010421156883\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -1.042023 KL: 0.541849 Neg-log -1.583873 MSE 0.028414031490683556\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 444\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.102477 KL: 0.541850 Neg-log -1.644327  [    0/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: -0.978740 KL: 0.541808 Neg-log -1.520547  [12800/30000]\n",
      "current batch loss: -0.964389 KL: 0.541760 Neg-log -1.506149  [25600/30000]\n",
      "avg train loss per batch in training: -0.964862\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.894141 KL: 0.541741 Neg-log -1.435882 MSE 0.04376671090722084\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.832955 KL: 0.541741 Neg-log -1.374697 MSE 0.03935111686587334\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 445\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.864056 KL: 0.541742 Neg-log -1.405798  [    0/30000]\n",
      "current batch loss: -0.738967 KL: 0.541702 Neg-log -1.280668  [12800/30000]\n",
      "current batch loss: -1.172799 KL: 0.541655 Neg-log -1.714454  [25600/30000]\n",
      "avg train loss per batch in training: -0.988131\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -1.051024 KL: 0.541647 Neg-log -1.592670 MSE 0.027329059317708015\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -1.037237 KL: 0.541647 Neg-log -1.578884 MSE 0.02902689017355442\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 446\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.077413 KL: 0.541647 Neg-log -1.619060  [    0/30000]\n",
      "current batch loss: -1.184361 KL: 0.541604 Neg-log -1.725965  [12800/30000]\n",
      "current batch loss: -0.823439 KL: 0.541571 Neg-log -1.365010  [25600/30000]\n",
      "avg train loss per batch in training: -0.973944\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -1.067590 KL: 0.541557 Neg-log -1.609148 MSE 0.026689516380429268\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -1.024282 KL: 0.541557 Neg-log -1.565840 MSE 0.02880222722887993\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 447\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.281076 KL: 0.541557 Neg-log -1.822633  [    0/30000]\n",
      "current batch loss: 0.157666 KL: 0.541520 Neg-log -0.383854  [12800/30000]\n",
      "current batch loss: -1.096459 KL: 0.541460 Neg-log -1.637919  [25600/30000]\n",
      "avg train loss per batch in training: -0.939864\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.932299 KL: 0.541443 Neg-log -1.473742 MSE 0.03233425319194794\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.908126 KL: 0.541443 Neg-log -1.449569 MSE 0.0342181995511055\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 448\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.949457 KL: 0.541443 Neg-log -1.490900  [    0/30000]\n",
      "current batch loss: -1.196495 KL: 0.541388 Neg-log -1.737883  [12800/30000]\n",
      "current batch loss: -0.701507 KL: 0.541333 Neg-log -1.242840  [25600/30000]\n",
      "avg train loss per batch in training: -0.952023\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -1.000090 KL: 0.541315 Neg-log -1.541406 MSE 0.02785259671509266\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.960605 KL: 0.541315 Neg-log -1.501921 MSE 0.031317152082920074\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 449\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.733278 KL: 0.541316 Neg-log -1.274594  [    0/30000]\n",
      "current batch loss: -0.636526 KL: 0.541278 Neg-log -1.177803  [12800/30000]\n",
      "current batch loss: -1.092374 KL: 0.541242 Neg-log -1.633616  [25600/30000]\n",
      "avg train loss per batch in training: -0.500790\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.799514 KL: 0.541235 Neg-log -1.340747 MSE 0.04666171967983246\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.803739 KL: 0.541235 Neg-log -1.344973 MSE 0.04660755395889282\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 450\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.088128 KL: 0.541233 Neg-log -1.629361  [    0/30000]\n",
      "current batch loss: -1.113088 KL: 0.541174 Neg-log -1.654262  [12800/30000]\n",
      "current batch loss: -0.886966 KL: 0.541115 Neg-log -1.428080  [25600/30000]\n",
      "avg train loss per batch in training: -0.898064\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -1.021940 KL: 0.541099 Neg-log -1.563040 MSE 0.02842807210981846\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.988134 KL: 0.541099 Neg-log -1.529235 MSE 0.03160928189754486\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 451\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.063758 KL: 0.541099 Neg-log -1.604858  [    0/30000]\n",
      "current batch loss: -1.054538 KL: 0.541060 Neg-log -1.595598  [12800/30000]\n",
      "current batch loss: -0.483462 KL: 0.541018 Neg-log -1.024480  [25600/30000]\n",
      "avg train loss per batch in training: -0.916903\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -1.028362 KL: 0.541002 Neg-log -1.569365 MSE 0.027004161849617958\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.986405 KL: 0.541002 Neg-log -1.527409 MSE 0.03201339766383171\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 452\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.085342 KL: 0.541003 Neg-log -1.626346  [    0/30000]\n",
      "current batch loss: -1.012574 KL: 0.540964 Neg-log -1.553538  [12800/30000]\n",
      "current batch loss: -1.035307 KL: 0.540920 Neg-log -1.576226  [25600/30000]\n",
      "avg train loss per batch in training: -0.979295\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -1.039233 KL: 0.540909 Neg-log -1.580141 MSE 0.026567330583930016\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 10.273265 KL: 0.540909 Neg-log 9.732355 MSE 0.03290281817317009\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 453\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.034093 KL: 0.540908 Neg-log -1.575001  [    0/30000]\n",
      "current batch loss: -1.031206 KL: 0.540868 Neg-log -1.572073  [12800/30000]\n",
      "current batch loss: -1.061339 KL: 0.540831 Neg-log -1.602171  [25600/30000]\n",
      "avg train loss per batch in training: -0.957645\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.765516 KL: 0.540811 Neg-log -1.306328 MSE 0.03963649272918701\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.735166 KL: 0.540811 Neg-log -1.275978 MSE 0.041219376027584076\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 454\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.792858 KL: 0.540813 Neg-log -1.333670  [    0/30000]\n",
      "current batch loss: -0.467836 KL: 0.540762 Neg-log -1.008597  [12800/30000]\n",
      "current batch loss: -1.015608 KL: 0.540707 Neg-log -1.556315  [25600/30000]\n",
      "avg train loss per batch in training: -0.924872\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -1.023197 KL: 0.540694 Neg-log -1.563890 MSE 0.027892595157027245\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -1.018372 KL: 0.540694 Neg-log -1.559065 MSE 0.028789551928639412\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 455\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.259494 KL: 0.540693 Neg-log -1.800187  [    0/30000]\n",
      "current batch loss: -1.158164 KL: 0.540639 Neg-log -1.698803  [12800/30000]\n",
      "current batch loss: -1.180166 KL: 0.540593 Neg-log -1.720759  [25600/30000]\n",
      "avg train loss per batch in training: -0.996619\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -1.022834 KL: 0.540581 Neg-log -1.563415 MSE 0.02844182774424553\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -1.023962 KL: 0.540581 Neg-log -1.564542 MSE 0.029835883527994156\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 456\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.278214 KL: 0.540580 Neg-log -0.818794  [    0/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: -1.149523 KL: 0.540550 Neg-log -1.690073  [12800/30000]\n",
      "current batch loss: -1.036686 KL: 0.540507 Neg-log -1.577193  [25600/30000]\n",
      "avg train loss per batch in training: -0.961156\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.802485 KL: 0.540489 Neg-log -1.342974 MSE 0.03674454987049103\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.782557 KL: 0.540489 Neg-log -1.323046 MSE 0.03899139538407326\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 457\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.206087 KL: 0.540488 Neg-log -1.746575  [    0/30000]\n",
      "current batch loss: -1.208828 KL: 0.540446 Neg-log -1.749274  [12800/30000]\n",
      "current batch loss: -1.109386 KL: 0.540391 Neg-log -1.649777  [25600/30000]\n",
      "avg train loss per batch in training: -0.912390\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.902882 KL: 0.540375 Neg-log -1.443256 MSE 0.03425949811935425\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.889114 KL: 0.540375 Neg-log -1.429489 MSE 0.03359796479344368\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 458\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.031593 KL: 0.540375 Neg-log -1.571967  [    0/30000]\n",
      "current batch loss: -1.140584 KL: 0.540335 Neg-log -1.680920  [12800/30000]\n",
      "current batch loss: -0.911022 KL: 0.540277 Neg-log -1.451299  [25600/30000]\n",
      "avg train loss per batch in training: -0.951647\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.870448 KL: 0.540260 Neg-log -1.410707 MSE 0.03436791151762009\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.827316 KL: 0.540260 Neg-log -1.367576 MSE 0.037802804261446\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 459\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.173378 KL: 0.540259 Neg-log -1.713637  [    0/30000]\n",
      "current batch loss: -1.139612 KL: 0.540218 Neg-log -1.679830  [12800/30000]\n",
      "current batch loss: -1.319558 KL: 0.540167 Neg-log -1.859725  [25600/30000]\n",
      "avg train loss per batch in training: -0.930848\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.473586 KL: 0.540147 Neg-log -1.013734 MSE 0.032424286007881165\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.980101 KL: 0.540147 Neg-log -1.520249 MSE 0.03178378567099571\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 460\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.940267 KL: 0.540148 Neg-log -1.480415  [    0/30000]\n",
      "current batch loss: -0.994948 KL: 0.540100 Neg-log -1.535047  [12800/30000]\n",
      "current batch loss: -1.017800 KL: 0.540054 Neg-log -1.557853  [25600/30000]\n",
      "avg train loss per batch in training: -0.979266\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -1.078724 KL: 0.540045 Neg-log -1.618768 MSE 0.02508438564836979\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -1.059318 KL: 0.540045 Neg-log -1.599362 MSE 0.02725456841289997\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 461\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.023784 KL: 0.540045 Neg-log -1.563829  [    0/30000]\n",
      "current batch loss: -1.079735 KL: 0.540006 Neg-log -1.619740  [12800/30000]\n",
      "current batch loss: -1.046143 KL: 0.539964 Neg-log -1.586107  [25600/30000]\n",
      "avg train loss per batch in training: -1.001576\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.910634 KL: 0.539949 Neg-log -1.450584 MSE 0.034810081124305725\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.867179 KL: 0.539949 Neg-log -1.407130 MSE 0.03738269954919815\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 462\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.095702 KL: 0.539951 Neg-log -1.635653  [    0/30000]\n",
      "current batch loss: -0.520639 KL: 0.539923 Neg-log -1.060562  [12800/30000]\n",
      "current batch loss: -0.232314 KL: 0.539881 Neg-log -0.772196  [25600/30000]\n",
      "avg train loss per batch in training: 25038.082799\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.461788 KL: 0.539856 Neg-log -1.001644 MSE 0.0748491957783699\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.490308 KL: 0.539856 Neg-log -1.030164 MSE 0.07044906169176102\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 463\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.226311 KL: 0.539856 Neg-log -0.766168  [    0/30000]\n",
      "current batch loss: -0.900691 KL: 0.539753 Neg-log -1.440444  [12800/30000]\n",
      "current batch loss: -0.593489 KL: 0.539673 Neg-log -1.133162  [25600/30000]\n",
      "avg train loss per batch in training: -0.727609\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.802627 KL: 0.539644 Neg-log -1.342272 MSE 0.04539152607321739\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.788072 KL: 0.539644 Neg-log -1.327717 MSE 0.044986844062805176\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 464\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.906082 KL: 0.539645 Neg-log -1.445726  [    0/30000]\n",
      "current batch loss: -0.555899 KL: 0.539571 Neg-log -1.095469  [12800/30000]\n",
      "current batch loss: -0.842454 KL: 0.539498 Neg-log -1.381952  [25600/30000]\n",
      "avg train loss per batch in training: -0.836767\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.920088 KL: 0.539474 Neg-log -1.459562 MSE 0.037598900496959686\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.939226 KL: 0.539474 Neg-log -1.478700 MSE 0.036938320845365524\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 465\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.950750 KL: 0.539474 Neg-log -1.490224  [    0/30000]\n",
      "current batch loss: -0.900969 KL: 0.539426 Neg-log -1.440395  [12800/30000]\n",
      "current batch loss: -1.098132 KL: 0.539373 Neg-log -1.637506  [25600/30000]\n",
      "avg train loss per batch in training: -0.858629\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.932801 KL: 0.539360 Neg-log -1.472160 MSE 0.03548922762274742\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.922360 KL: 0.539360 Neg-log -1.461719 MSE 0.035819217562675476\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 466\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.056024 KL: 0.539358 Neg-log -1.595382  [    0/30000]\n",
      "current batch loss: -1.107118 KL: 0.539303 Neg-log -1.646421  [12800/30000]\n",
      "current batch loss: -1.025180 KL: 0.539246 Neg-log -1.564426  [25600/30000]\n",
      "avg train loss per batch in training: -0.828238\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.973234 KL: 0.539223 Neg-log -1.512458 MSE 0.035644300282001495\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.950161 KL: 0.539223 Neg-log -1.489385 MSE 0.03296610713005066\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 467\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.964844 KL: 0.539225 Neg-log -1.504069  [    0/30000]\n",
      "current batch loss: -0.928769 KL: 0.539169 Neg-log -1.467938  [12800/30000]\n",
      "current batch loss: -1.078951 KL: 0.539100 Neg-log -1.618051  [25600/30000]\n",
      "avg train loss per batch in training: -0.917063\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.847914 KL: 0.539078 Neg-log -1.386992 MSE 0.040150102227926254\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.834299 KL: 0.539078 Neg-log -1.373378 MSE 0.0410320870578289\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 468\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.090617 KL: 0.539078 Neg-log -1.629695  [    0/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: -0.986665 KL: 0.539018 Neg-log -1.525682  [12800/30000]\n",
      "current batch loss: -1.108176 KL: 0.538963 Neg-log -1.647139  [25600/30000]\n",
      "avg train loss per batch in training: -0.888777\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.944505 KL: 0.538935 Neg-log -1.483442 MSE 0.03367292881011963\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.893860 KL: 0.538935 Neg-log -1.432796 MSE 0.03594526648521423\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 469\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.076915 KL: 0.538936 Neg-log -1.615851  [    0/30000]\n",
      "current batch loss: -0.820142 KL: 0.538870 Neg-log -1.359012  [12800/30000]\n",
      "current batch loss: -0.707791 KL: 0.538810 Neg-log -1.246601  [25600/30000]\n",
      "avg train loss per batch in training: -0.948358\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.929293 KL: 0.538788 Neg-log -1.468081 MSE 0.03479170426726341\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.899085 KL: 0.538788 Neg-log -1.437873 MSE 0.036648448556661606\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 470\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.945665 KL: 0.538788 Neg-log -1.484453  [    0/30000]\n",
      "current batch loss: -0.216413 KL: 0.538738 Neg-log -0.755151  [12800/30000]\n",
      "current batch loss: -1.068195 KL: 0.538691 Neg-log -1.606886  [25600/30000]\n",
      "avg train loss per batch in training: -0.900197\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.765729 KL: 0.538672 Neg-log -1.304400 MSE 0.04530065506696701\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.745581 KL: 0.538672 Neg-log -1.284252 MSE 0.04426049813628197\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 471\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.105768 KL: 0.538671 Neg-log -1.644439  [    0/30000]\n",
      "current batch loss: -0.797361 KL: 0.538606 Neg-log -1.335967  [12800/30000]\n",
      "current batch loss: -0.608054 KL: 0.538538 Neg-log -1.146592  [25600/30000]\n",
      "avg train loss per batch in training: -0.891380\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.985190 KL: 0.538512 Neg-log -1.523700 MSE 0.0337248295545578\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.931235 KL: 0.538512 Neg-log -1.469745 MSE 0.03904521092772484\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 472\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.212558 KL: 0.538510 Neg-log -1.751069  [    0/30000]\n",
      "current batch loss: -0.977943 KL: 0.538436 Neg-log -1.516379  [12800/30000]\n",
      "current batch loss: -0.754340 KL: 0.538364 Neg-log -1.292704  [25600/30000]\n",
      "avg train loss per batch in training: -0.940622\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.944844 KL: 0.538345 Neg-log -1.483190 MSE 0.0391731783747673\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.923762 KL: 0.538345 Neg-log -1.462107 MSE 0.039608534425497055\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 473\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.179047 KL: 0.538346 Neg-log -1.717394  [    0/30000]\n",
      "current batch loss: -1.117226 KL: 0.538276 Neg-log -1.655502  [12800/30000]\n",
      "current batch loss: -1.159312 KL: 0.538210 Neg-log -1.697523  [25600/30000]\n",
      "avg train loss per batch in training: -0.915416\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.949569 KL: 0.538187 Neg-log -1.487756 MSE 0.034730054438114166\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.949122 KL: 0.538187 Neg-log -1.487309 MSE 0.03454774618148804\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 474\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.698529 KL: 0.538187 Neg-log -1.236717  [    0/30000]\n",
      "current batch loss: -0.972511 KL: 0.538118 Neg-log -1.510629  [12800/30000]\n",
      "current batch loss: -1.234821 KL: 0.538037 Neg-log -1.772858  [25600/30000]\n",
      "avg train loss per batch in training: -0.946783\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.940248 KL: 0.538011 Neg-log -1.478260 MSE 0.03368702530860901\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.952553 KL: 0.538011 Neg-log -1.490565 MSE 0.034856654703617096\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 475\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.765743 KL: 0.538013 Neg-log -1.303756  [    0/30000]\n",
      "current batch loss: -0.281962 KL: 0.537942 Neg-log -0.819904  [12800/30000]\n",
      "current batch loss: -1.134616 KL: 0.537860 Neg-log -1.672475  [25600/30000]\n",
      "avg train loss per batch in training: -0.912880\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.830201 KL: 0.537832 Neg-log -1.368032 MSE 0.03679563105106354\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.813604 KL: 0.537832 Neg-log -1.351435 MSE 0.03937792778015137\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 476\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.892382 KL: 0.537831 Neg-log -1.430213  [    0/30000]\n",
      "current batch loss: -1.074310 KL: 0.537757 Neg-log -1.612067  [12800/30000]\n",
      "current batch loss: -0.399993 KL: 0.537689 Neg-log -0.937682  [25600/30000]\n",
      "avg train loss per batch in training: -0.942851\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -1.006591 KL: 0.537659 Neg-log -1.544252 MSE 0.03170294687151909\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.989814 KL: 0.537659 Neg-log -1.527474 MSE 0.033638425171375275\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 477\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.186254 KL: 0.537660 Neg-log -1.723913  [    0/30000]\n",
      "current batch loss: -1.141038 KL: 0.537584 Neg-log -1.678622  [12800/30000]\n",
      "current batch loss: -1.042088 KL: 0.537513 Neg-log -1.579600  [25600/30000]\n",
      "avg train loss per batch in training: -0.964654\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.892141 KL: 0.537490 Neg-log -1.429631 MSE 0.0365489199757576\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.887912 KL: 0.537490 Neg-log -1.425402 MSE 0.034762218594551086\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 478\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.586689 KL: 0.537490 Neg-log -1.124179  [    0/30000]\n",
      "current batch loss: -0.829004 KL: 0.537420 Neg-log -1.366423  [12800/30000]\n",
      "current batch loss: -1.074983 KL: 0.537359 Neg-log -1.612342  [25600/30000]\n",
      "avg train loss per batch in training: -0.951178\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -1.019732 KL: 0.537331 Neg-log -1.557063 MSE 0.029244663193821907\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 36.833825 KL: 0.537331 Neg-log 36.296482 MSE 0.0321248359978199\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 479\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.785652 KL: 0.537331 Neg-log -1.322984  [    0/30000]\n",
      "current batch loss: -1.167047 KL: 0.537250 Neg-log -1.704297  [12800/30000]\n",
      "current batch loss: -1.137551 KL: 0.537180 Neg-log -1.674731  [25600/30000]\n",
      "avg train loss per batch in training: -0.929465\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.943844 KL: 0.537154 Neg-log -1.480997 MSE 0.03599608317017555\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.939032 KL: 0.537154 Neg-log -1.476185 MSE 0.033291324973106384\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 480\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.882390 KL: 0.537153 Neg-log -1.419543  [    0/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: -0.998840 KL: 0.537095 Neg-log -1.535936  [12800/30000]\n",
      "current batch loss: -1.217392 KL: 0.537018 Neg-log -1.754410  [25600/30000]\n",
      "avg train loss per batch in training: -0.876639\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.757150 KL: 0.537001 Neg-log -1.294150 MSE 0.04710785672068596\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.751358 KL: 0.537001 Neg-log -1.288359 MSE 0.043681930750608444\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 481\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.017529 KL: 0.537000 Neg-log -1.554529  [    0/30000]\n",
      "current batch loss: -0.984809 KL: 0.536922 Neg-log -1.521730  [12800/30000]\n",
      "current batch loss: -1.014309 KL: 0.536851 Neg-log -1.551160  [25600/30000]\n",
      "avg train loss per batch in training: -0.943651\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -1.032472 KL: 0.536825 Neg-log -1.569296 MSE 0.02946905978024006\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.999383 KL: 0.536825 Neg-log -1.536206 MSE 0.03220853954553604\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 482\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.311993 KL: 0.536824 Neg-log -0.848817  [    0/30000]\n",
      "current batch loss: -0.895941 KL: 0.536749 Neg-log -1.432691  [12800/30000]\n",
      "current batch loss: -0.988860 KL: 0.536687 Neg-log -1.525548  [25600/30000]\n",
      "avg train loss per batch in training: -0.955604\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 10361927.959573 KL: 0.536667 Neg-log 10361928.000000 MSE 0.1083938479423523\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.998331 KL: 0.536667 Neg-log -1.534998 MSE 0.03471625596284866\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 483\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.283005 KL: 0.536667 Neg-log -1.819672  [    0/30000]\n",
      "current batch loss: -1.145736 KL: 0.536613 Neg-log -1.682349  [12800/30000]\n",
      "current batch loss: -1.092684 KL: 0.536533 Neg-log -1.629217  [25600/30000]\n",
      "avg train loss per batch in training: -0.944244\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.976402 KL: 0.536507 Neg-log -1.512908 MSE 0.03525366261601448\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.937259 KL: 0.536507 Neg-log -1.473765 MSE 0.03426951542496681\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 484\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.226387 KL: 0.536506 Neg-log -1.762893  [    0/30000]\n",
      "current batch loss: -0.926932 KL: 0.536440 Neg-log -1.463372  [12800/30000]\n",
      "current batch loss: -0.849732 KL: 0.536371 Neg-log -1.386103  [25600/30000]\n",
      "avg train loss per batch in training: -0.979233\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -1.016003 KL: 0.536346 Neg-log -1.552349 MSE 0.03124985471367836\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -1.026526 KL: 0.536346 Neg-log -1.562872 MSE 0.03083660826086998\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 485\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.056885 KL: 0.536346 Neg-log -1.593230  [    0/30000]\n",
      "current batch loss: -1.126940 KL: 0.536289 Neg-log -1.663229  [12800/30000]\n",
      "current batch loss: -1.126012 KL: 0.536239 Neg-log -1.662251  [25600/30000]\n",
      "avg train loss per batch in training: -0.955394\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.892559 KL: 0.536216 Neg-log -1.428774 MSE 0.03275700658559799\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.829809 KL: 0.536216 Neg-log -1.366024 MSE 0.03685355558991432\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 486\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.568097 KL: 0.536215 Neg-log -1.104312  [    0/30000]\n",
      "current batch loss: -1.016858 KL: 0.536135 Neg-log -1.552992  [12800/30000]\n",
      "current batch loss: -1.207599 KL: 0.536073 Neg-log -1.743671  [25600/30000]\n",
      "avg train loss per batch in training: -0.924453\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -1.015478 KL: 0.536049 Neg-log -1.551529 MSE 0.03520480543375015\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.985106 KL: 0.536049 Neg-log -1.521157 MSE 0.03155028820037842\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 487\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.190201 KL: 0.536050 Neg-log -1.726252  [    0/30000]\n",
      "current batch loss: -0.924391 KL: 0.535985 Neg-log -1.460377  [12800/30000]\n",
      "current batch loss: -1.212315 KL: 0.535923 Neg-log -1.748238  [25600/30000]\n",
      "avg train loss per batch in training: -0.960537\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -1.018945 KL: 0.535896 Neg-log -1.554841 MSE 0.031056983396410942\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -1.025444 KL: 0.535896 Neg-log -1.561341 MSE 0.030300335958600044\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 488\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.769168 KL: 0.535896 Neg-log -1.305064  [    0/30000]\n",
      "current batch loss: -1.149904 KL: 0.535842 Neg-log -1.685745  [12800/30000]\n",
      "current batch loss: -1.201034 KL: 0.535777 Neg-log -1.736811  [25600/30000]\n",
      "avg train loss per batch in training: -0.960790\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.895773 KL: 0.535757 Neg-log -1.431530 MSE 0.0334136076271534\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.906454 KL: 0.535757 Neg-log -1.442210 MSE 0.03418904170393944\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 489\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.738615 KL: 0.535757 Neg-log -1.274371  [    0/30000]\n",
      "current batch loss: -0.743633 KL: 0.535702 Neg-log -1.279335  [12800/30000]\n",
      "current batch loss: -0.945931 KL: 0.535639 Neg-log -1.481570  [25600/30000]\n",
      "avg train loss per batch in training: -0.997698\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.797440 KL: 0.535614 Neg-log -1.333053 MSE 0.03755040466785431\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.816630 KL: 0.535614 Neg-log -1.352242 MSE 0.03723178803920746\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 490\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.185633 KL: 0.535613 Neg-log -1.721246  [    0/30000]\n",
      "current batch loss: -1.162202 KL: 0.535558 Neg-log -1.697761  [12800/30000]\n",
      "current batch loss: -1.191332 KL: 0.535498 Neg-log -1.726830  [25600/30000]\n",
      "avg train loss per batch in training: -0.973668\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -1.064481 KL: 0.535471 Neg-log -1.599953 MSE 0.026588981971144676\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -1.048693 KL: 0.535471 Neg-log -1.584165 MSE 0.029387712478637695\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 491\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.212786 KL: 0.535472 Neg-log -1.748258  [    0/30000]\n",
      "current batch loss: -1.227521 KL: 0.535404 Neg-log -1.762926  [12800/30000]\n",
      "current batch loss: -0.917146 KL: 0.535346 Neg-log -1.452492  [25600/30000]\n",
      "avg train loss per batch in training: -0.933794\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -1.013547 KL: 0.535324 Neg-log -1.548869 MSE 0.031331729143857956\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.941654 KL: 0.535324 Neg-log -1.476977 MSE 0.03474447503685951\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 492\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.604477 KL: 0.535323 Neg-log -1.139800  [    0/30000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: -1.133947 KL: 0.535252 Neg-log -1.669199  [12800/30000]\n",
      "current batch loss: -1.146364 KL: 0.535192 Neg-log -1.681556  [25600/30000]\n",
      "avg train loss per batch in training: -1.000237\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -1.032915 KL: 0.535170 Neg-log -1.568084 MSE 0.029704710468649864\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -1.017537 KL: 0.535170 Neg-log -1.552705 MSE 0.03105481155216694\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 493\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.214501 KL: 0.535169 Neg-log -1.749669  [    0/30000]\n",
      "current batch loss: -1.119811 KL: 0.535117 Neg-log -1.654928  [12800/30000]\n",
      "current batch loss: -1.149057 KL: 0.535060 Neg-log -1.684117  [25600/30000]\n",
      "avg train loss per batch in training: -1.028206\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -1.055837 KL: 0.535040 Neg-log -1.590876 MSE 0.028749598190188408\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -1.046086 KL: 0.535040 Neg-log -1.581126 MSE 0.028192806988954544\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 494\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.190321 KL: 0.535040 Neg-log -1.725360  [    0/30000]\n",
      "current batch loss: 0.274266 KL: 0.534987 Neg-log -0.260721  [12800/30000]\n",
      "current batch loss: -0.741514 KL: 0.534937 Neg-log -1.276451  [25600/30000]\n",
      "avg train loss per batch in training: -0.963953\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -1.063990 KL: 0.534914 Neg-log -1.598906 MSE 0.0280962772667408\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -1.028942 KL: 0.534914 Neg-log -1.563857 MSE 0.030442798510193825\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 495\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.042603 KL: 0.534915 Neg-log -1.577518  [    0/30000]\n",
      "current batch loss: -0.880958 KL: 0.534845 Neg-log -1.415803  [12800/30000]\n",
      "current batch loss: -0.955222 KL: 0.534786 Neg-log -1.490008  [25600/30000]\n",
      "avg train loss per batch in training: -0.954434\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.960901 KL: 0.534768 Neg-log -1.495670 MSE 0.031062308698892593\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.961494 KL: 0.534768 Neg-log -1.496262 MSE 0.03494899719953537\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 496\n",
      "-----------------------------------------------\n",
      "current batch loss: -0.901800 KL: 0.534769 Neg-log -1.436569  [    0/30000]\n",
      "current batch loss: -1.085820 KL: 0.534706 Neg-log -1.620527  [12800/30000]\n",
      "current batch loss: -1.254592 KL: 0.534651 Neg-log -1.789243  [25600/30000]\n",
      "avg train loss per batch in training: -0.973528\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -1.011976 KL: 0.534630 Neg-log -1.546605 MSE 0.03059539385139942\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -1.027489 KL: 0.534630 Neg-log -1.562119 MSE 0.03059295006096363\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 497\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.111549 KL: 0.534629 Neg-log -1.646178  [    0/30000]\n",
      "current batch loss: -1.187529 KL: 0.534561 Neg-log -1.722090  [12800/30000]\n",
      "current batch loss: -1.153023 KL: 0.534512 Neg-log -1.687535  [25600/30000]\n",
      "avg train loss per batch in training: -1.024946\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -1.024730 KL: 0.534491 Neg-log -1.559219 MSE 0.02885873056948185\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -1.015300 KL: 0.534491 Neg-log -1.549789 MSE 0.03174544870853424\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 498\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.079125 KL: 0.534489 Neg-log -1.613614  [    0/30000]\n",
      "current batch loss: -0.742144 KL: 0.534428 Neg-log -1.276572  [12800/30000]\n",
      "current batch loss: -0.554988 KL: 0.534367 Neg-log -1.089355  [25600/30000]\n",
      "avg train loss per batch in training: -0.999827\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -1.054722 KL: 0.534349 Neg-log -1.589071 MSE 0.030516136437654495\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -1.037331 KL: 0.534349 Neg-log -1.571681 MSE 0.028998887166380882\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 499\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.309004 KL: 0.534350 Neg-log -1.843353  [    0/30000]\n",
      "current batch loss: -1.170267 KL: 0.534299 Neg-log -1.704566  [12800/30000]\n",
      "current batch loss: -0.961069 KL: 0.534231 Neg-log -1.495300  [25600/30000]\n",
      "avg train loss per batch in training: -0.431294\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.798330 KL: 0.534210 Neg-log -1.332539 MSE 0.06849084049463272\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.792789 KL: 0.534210 Neg-log -1.326999 MSE 0.06565528362989426\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 500\n",
      "-----------------------------------------------\n",
      "current batch loss: -1.003411 KL: 0.534209 Neg-log -1.537620  [    0/30000]\n",
      "current batch loss: -1.010355 KL: 0.534140 Neg-log -1.544495  [12800/30000]\n",
      "current batch loss: -0.973784 KL: 0.534070 Neg-log -1.507854  [25600/30000]\n",
      "avg train loss per batch in training: -0.880103\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: -0.970351 KL: 0.534048 Neg-log -1.504397 MSE 0.04543152078986168\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: -0.954881 KL: 0.534048 Neg-log -1.488928 MSE 0.03805692493915558\n",
      "-----------------------------------------------\n",
      "|\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# a useful function to present things clearer\n",
    "def seperator():\n",
    "    print( \"-----------------------------------------------\" )\n",
    "\n",
    "# reset some parameters\n",
    "batch_size = 128\n",
    "\n",
    "trn_dataset = amp_dataset(trn_datfp.to(device), trn_amplp.unsqueeze(-1).to(device))\n",
    "val_dataset = amp_dataset(val_datfp.to(device), val_amplp.unsqueeze(-1).to(device))\n",
    "tst_dataset = amp_dataset(tst_datfp.to(device), tst_amplp.unsqueeze(-1).to(device))\n",
    "\n",
    "trn_dataloader = DataLoader(trn_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "tst_dataloader = DataLoader(tst_dataset, batch_size=batch_size, shuffle=True)\n",
    "epochs = 500\n",
    "\n",
    "# re-initialise the model and the optimizer\n",
    "hdn_dim = 50\n",
    "trn_len = trn_amplp.shape[0]\n",
    "print(f\"Training dataset length: {trn_len}\")\n",
    "model = bayes_amp_net(trn_len, hdn_dim=hdn_dim).to(device)\n",
    "learning_rate = 5e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "seperator()\n",
    "print(\"model architecture\")\n",
    "seperator()\n",
    "print(model)\n",
    "total_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model has {total_parameters:d} trainable parameters\")\n",
    "\n",
    "\n",
    "# track train and val losses\n",
    "trn_nl_losses = []\n",
    "trn_kl_losses = []\n",
    "trn_losses = []\n",
    "trn_mse_losses = []\n",
    "val_nl_losses = []\n",
    "val_kl_losses = []\n",
    "val_losses = []\n",
    "val_mse_losses = []\n",
    "trn_nl_losses_live = []\n",
    "trn_kl_losses_live = []\n",
    "trn_losses_live = []\n",
    "\n",
    "for t in range(epochs):\n",
    "    seperator()\n",
    "    print(f\"Epoch {t+1}\")\n",
    "    seperator()\n",
    "    loss_live, kl_live, nl_live = train_epoch(trn_dataloader, model, optimizer)\n",
    "    trn_nl_losses_live.append(nl_live)\n",
    "    trn_kl_losses_live.append(kl_live)\n",
    "    trn_losses_live.append(loss_live)\n",
    "    seperator()\n",
    "    trn_nl_loss, trn_kl_loss, trn_loss, trn_mse_loss = trn_pass(trn_dataloader, model)\n",
    "    trn_nl_losses.append(trn_nl_loss)\n",
    "    trn_kl_losses.append(trn_kl_loss)\n",
    "    trn_losses.append(trn_loss)\n",
    "    trn_mse_losses.append(trn_mse_loss)\n",
    "    seperator()\n",
    "    val_nl_loss, val_kl_loss, val_loss, val_mse_loss = val_pass(val_dataloader, model)\n",
    "    val_nl_losses.append(val_nl_loss)\n",
    "    val_kl_losses.append(val_kl_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    val_mse_losses.append(val_mse_loss)\n",
    "    seperator()\n",
    "    print( \"|\" )\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the train and validation losses as a function of the epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 4, figsize=(18,5))\n",
    "\n",
    "c1 = 'tab:red'\n",
    "c2 = 'tab:green'\n",
    "\n",
    "axs[0].plot(trn_losses, label=\"train\", color=c1)\n",
    "axs[0].plot(val_losses, label=\"val\", color=c2)\n",
    "axs[0].plot(trn_losses_live, label=\"train live\", color=c1, ls='dashed')\n",
    "axs[0].set_xlabel(\"epoch\")\n",
    "axs[0].set_ylabel(\"Total loss\")\n",
    "axs[0].set_ylim(-2, 5)\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(trn_nl_losses, label=\"train\", color=c1)\n",
    "axs[1].plot(val_nl_losses, label=\"val\", color=c2)\n",
    "axs[1].plot(trn_nl_losses_live, label=\"train live\", color=c1, ls='dashed')\n",
    "axs[1].set_xlabel(\"epoch\")\n",
    "axs[1].set_ylabel(\"Neg-log-gauss loss\")\n",
    "axs[1].set_ylim(-2, 5)\n",
    "axs[1].legend()\n",
    "\n",
    "axs[2].plot(trn_kl_losses, label=\"train\", color=c1)\n",
    "axs[2].plot(val_kl_losses, label=\"val\", color=c2)\n",
    "axs[2].plot(trn_kl_losses_live, label=\"train live\", color=c1, ls='dashed')\n",
    "axs[2].set_yscale('log')\n",
    "axs[2].set_xlabel(\"epoch\")\n",
    "axs[2].set_ylabel(\"KL loss\")\n",
    "axs[2].legend()\n",
    "\n",
    "\n",
    "axs[3].plot(trn_mse_losses, label=\"train\", color=c1)\n",
    "axs[3].plot(val_mse_losses, label=\"val\", color=c2)\n",
    "axs[3].set_yscale('log')\n",
    "axs[3].set_xlabel(\"epoch\")\n",
    "axs[3].set_ylabel(\"MSE Loss\")\n",
    "axs[3].legend()\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that both the train and validation losses are being reduced during training, the model is fitting well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to get some visualisation of how well our amplitude regression has worked.\n",
    "\n",
    "The simplest thing we can do is to pass our data through the neural network to get a predicted amplitude for each event, then histogram this and compare it to the histogram of the true amplitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(model, dataloader, n_monte=30):\n",
    "\n",
    "    # we don't need gradients here since we only use the forward pass\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # sample from weight distributions\n",
    "        amps_samples = []\n",
    "        sigma2_samples = []\n",
    "        for i in range(n_monte):\n",
    "            print(f\"Evaluating prediction: {i+1} / {n_monte}\")\n",
    "            # go through dataset\n",
    "            amps = []\n",
    "            sigma2 = []\n",
    "            for X, y in dataloader:\n",
    "                pred = model(X).detach().cpu().numpy()\n",
    "                amps.extend(pred[:, 0]) # dimensions: [batch_size, 2]\n",
    "                sigma2.extend(np.exp(pred[:, 1]))\n",
    "\n",
    "            amps_samples.append(amps)\n",
    "            sigma2_samples.append(sigma2)\n",
    "           \n",
    "    # dimensionaility (n_monte, batch_size)\n",
    "    amps_samples = np.stack(amps_samples, axis=0)\n",
    "    sigma2_samples = np.stack(sigma2_samples, axis=0)\n",
    "                \n",
    "    return amps_samples, sigma2_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating prediction: 1 / 30\n",
      "Evaluating prediction: 2 / 30\n",
      "Evaluating prediction: 3 / 30\n",
      "Evaluating prediction: 4 / 30\n",
      "Evaluating prediction: 5 / 30\n",
      "Evaluating prediction: 6 / 30\n",
      "Evaluating prediction: 7 / 30\n",
      "Evaluating prediction: 8 / 30\n",
      "Evaluating prediction: 9 / 30\n",
      "Evaluating prediction: 10 / 30\n",
      "Evaluating prediction: 11 / 30\n",
      "Evaluating prediction: 12 / 30\n",
      "Evaluating prediction: 13 / 30\n",
      "Evaluating prediction: 14 / 30\n",
      "Evaluating prediction: 15 / 30\n",
      "Evaluating prediction: 16 / 30\n",
      "Evaluating prediction: 17 / 30\n",
      "Evaluating prediction: 18 / 30\n",
      "Evaluating prediction: 19 / 30\n",
      "Evaluating prediction: 20 / 30\n",
      "Evaluating prediction: 21 / 30\n",
      "Evaluating prediction: 22 / 30\n",
      "Evaluating prediction: 23 / 30\n",
      "Evaluating prediction: 24 / 30\n",
      "Evaluating prediction: 25 / 30\n",
      "Evaluating prediction: 26 / 30\n",
      "Evaluating prediction: 27 / 30\n",
      "Evaluating prediction: 28 / 30\n",
      "Evaluating prediction: 29 / 30\n",
      "Evaluating prediction: 30 / 30\n",
      "Evaluating prediction: 1 / 30\n",
      "Evaluating prediction: 2 / 30\n",
      "Evaluating prediction: 3 / 30\n",
      "Evaluating prediction: 4 / 30\n",
      "Evaluating prediction: 5 / 30\n",
      "Evaluating prediction: 6 / 30\n",
      "Evaluating prediction: 7 / 30\n",
      "Evaluating prediction: 8 / 30\n",
      "Evaluating prediction: 9 / 30\n",
      "Evaluating prediction: 10 / 30\n",
      "Evaluating prediction: 11 / 30\n",
      "Evaluating prediction: 12 / 30\n",
      "Evaluating prediction: 13 / 30\n",
      "Evaluating prediction: 14 / 30\n",
      "Evaluating prediction: 15 / 30\n",
      "Evaluating prediction: 16 / 30\n",
      "Evaluating prediction: 17 / 30\n",
      "Evaluating prediction: 18 / 30\n",
      "Evaluating prediction: 19 / 30\n",
      "Evaluating prediction: 20 / 30\n",
      "Evaluating prediction: 21 / 30\n",
      "Evaluating prediction: 22 / 30\n",
      "Evaluating prediction: 23 / 30\n",
      "Evaluating prediction: 24 / 30\n",
      "Evaluating prediction: 25 / 30\n",
      "Evaluating prediction: 26 / 30\n",
      "Evaluating prediction: 27 / 30\n",
      "Evaluating prediction: 28 / 30\n",
      "Evaluating prediction: 29 / 30\n",
      "Evaluating prediction: 30 / 30\n",
      "Mean std pred:  0.107057035\n",
      "Mean std stoch:  0.22751991\n"
     ]
    }
   ],
   "source": [
    "# TURN OFF shuffeling to not mess up the weight sampling!\n",
    "trn_dataloader = DataLoader(trn_dataset, batch_size=64, shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "tst_dataloader = DataLoader(tst_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# dimensionality is (n_monte, batch_size)\n",
    "pred_trn_ampls_samples, sigma2_trn_samples = get_prediction(model, trn_dataloader)\n",
    "pred_val_ampls_samples, sigma2_val_samples = get_prediction(model, val_dataloader)\n",
    "\n",
    "# compute mean prediction, standard deviation of predictions and mean sigma-output, see lecture notes\n",
    "pred_trn_ampls = np.mean(pred_trn_ampls_samples, axis=0) # mean prediction\n",
    "pred_trn_ampls_std = np.std(pred_trn_ampls_samples, axis=0)\n",
    "pred_trn_ampls_std_stoch = np.sqrt(np.mean(sigma2_trn_samples, axis=0))\n",
    "pred_trn_ampls_std_tot = np.sqrt(pred_trn_ampls_std**2 + pred_trn_ampls_std_stoch**2)\n",
    "\n",
    "# same for validation data\n",
    "pred_val_ampls = np.mean(pred_val_ampls_samples, axis=0) # mean prediction\n",
    "pred_val_ampls_std = np.std(pred_val_ampls_samples, axis=0)\n",
    "pred_val_ampls_std_stoch = np.sqrt(np.mean(sigma2_val_samples, axis=0))\n",
    "pred_val_ampls_std_tot = np.sqrt(pred_val_ampls_std**2 + pred_val_ampls_std_stoch**2)\n",
    "\n",
    "print(\"Mean std pred: \", np.mean(pred_trn_ampls_std))\n",
    "print(\"Mean std stoch: \", np.mean(pred_trn_ampls_std_stoch))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First for the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 4, figsize=(18,5))\n",
    "\n",
    "c1 = 'tab:red'\n",
    "c2 = 'tab:green'\n",
    "\n",
    "(n1, bins, patches1 ) = axs[0].hist(trn_ampl, histtype='stepfilled', fill=None, edgecolor=c1, label=\"train data\", ls=\"--\")\n",
    "(n2, _, patches2 ) = axs[0].hist(pred_trn_ampls, histtype='stepfilled', fill=None, edgecolor=c2, label=\"train preds\", bins=bins)\n",
    "\n",
    "bins= np.linspace(0, 1., 50)\n",
    "(n3, _, patches3 ) = axs[1].hist(pred_trn_ampls_std, bins=bins, histtype='stepfilled', fill=None, edgecolor=c1, label=\"train data\", ls=\"--\")\n",
    "(n4, _, patches4 ) = axs[2].hist(pred_trn_ampls_std_tot, bins=bins, histtype='stepfilled', fill=None, edgecolor=c1, label=\"train data\", ls=\"--\")\n",
    "(n5, _, patches5 ) = axs[3].hist(pred_trn_ampls_std_stoch, bins=bins, histtype='stepfilled', fill=None, edgecolor=c1, label=\"train data\", ls=\"--\")\n",
    "\n",
    "axs[0].set_yscale('log')\n",
    "axs[0].set_xlabel(\"log(train amplitudes)\")\n",
    "axs[0].set_ylabel(\"number of events\")\n",
    "axs[0].legend(loc='best')\n",
    "\n",
    "axs[1].set_xlabel(\"Predictive Uncertainty\")\n",
    "axs[1].set_ylabel(\"number of events\")\n",
    "axs[1].legend(loc='best')\n",
    "\n",
    "axs[2].set_xlabel(\"Total Uncertainty \")\n",
    "axs[2].set_ylabel(\"number of events\")\n",
    "axs[2].legend(loc='best')\n",
    "\n",
    "axs[3].set_xlabel(\"Stochastic Uncertainty\")\n",
    "axs[3].set_ylabel(\"number of events\")\n",
    "axs[3].legend(loc='best')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 4, figsize=(20,5))\n",
    "\n",
    "c1 = 'tab:red'\n",
    "c2 = 'tab:green'\n",
    "\n",
    "n_bins = 50\n",
    "bins = np.linspace(-5, 5, n_bins)\n",
    "\n",
    "pull = (trn_ampl - pred_trn_ampls) / trn_ampl\n",
    "pull_normalized_tot = (trn_ampl - pred_trn_ampls) / pred_val_ampls_std_tot\n",
    "pull_normalized_stoch = (trn_ampl - pred_trn_ampls) / pred_val_ampls_std_stoch\n",
    "pull_normalized = (trn_ampl - pred_trn_ampls) / pred_val_ampls_std\n",
    "\n",
    "\n",
    "(n1, bins1, patches1 ) = axs[0].hist( pull, histtype='stepfilled', bins=n_bins, fill=None, edgecolor=c1, label=\"train data\", ls=\"--\" )\n",
    "(n2, bins2, patches2 ) = axs[1].hist( pull_normalized_tot, histtype='stepfilled', bins=np.linspace(-5, 5, n_bins), fill=None, edgecolor=c1, label=\"train data\", ls=\"--\" )\n",
    "(n3, bins3, patches3 ) = axs[2].hist( pull_normalized_stoch, histtype='stepfilled', bins=np.linspace(-5, 5, n_bins), fill=None, edgecolor=c1, label=\"train data\", ls=\"--\" )\n",
    "(n4, bins4, patches4 ) = axs[3].hist( pull_normalized, histtype='stepfilled', bins=n_bins, fill=None, edgecolor=c1, label=\"train data\", ls=\"--\" )\n",
    "\n",
    "axs[0].set_xlabel(\"Pull\")\n",
    "axs[0].set_ylabel(\"number of events\")\n",
    "axs[0].legend(loc='best')\n",
    "\n",
    "axs[1].set_xlabel(\"Pull total unc\")\n",
    "axs[1].set_ylabel(\"number of events\")\n",
    "axs[1].legend(loc='best')\n",
    "\n",
    "axs[2].set_xlabel(\"Pull stochastic unc\")\n",
    "axs[2].set_ylabel(\"number of events\")\n",
    "axs[2].legend(loc='best')\n",
    "\n",
    "axs[3].set_yscale('log')\n",
    "axs[3].set_xlabel(\"Pull predictive unc\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# same plots for exp(log (train amplitudes) ) = train amplitudes\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(20,5))\n",
    "\n",
    "c1 = 'tab:red'\n",
    "c2 = 'tab:green'\n",
    "\n",
    "n_bins = 50\n",
    "bins =np.linspace(-5, 5, n_bins)\n",
    "\n",
    "# revert preprocessing\n",
    "trn_ampl_exp = np.exp(trn_ampl)\n",
    "pred_trn_ampls_exp = np.exp(pred_trn_ampls)\n",
    "pred_val_ampls_std_tot_exp = np.exp(trn_ampl) * pred_val_ampls_std_tot # error propagation\n",
    "pred_val_ampls_std_stoch_exp = np.exp(trn_ampl) * pred_val_ampls_std_stoch # error propagation\n",
    "pred_val_ampls_std_exp = np.exp(trn_ampl) * pred_val_ampls_std # error propagation\n",
    "\n",
    "pull = (trn_ampl_exp - pred_trn_ampls_exp) / trn_ampl_exp\n",
    "pull_normalized_tot = (trn_ampl_exp - pred_trn_ampls_exp) / pred_val_ampls_std_tot_exp\n",
    "pull_normalized_stoch = (trn_ampl_exp - pred_trn_ampls_exp) / pred_val_ampls_std_stoch_exp\n",
    "pull_normalized = (trn_ampl_exp - pred_trn_ampls_exp) / pred_val_ampls_std_exp\n",
    "(n1, bins1, patches1 ) = axs[0].hist(pull, histtype='stepfilled', bins=bins, fill=None, edgecolor=c1, label=\"train data\", ls=\"--\")\n",
    "(n2, bins2, patches2 ) = axs[1].hist(pull_normalized_tot, histtype='stepfilled', bins=bins, fill=None, edgecolor=c1, label=\"train data\", ls=\"--\")\n",
    "(n3, bins3, patches3 ) = axs[2].hist(pull_normalized_stoch, histtype='stepfilled', bins=bins, fill=None, edgecolor=c1, label=\"train data\", ls=\"--\")\n",
    "(n4, bins4, patches4 ) = axs[3].hist(pull_normalized, histtype='stepfilled', bins=bins, fill=None, edgecolor=c1, label=\"train data\", ls=\"--\")\n",
    "\n",
    "\n",
    "axs[0].set_xlabel(\"Pull\")\n",
    "axs[0].set_ylabel(\"number of events\")\n",
    "axs[0].set_xlim([-5, 5])\n",
    "axs[0].legend(loc='best')\n",
    "\n",
    "axs[1].set_xlabel(\"Pull total unc\")\n",
    "axs[1].set_ylabel(\"number of events\")\n",
    "axs[1].legend(loc='best')\n",
    "\n",
    "axs[2].set_xlabel(\"Pull stochastic unc\")\n",
    "axs[2].set_ylabel(\"number of events\")\n",
    "axs[2].legend(loc='best')\n",
    "\n",
    "axs[3].set_yscale('log')\n",
    "axs[3].set_xlabel(\"Pull predictive unc\")\n",
    "axs[3].set_ylabel(\"number of events\")\n",
    "axs[3].legend(loc='best')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error is lowest in the regions where there is more training data, this is expected.  The more data the network has to learn from, the better it can learn to predict the correct amplitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then for the validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(7, 5))\n",
    "\n",
    "c1 = 'tab:red'\n",
    "c2 = 'tab:green'\n",
    "\n",
    "(n1, bins, patches1) = axs.hist(val_ampl, histtype='stepfilled', fill=None, edgecolor=c1, label=\"val data\", ls=\"--\")\n",
    "(n2, _, patches2) = axs.hist(pred_val_ampls, histtype='stepfilled', fill=None, edgecolor=c2, label=\"val preds\", bins=bins)\n",
    "\n",
    "axs.set_yscale('log')\n",
    "\n",
    "axs.set_xlabel(\"log( train amplitudes )\")\n",
    "axs.set_ylabel(\"number of events\")\n",
    "\n",
    "axs.legend(loc='best')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(7,5))\n",
    "\n",
    "c1 = 'tab:red'\n",
    "c2 = 'tab:green'\n",
    "\n",
    "axs.plot((bins[1:]+bins[:-1])/2., np.abs((n1-n2)/n1), color=c1)\n",
    "\n",
    "axs.set_xlabel(\"log(train amplitudes)\")\n",
    "axs.set_ylabel(\"Error\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
