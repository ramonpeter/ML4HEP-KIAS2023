{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification without labels (CWoLa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week we will use the CWoLa method to find anomalous di-jet events in LHC data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Background:\n",
    "- Classification without labels is an semi-supervised classification technique, where we define two data samples, and assume that one contains much more of the signal class than the other\n",
    "- In our case we'll use the LHC Olympics dataset, where each event consists of two jets\n",
    "- Instead of working with low-level constituent-level information we will use high-level observables:\n",
    "    - jet masses - $m_{j_1}$ and $m_{j_2}$\n",
    "    - N-subjettiness - $\\left(\\tau_{21}\\right)_{1}$ and $\\left(\\tau_{21}\\right)_{2}$\n",
    "    - $\\tau_{21}$ is an observable which measures how two-prong a jet is\n",
    "    - the other subscripts refer to the two jets in the event, with the highest $p_T$ jet coming first\n",
    "- To define the two data samples we use in the CWoLa method we will use the invariant mass observable, this is similar to what is done in a 'bump-hunt' search at the LHC\n",
    "- You will see from the plots below that the signal events have invariant masses clustered around $3.5$ TeV, so we choose:\n",
    "    - Signal-region: jets in $3$ TeV $\\leq m_{JJ} \\leq$ $4$ TeV\n",
    "    - Sideband-region: jets with $m_{JJ}<3$ TeV or $m_{JJ}>4$ TeV\n",
    "- To implement the CWoLa method we then build a binary classifier to classify between the events in the two regions, because there are more signal events in the signal enriched region than in the sideband-region the classifier should be able to learn to separate signal and background\n",
    "- **please read up on this in the lecture notes**\n",
    "- In the data sample provided, there are 100k signal events and 1M background events, so it should be easy to find the signal\n",
    "- However, this gets more difficult as the number of signal events is decreased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting papers:\n",
    "- Classification without labels: Learning from mixed samples in high energy physics\n",
    "    - Eric M. Metodiev, Benjamin Nachman, Jesse Thaler\n",
    "    - https://arxiv.org/abs/1708.02949\n",
    "- Extending the Bump Hunt with Machine Learning\n",
    "    - Jack H Collins, Kiel Howe, Benjamin Nachman\n",
    "    - https://arxiv.org/abs/1902.02634\n",
    "- Dijet resonance search with weak supervision using $s=\\sqrt{13}$ TeV pp collisions in the ATLAS detector\n",
    "    - ATLAS collaboration\n",
    "    - https://arxiv.org/abs/2005.02983"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outline:\n",
    "- Imports and plotting set-up\n",
    "- Study the data\n",
    "- Select a signal window\n",
    "- Building the dataset\n",
    "    - Preprocessing\n",
    "    - Dataloaders\n",
    "- Building the semi-supervised CWoLa classifier\n",
    "- Optimise the classifier\n",
    "    - if you need to, you can reduce the total number of events so that this runs on a laptop\n",
    "- Study the results\n",
    "- Reduce the number of signal events a re-train the classifier\n",
    "    - when does the performance break down?\n",
    "- What else might limit the performance?  Correlations?\n",
    "- Study the optimal case of no correlations in the data\n",
    "    - 'signal-region data' = 100k background events and 10k signal events, both from the signal region\n",
    "    - 'sideband-region data' = 100k different background events, also from the signal region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some tips:\n",
    "- read up on the CWoLa method in your lecture notes and understand the different steps taken\n",
    "- and again, a lot of the code you need is in the previous tutorial notebooks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import Sequential, Linear, ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import matplotlib.colors as mcolors\n",
    "import colorsys\n",
    "\n",
    "labelfont = FontProperties()\n",
    "labelfont.set_family('serif')\n",
    "labelfont.set_name('Times New Roman')\n",
    "labelfont.set_size(14)\n",
    "\n",
    "axislabelfont = FontProperties()\n",
    "axislabelfont.set_family('serif')\n",
    "axislabelfont.set_name('Times New Roman')\n",
    "axislabelfont.set_size(22)\n",
    "\n",
    "tickfont = FontProperties()\n",
    "tickfont.set_family('serif')\n",
    "tickfont.set_name('Times New Roman')\n",
    "tickfont.set_size(16)\n",
    "\n",
    "axisfontsize = 16\n",
    "labelfontsize = 16\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams[\"mathtext.default\"] = \"rm\"\n",
    "plt.rcParams['text.usetex'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the LHC Olympics dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = np.load( \"tutorial-10-data/lhco-dat.npy\" )\n",
    "lbs = np.load( \"tutorial-10-data/lhco-lbs.npy\" )\n",
    "msq = np.load( \"tutorial-10-data/lhco-msq.npy\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the train/val/test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dat = dat[0:900000]\n",
    "trn_lbs = lbs[0:900000]\n",
    "trn_msq = msq[0:900000]\n",
    "\n",
    "val_dat = dat[900000:1000000]\n",
    "val_lbs = lbs[900000:1000000]\n",
    "val_msq = msq[900000:1000000]\n",
    "\n",
    "tst_dat = dat[1000000:11000000]\n",
    "tst_lbs = lbs[1000000:11000000]\n",
    "tst_msq = msq[1000000:11000000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the invariant mass of the signal and background events separately.\n",
    "\n",
    "We can clearly see the mass peak for the signal events at $3.5$ TeV, while the mass distribution for the background events falls exponentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots( figsize=(6,5) )\n",
    "\n",
    "axs.hist( msq[ np.where(lbs==1.0) ], alpha=0.5, density=True, bins=15, range=(1200,6500) )\n",
    "axs.hist( msq[ np.where(lbs==0.0) ], alpha=0.5, density=True, bins=15, range=(1200,6500) )\n",
    "\n",
    "axs.set_xlabel( \"$m_{JJ}$ (GeV)\", fontproperties=axislabelfont )\n",
    "axs.set_ylabel( \"\\#events\", fontproperties=axislabelfont )\n",
    "\n",
    "yticks = [  ]\n",
    "axs.set_yticklabels( yticks, fontproperties=tickfont )\n",
    "\n",
    "xticks = [ int(x) for x in axs.get_xticks() ]\n",
    "axs.set_xticklabels( xticks, fontproperties=tickfont )\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualise the 4 features, 2 for each jet in the events.\n",
    "\n",
    "Here we see that these 4 observables are particularly good at separating the signal and background events!\n",
    "\n",
    "This is because the signal events come from a process $pp\\rightarrow A \\rightarrow B~C$, where \n",
    "- $m_A=3.5$ TeV\n",
    "- $m_B=0.4$ TeV\n",
    "- $m_C=0.1$ TeV\n",
    "\n",
    "Particles $B$ and $C$ then decay to two quarks each, giving these jets a boosted two-prong topology which we can see from the skewed N-subjettiness distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots( 2, 2, figsize=(12,10) )\n",
    "\n",
    "axs[0,0].hist( dat[:,0][ np.where(lbs==1.0) ], alpha=0.5, density=True, bins=15, range=(0,700) )\n",
    "axs[0,0].hist( dat[:,0][ np.where(lbs==0.0) ], alpha=0.5, density=True, bins=15, range=(0,700) )\n",
    "axs[0,0].set_xlabel( \"$m_{j_1}$ (GeV)\", fontproperties=axislabelfont )\n",
    "axs[0,0].set_ylabel( \"\\#events\", fontproperties=axislabelfont )\n",
    "xticks = [ int(x) for x in axs[0,0].get_xticks() ]\n",
    "axs[0,0].set_xticklabels( xticks, fontproperties=tickfont )\n",
    "yticks = [  ]\n",
    "axs[0,0].set_yticklabels( yticks, fontproperties=tickfont )\n",
    "\n",
    "axs[0,1].hist( dat[:,1][ np.where(lbs==1.0) ], alpha=0.5, density=True, bins=15, range=(0,1.2) )\n",
    "axs[0,1].hist( dat[:,1][ np.where(lbs==0.0) ], alpha=0.5, density=True, bins=15, range=(0,1.2) )\n",
    "axs[0,1].set_xlabel( \"$\\\\left(\\\\tau_{21}\\\\right)_{1}$\", fontproperties=axislabelfont )\n",
    "axs[0,1].set_ylabel( \"\\#events\", fontproperties=axislabelfont )\n",
    "xticks = [ round(x,2) for x in axs[0,1].get_xticks() ]\n",
    "axs[0,1].set_xticklabels( xticks, fontproperties=tickfont )\n",
    "yticks = [  ]\n",
    "axs[0,1].set_yticklabels( yticks, fontproperties=tickfont )\n",
    "\n",
    "axs[1,0].hist( dat[:,2][ np.where(lbs==1.0) ], alpha=0.5, density=True, bins=15, range=(0,700) )\n",
    "axs[1,0].hist( dat[:,2][ np.where(lbs==0.0) ], alpha=0.5, density=True, bins=15, range=(0,700) )\n",
    "axs[1,0].set_xlabel( \"$m_{j_2}$ (GeV)\", fontproperties=axislabelfont )\n",
    "axs[1,0].set_ylabel( \"\\#events\", fontproperties=axislabelfont )\n",
    "xticks = [ int(x) for x in axs[1,0].get_xticks() ]\n",
    "axs[1,0].set_xticklabels( xticks, fontproperties=tickfont )\n",
    "yticks = [  ]\n",
    "axs[1,0].set_yticklabels( yticks, fontproperties=tickfont )\n",
    "\n",
    "axs[1,1].hist( dat[:,3][ np.where(lbs==1.0) ], alpha=0.5, density=True, bins=15, range=(0,1.2) )\n",
    "axs[1,1].hist( dat[:,3][ np.where(lbs==0.0) ], alpha=0.5, density=True, bins=15, range=(0,1.2) )\n",
    "axs[1,1].set_xlabel( \"$\\\\left(\\\\tau_{21}\\\\right)_{2}$\", fontproperties=axislabelfont )\n",
    "axs[1,1].set_ylabel( \"\\#events\", fontproperties=axislabelfont )\n",
    "xticks = [ round(x,2) for x in axs[1,1].get_xticks() ]\n",
    "axs[1,1].set_xticklabels( xticks, fontproperties=tickfont )\n",
    "yticks = [  ]\n",
    "axs[1,1].set_yticklabels( yticks, fontproperties=tickfont )\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select a signal window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CWoLa method requires us to train a binary classifier where the labels identify which bin the event came from, so here we split the data into the signal and sideband bins to label them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "msq_window = [3000,4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dat_sw = trn_dat[ [ ( m > msq_window[0] and m < msq_window[1] ) for m in trn_msq ] ]\n",
    "trn_lbs_sw = trn_lbs[ [ ( m > msq_window[0] and m < msq_window[1] ) for m in trn_msq ] ]\n",
    "trn_dat_sb = trn_dat[ [ ( m < msq_window[0] or m > msq_window[1] ) for m in trn_msq ] ]\n",
    "trn_lbs_sb = trn_lbs[ [ ( m < msq_window[0] or m > msq_window[1] ) for m in trn_msq ] ]\n",
    "\n",
    "val_dat_sw = val_dat[ [ ( m > msq_window[0] and m < msq_window[1] ) for m in val_msq ] ]\n",
    "val_lbs_sw = val_lbs[ [ ( m > msq_window[0] and m < msq_window[1] ) for m in val_msq ] ]\n",
    "val_dat_sb = val_dat[ [ ( m < msq_window[0] or m > msq_window[1] ) for m in val_msq ] ]\n",
    "val_lbs_sb = val_lbs[ [ ( m < msq_window[0] or m > msq_window[1] ) for m in val_msq ] ]\n",
    "\n",
    "tst_dat_sw = tst_dat[ [ ( m > msq_window[0] and m < msq_window[1] ) for m in tst_msq ] ]\n",
    "tst_lbs_sw = tst_lbs[ [ ( m > msq_window[0] and m < msq_window[1] ) for m in tst_msq ] ]\n",
    "tst_dat_sb = tst_dat[ [ ( m < msq_window[0] or m > msq_window[1] ) for m in tst_msq ] ]\n",
    "tst_lbs_sb = tst_lbs[ [ ( m < msq_window[0] or m > msq_window[1] ) for m in tst_msq ] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the shapes and the sums, the sums tell us how many signal events there is in the bin since they are labelled with a $1.0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((380294, 4), (380294,), (519706, 4), (519706,), 76608, 5117)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_dat_sw.shape, trn_lbs_sw.shape, trn_dat_sb.shape, trn_lbs_sb.shape, trn_lbs_sw.sum(), trn_lbs_sb.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42375, 4), (42375,), (57625, 4), (57625,), 8613, 578)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dat_sw.shape, val_lbs_sw.shape, val_dat_sb.shape, val_lbs_sb.shape, val_lbs_sw.sum(), val_lbs_sb.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42020, 4), (42020,), (57980, 4), (57980,), 8481, 603)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_dat_sw.shape, tst_lbs_sw.shape, tst_dat_sb.shape, tst_lbs_sb.shape, tst_lbs_sw.sum(), tst_lbs_sb.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets and dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before constructing the dataloaders we need to put the data together again and define the bin labels that we will use in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate( (trn_dat_sw,trn_dat_sb), axis=0 )\n",
    "y_train = np.concatenate( (trn_lbs_sw,trn_lbs_sb), axis=0 )\n",
    "b_train = np.concatenate( ( np.ones( trn_lbs_sw.shape[0] ), np.zeros( trn_lbs_sb.shape[0] ) ), axis=0 )\n",
    "\n",
    "X_val = np.concatenate( (val_dat_sw,val_dat_sb), axis=0 )\n",
    "y_val = np.concatenate( (val_lbs_sw,val_lbs_sb), axis=0 )\n",
    "b_val = np.concatenate( ( np.ones( val_lbs_sw.shape[0] ), np.zeros( val_lbs_sb.shape[0] ) ), axis=0 )\n",
    "\n",
    "X_test = np.concatenate( (tst_dat_sw,tst_dat_sb), axis=0 )\n",
    "y_test = np.concatenate( (tst_lbs_sw,tst_lbs_sb), axis=0 )\n",
    "b_test = np.concatenate( ( np.ones( tst_lbs_sw.shape[0] ), np.zeros( tst_lbs_sb.shape[0] ) ), axis=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((900000, 4), (900000,), (900000,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, b_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100000, 4), (100000,), (100000,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape, y_val.shape, b_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100000, 4), (100000,), (100000,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape, b_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the CWoLa dataset such that get_item returns the data, the true sig/bkg labels, and the bin label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cwola_data( Dataset ):\n",
    "    \n",
    "    def __init__( self, data, labels,  bins ):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.bins = bins\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx], self.bins[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_p = torch.Tensor( X_train )\n",
    "y_train_p = torch.Tensor( y_train ).unsqueeze(-1)\n",
    "b_train_p = torch.Tensor( b_train ).unsqueeze(-1)\n",
    "\n",
    "X_val_p = torch.Tensor( X_val )\n",
    "y_val_p = torch.Tensor( y_val ).unsqueeze(-1)\n",
    "b_val_p = torch.Tensor( b_val ).unsqueeze(-1)\n",
    "\n",
    "X_test_p = torch.Tensor( X_test )\n",
    "y_test_p = torch.Tensor( y_test ).unsqueeze(-1)\n",
    "b_test_p = torch.Tensor( b_test ).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dataset = cwola_data( X_train_p, y_train_p, b_train_p )\n",
    "val_dataset = cwola_data( X_val_p, y_val_p, b_val_p )\n",
    "tst_dataset = cwola_data( X_test_p, y_test_p, b_test_p )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dataloader = DataLoader( trn_dataset, batch_size=64, shuffle=True )\n",
    "val_dataloader = DataLoader( val_dataset, batch_size=64, shuffle=True )\n",
    "tst_dataloader = DataLoader( tst_dataset, batch_size=64, shuffle=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the CWoLa classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CWoLa classifier is just a regular binary classifier, so there's not much work to do here.  We define the classifier below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cwolaNet( torch.nn.Module ):\n",
    "    \n",
    "    def __init__( self, data_dim, hiddenlayer_size ):\n",
    "        super( cwolaNet, self ).__init__()\n",
    "                \n",
    "        self.layer1 = nn.Linear( data_dim, hiddenlayer_size )\n",
    "        self.relu_1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear( hiddenlayer_size, hiddenlayer_size )\n",
    "        self.relu_2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear( hiddenlayer_size, 1 )\n",
    "        \n",
    "    def forward( self, batch ):\n",
    "        \n",
    "        x = self.layer1( batch )\n",
    "        x = self.relu_1( x )\n",
    "        x = self.layer2( x )\n",
    "        x = self.relu_2( x )\n",
    "        x = self.layer3( x )\n",
    "        \n",
    "        return torch.sigmoid( x )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimising the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference with CWoLa and a regular supervised classifier is in how we train it, as with CWoLa we train the classifier on the bin labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we set the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the binary cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When defining the training loop and the functions to evaluate the training and validation losses we need to be careful that we pass the bin label to the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch( dataloader, model, loss_fn, optimizer ):\n",
    "    \n",
    "    size = len( dataloader.dataset )\n",
    "    \n",
    "    for batch, (X, y, b) in enumerate(dataloader):\n",
    "\n",
    "        # pass data through network\n",
    "        pred = model(X)\n",
    "        \n",
    "        # compute loss\n",
    "        loss = loss_fn( pred, b )\n",
    "\n",
    "        # reset gradients in optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # update weights with optimizer\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print the training loss every 100 updates\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print( f\"current batch loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_pass( dataloader, model, loss_fn ):\n",
    "    \n",
    "    size = len( dataloader.dataset )\n",
    "    num_batches = len( dataloader )\n",
    "    vl = 0.0\n",
    "\n",
    "    # we don't need gradients here since we only use the forward pass\n",
    "    with torch.no_grad():\n",
    "        for X, y, b in dataloader:\n",
    "            pred = model( X )\n",
    "            vl += loss_fn( pred, b ).item()\n",
    "\n",
    "    vl /= num_batches\n",
    "    print( f\"avg val loss per batch: {vl:>8f}\" )\n",
    "    \n",
    "    return vl\n",
    "\n",
    "def trn_pass( dataloader, model, loss_fn ):\n",
    "    \n",
    "    size = len( dataloader.dataset )\n",
    "    num_batches = len( dataloader )\n",
    "    tl = 0.0\n",
    "\n",
    "    # we don't need gradients here since we only use the forward pass\n",
    "    with torch.no_grad():\n",
    "        for X, y, b in dataloader:\n",
    "            pred = model( X )\n",
    "            tl += loss_fn( pred, b ).item()\n",
    "\n",
    "    tl /= num_batches\n",
    "    print( f\"avg trn loss per batch: {tl:>8f}\" )\n",
    "    \n",
    "    return tl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can optimise the network in the same way we would optimise a supervised classifier.\n",
    "\n",
    "Since we have so many events here we won't train the network for too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------\n",
      "model architecture \n",
      "-----------------------------------------------\n",
      "cwolaNet(\n",
      "  (layer1): Linear(in_features=4, out_features=64, bias=True)\n",
      "  (relu_1): ReLU()\n",
      "  (layer2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (relu_2): ReLU()\n",
      "  (layer3): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "-----------------------------------------------\n",
      "Epoch 1\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.792623  [    0/900000]\n",
      "current batch loss: 0.712343  [ 6400/900000]\n",
      "current batch loss: 0.763443  [12800/900000]\n",
      "current batch loss: 0.776673  [19200/900000]\n",
      "current batch loss: 0.760521  [25600/900000]\n",
      "current batch loss: 0.669799  [32000/900000]\n",
      "current batch loss: 0.687167  [38400/900000]\n",
      "current batch loss: 0.648401  [44800/900000]\n",
      "current batch loss: 0.637791  [51200/900000]\n",
      "current batch loss: 0.640539  [57600/900000]\n",
      "current batch loss: 0.650114  [64000/900000]\n",
      "current batch loss: 0.722756  [70400/900000]\n",
      "current batch loss: 0.722564  [76800/900000]\n",
      "current batch loss: 0.678790  [83200/900000]\n",
      "current batch loss: 0.672646  [89600/900000]\n",
      "current batch loss: 0.641436  [96000/900000]\n",
      "current batch loss: 0.717810  [102400/900000]\n",
      "current batch loss: 0.657671  [108800/900000]\n",
      "current batch loss: 0.719797  [115200/900000]\n",
      "current batch loss: 0.712856  [121600/900000]\n",
      "current batch loss: 0.686791  [128000/900000]\n",
      "current batch loss: 0.755316  [134400/900000]\n",
      "current batch loss: 0.661452  [140800/900000]\n",
      "current batch loss: 0.592474  [147200/900000]\n",
      "current batch loss: 0.646435  [153600/900000]\n",
      "current batch loss: 0.623314  [160000/900000]\n",
      "current batch loss: 0.641800  [166400/900000]\n",
      "current batch loss: 0.713378  [172800/900000]\n",
      "current batch loss: 0.674311  [179200/900000]\n",
      "current batch loss: 0.686305  [185600/900000]\n",
      "current batch loss: 0.605205  [192000/900000]\n",
      "current batch loss: 0.643968  [198400/900000]\n",
      "current batch loss: 0.697802  [204800/900000]\n",
      "current batch loss: 0.684736  [211200/900000]\n",
      "current batch loss: 0.674526  [217600/900000]\n",
      "current batch loss: 0.670081  [224000/900000]\n",
      "current batch loss: 0.609949  [230400/900000]\n",
      "current batch loss: 0.707382  [236800/900000]\n",
      "current batch loss: 0.676690  [243200/900000]\n",
      "current batch loss: 0.628668  [249600/900000]\n",
      "current batch loss: 0.690392  [256000/900000]\n",
      "current batch loss: 0.651887  [262400/900000]\n",
      "current batch loss: 0.650051  [268800/900000]\n",
      "current batch loss: 0.671954  [275200/900000]\n",
      "current batch loss: 0.652156  [281600/900000]\n",
      "current batch loss: 0.619575  [288000/900000]\n",
      "current batch loss: 0.714081  [294400/900000]\n",
      "current batch loss: 0.673078  [300800/900000]\n",
      "current batch loss: 0.633459  [307200/900000]\n",
      "current batch loss: 0.635185  [313600/900000]\n",
      "current batch loss: 0.722342  [320000/900000]\n",
      "current batch loss: 0.621492  [326400/900000]\n",
      "current batch loss: 0.638734  [332800/900000]\n",
      "current batch loss: 0.683943  [339200/900000]\n",
      "current batch loss: 0.667826  [345600/900000]\n",
      "current batch loss: 0.685739  [352000/900000]\n",
      "current batch loss: 0.671885  [358400/900000]\n",
      "current batch loss: 0.682419  [364800/900000]\n",
      "current batch loss: 0.611403  [371200/900000]\n",
      "current batch loss: 0.681635  [377600/900000]\n",
      "current batch loss: 0.681769  [384000/900000]\n",
      "current batch loss: 0.613042  [390400/900000]\n",
      "current batch loss: 0.648941  [396800/900000]\n",
      "current batch loss: 0.704999  [403200/900000]\n",
      "current batch loss: 0.763884  [409600/900000]\n",
      "current batch loss: 0.622293  [416000/900000]\n",
      "current batch loss: 0.636202  [422400/900000]\n",
      "current batch loss: 0.666339  [428800/900000]\n",
      "current batch loss: 0.608951  [435200/900000]\n",
      "current batch loss: 0.667787  [441600/900000]\n",
      "current batch loss: 0.624551  [448000/900000]\n",
      "current batch loss: 0.621781  [454400/900000]\n",
      "current batch loss: 0.631807  [460800/900000]\n",
      "current batch loss: 0.673648  [467200/900000]\n",
      "current batch loss: 0.575664  [473600/900000]\n",
      "current batch loss: 0.606800  [480000/900000]\n",
      "current batch loss: 0.625358  [486400/900000]\n",
      "current batch loss: 0.709686  [492800/900000]\n",
      "current batch loss: 0.598087  [499200/900000]\n",
      "current batch loss: 0.661381  [505600/900000]\n",
      "current batch loss: 0.684845  [512000/900000]\n",
      "current batch loss: 0.603418  [518400/900000]\n",
      "current batch loss: 0.606403  [524800/900000]\n",
      "current batch loss: 0.677619  [531200/900000]\n",
      "current batch loss: 0.731323  [537600/900000]\n",
      "current batch loss: 0.653256  [544000/900000]\n",
      "current batch loss: 0.644999  [550400/900000]\n",
      "current batch loss: 0.739501  [556800/900000]\n",
      "current batch loss: 0.644633  [563200/900000]\n",
      "current batch loss: 0.606149  [569600/900000]\n",
      "current batch loss: 0.650313  [576000/900000]\n",
      "current batch loss: 0.654788  [582400/900000]\n",
      "current batch loss: 0.613453  [588800/900000]\n",
      "current batch loss: 0.608936  [595200/900000]\n",
      "current batch loss: 0.678540  [601600/900000]\n",
      "current batch loss: 0.680140  [608000/900000]\n",
      "current batch loss: 0.655363  [614400/900000]\n",
      "current batch loss: 0.676025  [620800/900000]\n",
      "current batch loss: 0.647052  [627200/900000]\n",
      "current batch loss: 0.693929  [633600/900000]\n",
      "current batch loss: 0.706377  [640000/900000]\n",
      "current batch loss: 0.680921  [646400/900000]\n",
      "current batch loss: 0.638839  [652800/900000]\n",
      "current batch loss: 0.686718  [659200/900000]\n",
      "current batch loss: 0.667704  [665600/900000]\n",
      "current batch loss: 0.604258  [672000/900000]\n",
      "current batch loss: 0.622335  [678400/900000]\n",
      "current batch loss: 0.617786  [684800/900000]\n",
      "current batch loss: 0.677755  [691200/900000]\n",
      "current batch loss: 0.670959  [697600/900000]\n",
      "current batch loss: 0.587167  [704000/900000]\n",
      "current batch loss: 0.634244  [710400/900000]\n",
      "current batch loss: 0.667965  [716800/900000]\n",
      "current batch loss: 0.670239  [723200/900000]\n",
      "current batch loss: 0.662366  [729600/900000]\n",
      "current batch loss: 0.656234  [736000/900000]\n",
      "current batch loss: 0.633674  [742400/900000]\n",
      "current batch loss: 0.607248  [748800/900000]\n",
      "current batch loss: 0.660674  [755200/900000]\n",
      "current batch loss: 0.662987  [761600/900000]\n",
      "current batch loss: 0.647181  [768000/900000]\n",
      "current batch loss: 0.683963  [774400/900000]\n",
      "current batch loss: 0.619240  [780800/900000]\n",
      "current batch loss: 0.639378  [787200/900000]\n",
      "current batch loss: 0.665384  [793600/900000]\n",
      "current batch loss: 0.648243  [800000/900000]\n",
      "current batch loss: 0.707691  [806400/900000]\n",
      "current batch loss: 0.637256  [812800/900000]\n",
      "current batch loss: 0.672781  [819200/900000]\n",
      "current batch loss: 0.644672  [825600/900000]\n",
      "current batch loss: 0.721874  [832000/900000]\n",
      "current batch loss: 0.672388  [838400/900000]\n",
      "current batch loss: 0.629980  [844800/900000]\n",
      "current batch loss: 0.589354  [851200/900000]\n",
      "current batch loss: 0.655207  [857600/900000]\n",
      "current batch loss: 0.714264  [864000/900000]\n",
      "current batch loss: 0.591311  [870400/900000]\n",
      "current batch loss: 0.721066  [876800/900000]\n",
      "current batch loss: 0.707075  [883200/900000]\n",
      "current batch loss: 0.638522  [889600/900000]\n",
      "current batch loss: 0.607454  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.647819\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.646324\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 2\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.616814  [    0/900000]\n",
      "current batch loss: 0.700397  [ 6400/900000]\n",
      "current batch loss: 0.628050  [12800/900000]\n",
      "current batch loss: 0.616942  [19200/900000]\n",
      "current batch loss: 0.622149  [25600/900000]\n",
      "current batch loss: 0.650381  [32000/900000]\n",
      "current batch loss: 0.603565  [38400/900000]\n",
      "current batch loss: 0.658141  [44800/900000]\n",
      "current batch loss: 0.688797  [51200/900000]\n",
      "current batch loss: 0.551272  [57600/900000]\n",
      "current batch loss: 0.634886  [64000/900000]\n",
      "current batch loss: 0.694298  [70400/900000]\n",
      "current batch loss: 0.598872  [76800/900000]\n",
      "current batch loss: 0.635917  [83200/900000]\n",
      "current batch loss: 0.658670  [89600/900000]\n",
      "current batch loss: 0.648830  [96000/900000]\n",
      "current batch loss: 0.647965  [102400/900000]\n",
      "current batch loss: 0.637219  [108800/900000]\n",
      "current batch loss: 0.636875  [115200/900000]\n",
      "current batch loss: 0.740037  [121600/900000]\n",
      "current batch loss: 0.660036  [128000/900000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.693525  [134400/900000]\n",
      "current batch loss: 0.617278  [140800/900000]\n",
      "current batch loss: 0.612064  [147200/900000]\n",
      "current batch loss: 0.648674  [153600/900000]\n",
      "current batch loss: 0.678957  [160000/900000]\n",
      "current batch loss: 0.671588  [166400/900000]\n",
      "current batch loss: 0.591623  [172800/900000]\n",
      "current batch loss: 0.621382  [179200/900000]\n",
      "current batch loss: 0.637007  [185600/900000]\n",
      "current batch loss: 0.669996  [192000/900000]\n",
      "current batch loss: 0.665981  [198400/900000]\n",
      "current batch loss: 0.648951  [204800/900000]\n",
      "current batch loss: 0.548946  [211200/900000]\n",
      "current batch loss: 0.601353  [217600/900000]\n",
      "current batch loss: 0.629203  [224000/900000]\n",
      "current batch loss: 0.634871  [230400/900000]\n",
      "current batch loss: 0.642401  [236800/900000]\n",
      "current batch loss: 0.630574  [243200/900000]\n",
      "current batch loss: 0.588158  [249600/900000]\n",
      "current batch loss: 0.636131  [256000/900000]\n",
      "current batch loss: 0.727524  [262400/900000]\n",
      "current batch loss: 0.655501  [268800/900000]\n",
      "current batch loss: 0.589243  [275200/900000]\n",
      "current batch loss: 0.610764  [281600/900000]\n",
      "current batch loss: 0.675064  [288000/900000]\n",
      "current batch loss: 0.591868  [294400/900000]\n",
      "current batch loss: 0.596032  [300800/900000]\n",
      "current batch loss: 0.601339  [307200/900000]\n",
      "current batch loss: 0.665842  [313600/900000]\n",
      "current batch loss: 0.636521  [320000/900000]\n",
      "current batch loss: 0.642586  [326400/900000]\n",
      "current batch loss: 0.623137  [332800/900000]\n",
      "current batch loss: 0.637901  [339200/900000]\n",
      "current batch loss: 0.678253  [345600/900000]\n",
      "current batch loss: 0.589612  [352000/900000]\n",
      "current batch loss: 0.657030  [358400/900000]\n",
      "current batch loss: 0.599477  [364800/900000]\n",
      "current batch loss: 0.611910  [371200/900000]\n",
      "current batch loss: 0.666527  [377600/900000]\n",
      "current batch loss: 0.607577  [384000/900000]\n",
      "current batch loss: 0.648422  [390400/900000]\n",
      "current batch loss: 0.715968  [396800/900000]\n",
      "current batch loss: 0.738005  [403200/900000]\n",
      "current batch loss: 0.651659  [409600/900000]\n",
      "current batch loss: 0.614259  [416000/900000]\n",
      "current batch loss: 0.624691  [422400/900000]\n",
      "current batch loss: 0.653970  [428800/900000]\n",
      "current batch loss: 0.657876  [435200/900000]\n",
      "current batch loss: 0.673693  [441600/900000]\n",
      "current batch loss: 0.598099  [448000/900000]\n",
      "current batch loss: 0.567562  [454400/900000]\n",
      "current batch loss: 0.677783  [460800/900000]\n",
      "current batch loss: 0.644981  [467200/900000]\n",
      "current batch loss: 0.680444  [473600/900000]\n",
      "current batch loss: 0.679019  [480000/900000]\n",
      "current batch loss: 0.677698  [486400/900000]\n",
      "current batch loss: 0.668045  [492800/900000]\n",
      "current batch loss: 0.635500  [499200/900000]\n",
      "current batch loss: 0.595093  [505600/900000]\n",
      "current batch loss: 0.627779  [512000/900000]\n",
      "current batch loss: 0.708119  [518400/900000]\n",
      "current batch loss: 0.585493  [524800/900000]\n",
      "current batch loss: 0.639198  [531200/900000]\n",
      "current batch loss: 0.608788  [537600/900000]\n",
      "current batch loss: 0.622210  [544000/900000]\n",
      "current batch loss: 0.630890  [550400/900000]\n",
      "current batch loss: 0.590852  [556800/900000]\n",
      "current batch loss: 0.695252  [563200/900000]\n",
      "current batch loss: 0.619218  [569600/900000]\n",
      "current batch loss: 0.613277  [576000/900000]\n",
      "current batch loss: 0.603251  [582400/900000]\n",
      "current batch loss: 0.657693  [588800/900000]\n",
      "current batch loss: 0.648353  [595200/900000]\n",
      "current batch loss: 0.672975  [601600/900000]\n",
      "current batch loss: 0.647890  [608000/900000]\n",
      "current batch loss: 0.656880  [614400/900000]\n",
      "current batch loss: 0.577944  [620800/900000]\n",
      "current batch loss: 0.627771  [627200/900000]\n",
      "current batch loss: 0.625727  [633600/900000]\n",
      "current batch loss: 0.659368  [640000/900000]\n",
      "current batch loss: 0.613956  [646400/900000]\n",
      "current batch loss: 0.638601  [652800/900000]\n",
      "current batch loss: 0.672802  [659200/900000]\n",
      "current batch loss: 0.702865  [665600/900000]\n",
      "current batch loss: 0.674936  [672000/900000]\n",
      "current batch loss: 0.675534  [678400/900000]\n",
      "current batch loss: 0.640617  [684800/900000]\n",
      "current batch loss: 0.627475  [691200/900000]\n",
      "current batch loss: 0.644095  [697600/900000]\n",
      "current batch loss: 0.652487  [704000/900000]\n",
      "current batch loss: 0.627174  [710400/900000]\n",
      "current batch loss: 0.638145  [716800/900000]\n",
      "current batch loss: 0.638687  [723200/900000]\n",
      "current batch loss: 0.596229  [729600/900000]\n",
      "current batch loss: 0.628473  [736000/900000]\n",
      "current batch loss: 0.638688  [742400/900000]\n",
      "current batch loss: 0.629677  [748800/900000]\n",
      "current batch loss: 0.617632  [755200/900000]\n",
      "current batch loss: 0.628557  [761600/900000]\n",
      "current batch loss: 0.625068  [768000/900000]\n",
      "current batch loss: 0.651445  [774400/900000]\n",
      "current batch loss: 0.625793  [780800/900000]\n",
      "current batch loss: 0.662515  [787200/900000]\n",
      "current batch loss: 0.632165  [793600/900000]\n",
      "current batch loss: 0.692791  [800000/900000]\n",
      "current batch loss: 0.648141  [806400/900000]\n",
      "current batch loss: 0.610973  [812800/900000]\n",
      "current batch loss: 0.647964  [819200/900000]\n",
      "current batch loss: 0.668668  [825600/900000]\n",
      "current batch loss: 0.581773  [832000/900000]\n",
      "current batch loss: 0.675340  [838400/900000]\n",
      "current batch loss: 0.647178  [844800/900000]\n",
      "current batch loss: 0.649157  [851200/900000]\n",
      "current batch loss: 0.669199  [857600/900000]\n",
      "current batch loss: 0.683193  [864000/900000]\n",
      "current batch loss: 0.558860  [870400/900000]\n",
      "current batch loss: 0.650890  [876800/900000]\n",
      "current batch loss: 0.696016  [883200/900000]\n",
      "current batch loss: 0.626359  [889600/900000]\n",
      "current batch loss: 0.570680  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.640499\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.639462\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 3\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.606824  [    0/900000]\n",
      "current batch loss: 0.585364  [ 6400/900000]\n",
      "current batch loss: 0.563223  [12800/900000]\n",
      "current batch loss: 0.698724  [19200/900000]\n",
      "current batch loss: 0.640733  [25600/900000]\n",
      "current batch loss: 0.576629  [32000/900000]\n",
      "current batch loss: 0.672997  [38400/900000]\n",
      "current batch loss: 0.695304  [44800/900000]\n",
      "current batch loss: 0.696459  [51200/900000]\n",
      "current batch loss: 0.650617  [57600/900000]\n",
      "current batch loss: 0.576163  [64000/900000]\n",
      "current batch loss: 0.619199  [70400/900000]\n",
      "current batch loss: 0.623882  [76800/900000]\n",
      "current batch loss: 0.632817  [83200/900000]\n",
      "current batch loss: 0.666511  [89600/900000]\n",
      "current batch loss: 0.642838  [96000/900000]\n",
      "current batch loss: 0.691512  [102400/900000]\n",
      "current batch loss: 0.768303  [108800/900000]\n",
      "current batch loss: 0.561678  [115200/900000]\n",
      "current batch loss: 0.552086  [121600/900000]\n",
      "current batch loss: 0.592121  [128000/900000]\n",
      "current batch loss: 0.596733  [134400/900000]\n",
      "current batch loss: 0.549039  [140800/900000]\n",
      "current batch loss: 0.602287  [147200/900000]\n",
      "current batch loss: 0.663308  [153600/900000]\n",
      "current batch loss: 0.640299  [160000/900000]\n",
      "current batch loss: 0.627252  [166400/900000]\n",
      "current batch loss: 0.611341  [172800/900000]\n",
      "current batch loss: 0.657350  [179200/900000]\n",
      "current batch loss: 0.584268  [185600/900000]\n",
      "current batch loss: 0.665622  [192000/900000]\n",
      "current batch loss: 0.629334  [198400/900000]\n",
      "current batch loss: 0.644388  [204800/900000]\n",
      "current batch loss: 0.670375  [211200/900000]\n",
      "current batch loss: 0.637511  [217600/900000]\n",
      "current batch loss: 0.642587  [224000/900000]\n",
      "current batch loss: 0.610667  [230400/900000]\n",
      "current batch loss: 0.622336  [236800/900000]\n",
      "current batch loss: 0.569975  [243200/900000]\n",
      "current batch loss: 0.634770  [249600/900000]\n",
      "current batch loss: 0.659827  [256000/900000]\n",
      "current batch loss: 0.638301  [262400/900000]\n",
      "current batch loss: 0.639676  [268800/900000]\n",
      "current batch loss: 0.599593  [275200/900000]\n",
      "current batch loss: 0.641585  [281600/900000]\n",
      "current batch loss: 0.628786  [288000/900000]\n",
      "current batch loss: 0.613011  [294400/900000]\n",
      "current batch loss: 0.669876  [300800/900000]\n",
      "current batch loss: 0.650424  [307200/900000]\n",
      "current batch loss: 0.603989  [313600/900000]\n",
      "current batch loss: 0.661352  [320000/900000]\n",
      "current batch loss: 0.698025  [326400/900000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.610461  [332800/900000]\n",
      "current batch loss: 0.598198  [339200/900000]\n",
      "current batch loss: 0.644099  [345600/900000]\n",
      "current batch loss: 0.629855  [352000/900000]\n",
      "current batch loss: 0.632144  [358400/900000]\n",
      "current batch loss: 0.669864  [364800/900000]\n",
      "current batch loss: 0.646260  [371200/900000]\n",
      "current batch loss: 0.541356  [377600/900000]\n",
      "current batch loss: 0.658320  [384000/900000]\n",
      "current batch loss: 0.554158  [390400/900000]\n",
      "current batch loss: 0.606635  [396800/900000]\n",
      "current batch loss: 0.664699  [403200/900000]\n",
      "current batch loss: 0.633700  [409600/900000]\n",
      "current batch loss: 0.667818  [416000/900000]\n",
      "current batch loss: 0.602163  [422400/900000]\n",
      "current batch loss: 0.649641  [428800/900000]\n",
      "current batch loss: 0.702940  [435200/900000]\n",
      "current batch loss: 0.628979  [441600/900000]\n",
      "current batch loss: 0.649248  [448000/900000]\n",
      "current batch loss: 0.639356  [454400/900000]\n",
      "current batch loss: 0.642738  [460800/900000]\n",
      "current batch loss: 0.619441  [467200/900000]\n",
      "current batch loss: 0.716922  [473600/900000]\n",
      "current batch loss: 0.583390  [480000/900000]\n",
      "current batch loss: 0.706886  [486400/900000]\n",
      "current batch loss: 0.576335  [492800/900000]\n",
      "current batch loss: 0.574428  [499200/900000]\n",
      "current batch loss: 0.634573  [505600/900000]\n",
      "current batch loss: 0.643308  [512000/900000]\n",
      "current batch loss: 0.679344  [518400/900000]\n",
      "current batch loss: 0.712347  [524800/900000]\n",
      "current batch loss: 0.603884  [531200/900000]\n",
      "current batch loss: 0.662258  [537600/900000]\n",
      "current batch loss: 0.662335  [544000/900000]\n",
      "current batch loss: 0.609595  [550400/900000]\n",
      "current batch loss: 0.584874  [556800/900000]\n",
      "current batch loss: 0.634752  [563200/900000]\n",
      "current batch loss: 0.603870  [569600/900000]\n",
      "current batch loss: 0.629515  [576000/900000]\n",
      "current batch loss: 0.685473  [582400/900000]\n",
      "current batch loss: 0.646035  [588800/900000]\n",
      "current batch loss: 0.699919  [595200/900000]\n",
      "current batch loss: 0.704727  [601600/900000]\n",
      "current batch loss: 0.679036  [608000/900000]\n",
      "current batch loss: 0.635633  [614400/900000]\n",
      "current batch loss: 0.629545  [620800/900000]\n",
      "current batch loss: 0.614114  [627200/900000]\n",
      "current batch loss: 0.595608  [633600/900000]\n",
      "current batch loss: 0.625668  [640000/900000]\n",
      "current batch loss: 0.686357  [646400/900000]\n",
      "current batch loss: 0.627554  [652800/900000]\n",
      "current batch loss: 0.630568  [659200/900000]\n",
      "current batch loss: 0.662696  [665600/900000]\n",
      "current batch loss: 0.584846  [672000/900000]\n",
      "current batch loss: 0.714946  [678400/900000]\n",
      "current batch loss: 0.736983  [684800/900000]\n",
      "current batch loss: 0.658116  [691200/900000]\n",
      "current batch loss: 0.605148  [697600/900000]\n",
      "current batch loss: 0.628409  [704000/900000]\n",
      "current batch loss: 0.613153  [710400/900000]\n",
      "current batch loss: 0.592121  [716800/900000]\n",
      "current batch loss: 0.602687  [723200/900000]\n",
      "current batch loss: 0.595156  [729600/900000]\n",
      "current batch loss: 0.603535  [736000/900000]\n",
      "current batch loss: 0.664972  [742400/900000]\n",
      "current batch loss: 0.656972  [748800/900000]\n",
      "current batch loss: 0.613594  [755200/900000]\n",
      "current batch loss: 0.620529  [761600/900000]\n",
      "current batch loss: 0.705226  [768000/900000]\n",
      "current batch loss: 0.652851  [774400/900000]\n",
      "current batch loss: 0.704375  [780800/900000]\n",
      "current batch loss: 0.670645  [787200/900000]\n",
      "current batch loss: 0.648171  [793600/900000]\n",
      "current batch loss: 0.603325  [800000/900000]\n",
      "current batch loss: 0.650862  [806400/900000]\n",
      "current batch loss: 0.610801  [812800/900000]\n",
      "current batch loss: 0.655897  [819200/900000]\n",
      "current batch loss: 0.605213  [825600/900000]\n",
      "current batch loss: 0.607639  [832000/900000]\n",
      "current batch loss: 0.658295  [838400/900000]\n",
      "current batch loss: 0.633086  [844800/900000]\n",
      "current batch loss: 0.680624  [851200/900000]\n",
      "current batch loss: 0.714107  [857600/900000]\n",
      "current batch loss: 0.610180  [864000/900000]\n",
      "current batch loss: 0.725164  [870400/900000]\n",
      "current batch loss: 0.638454  [876800/900000]\n",
      "current batch loss: 0.636129  [883200/900000]\n",
      "current batch loss: 0.646316  [889600/900000]\n",
      "current batch loss: 0.632497  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.639713\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.638494\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 4\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.676978  [    0/900000]\n",
      "current batch loss: 0.661756  [ 6400/900000]\n",
      "current batch loss: 0.646016  [12800/900000]\n",
      "current batch loss: 0.695142  [19200/900000]\n",
      "current batch loss: 0.645096  [25600/900000]\n",
      "current batch loss: 0.684695  [32000/900000]\n",
      "current batch loss: 0.688758  [38400/900000]\n",
      "current batch loss: 0.634032  [44800/900000]\n",
      "current batch loss: 0.704437  [51200/900000]\n",
      "current batch loss: 0.562615  [57600/900000]\n",
      "current batch loss: 0.616229  [64000/900000]\n",
      "current batch loss: 0.660862  [70400/900000]\n",
      "current batch loss: 0.647116  [76800/900000]\n",
      "current batch loss: 0.640724  [83200/900000]\n",
      "current batch loss: 0.610225  [89600/900000]\n",
      "current batch loss: 0.722791  [96000/900000]\n",
      "current batch loss: 0.628473  [102400/900000]\n",
      "current batch loss: 0.600858  [108800/900000]\n",
      "current batch loss: 0.655041  [115200/900000]\n",
      "current batch loss: 0.632063  [121600/900000]\n",
      "current batch loss: 0.593985  [128000/900000]\n",
      "current batch loss: 0.605350  [134400/900000]\n",
      "current batch loss: 0.685421  [140800/900000]\n",
      "current batch loss: 0.647185  [147200/900000]\n",
      "current batch loss: 0.678277  [153600/900000]\n",
      "current batch loss: 0.627685  [160000/900000]\n",
      "current batch loss: 0.686977  [166400/900000]\n",
      "current batch loss: 0.596709  [172800/900000]\n",
      "current batch loss: 0.685991  [179200/900000]\n",
      "current batch loss: 0.640184  [185600/900000]\n",
      "current batch loss: 0.678054  [192000/900000]\n",
      "current batch loss: 0.639987  [198400/900000]\n",
      "current batch loss: 0.638928  [204800/900000]\n",
      "current batch loss: 0.668365  [211200/900000]\n",
      "current batch loss: 0.627859  [217600/900000]\n",
      "current batch loss: 0.605588  [224000/900000]\n",
      "current batch loss: 0.629779  [230400/900000]\n",
      "current batch loss: 0.668996  [236800/900000]\n",
      "current batch loss: 0.673703  [243200/900000]\n",
      "current batch loss: 0.607523  [249600/900000]\n",
      "current batch loss: 0.607758  [256000/900000]\n",
      "current batch loss: 0.672610  [262400/900000]\n",
      "current batch loss: 0.637565  [268800/900000]\n",
      "current batch loss: 0.664649  [275200/900000]\n",
      "current batch loss: 0.654300  [281600/900000]\n",
      "current batch loss: 0.622280  [288000/900000]\n",
      "current batch loss: 0.651327  [294400/900000]\n",
      "current batch loss: 0.623717  [300800/900000]\n",
      "current batch loss: 0.690635  [307200/900000]\n",
      "current batch loss: 0.676745  [313600/900000]\n",
      "current batch loss: 0.620037  [320000/900000]\n",
      "current batch loss: 0.593456  [326400/900000]\n",
      "current batch loss: 0.610937  [332800/900000]\n",
      "current batch loss: 0.619976  [339200/900000]\n",
      "current batch loss: 0.663017  [345600/900000]\n",
      "current batch loss: 0.667223  [352000/900000]\n",
      "current batch loss: 0.633214  [358400/900000]\n",
      "current batch loss: 0.666941  [364800/900000]\n",
      "current batch loss: 0.613440  [371200/900000]\n",
      "current batch loss: 0.574987  [377600/900000]\n",
      "current batch loss: 0.575300  [384000/900000]\n",
      "current batch loss: 0.673409  [390400/900000]\n",
      "current batch loss: 0.661502  [396800/900000]\n",
      "current batch loss: 0.668365  [403200/900000]\n",
      "current batch loss: 0.657203  [409600/900000]\n",
      "current batch loss: 0.621321  [416000/900000]\n",
      "current batch loss: 0.585404  [422400/900000]\n",
      "current batch loss: 0.606753  [428800/900000]\n",
      "current batch loss: 0.626599  [435200/900000]\n",
      "current batch loss: 0.610688  [441600/900000]\n",
      "current batch loss: 0.548040  [448000/900000]\n",
      "current batch loss: 0.676525  [454400/900000]\n",
      "current batch loss: 0.639518  [460800/900000]\n",
      "current batch loss: 0.585757  [467200/900000]\n",
      "current batch loss: 0.635732  [473600/900000]\n",
      "current batch loss: 0.589324  [480000/900000]\n",
      "current batch loss: 0.608736  [486400/900000]\n",
      "current batch loss: 0.569356  [492800/900000]\n",
      "current batch loss: 0.626559  [499200/900000]\n",
      "current batch loss: 0.656661  [505600/900000]\n",
      "current batch loss: 0.634500  [512000/900000]\n",
      "current batch loss: 0.602902  [518400/900000]\n",
      "current batch loss: 0.605698  [524800/900000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.684365  [531200/900000]\n",
      "current batch loss: 0.633027  [537600/900000]\n",
      "current batch loss: 0.654466  [544000/900000]\n",
      "current batch loss: 0.667071  [550400/900000]\n",
      "current batch loss: 0.595218  [556800/900000]\n",
      "current batch loss: 0.621576  [563200/900000]\n",
      "current batch loss: 0.648997  [569600/900000]\n",
      "current batch loss: 0.680851  [576000/900000]\n",
      "current batch loss: 0.622850  [582400/900000]\n",
      "current batch loss: 0.581645  [588800/900000]\n",
      "current batch loss: 0.655746  [595200/900000]\n",
      "current batch loss: 0.661219  [601600/900000]\n",
      "current batch loss: 0.614719  [608000/900000]\n",
      "current batch loss: 0.659513  [614400/900000]\n",
      "current batch loss: 0.621244  [620800/900000]\n",
      "current batch loss: 0.646232  [627200/900000]\n",
      "current batch loss: 0.666188  [633600/900000]\n",
      "current batch loss: 0.667162  [640000/900000]\n",
      "current batch loss: 0.633241  [646400/900000]\n",
      "current batch loss: 0.605777  [652800/900000]\n",
      "current batch loss: 0.683928  [659200/900000]\n",
      "current batch loss: 0.615575  [665600/900000]\n",
      "current batch loss: 0.653563  [672000/900000]\n",
      "current batch loss: 0.658779  [678400/900000]\n",
      "current batch loss: 0.660079  [684800/900000]\n",
      "current batch loss: 0.597176  [691200/900000]\n",
      "current batch loss: 0.696321  [697600/900000]\n",
      "current batch loss: 0.652453  [704000/900000]\n",
      "current batch loss: 0.570209  [710400/900000]\n",
      "current batch loss: 0.650991  [716800/900000]\n",
      "current batch loss: 0.675154  [723200/900000]\n",
      "current batch loss: 0.589383  [729600/900000]\n",
      "current batch loss: 0.645897  [736000/900000]\n",
      "current batch loss: 0.626634  [742400/900000]\n",
      "current batch loss: 0.652515  [748800/900000]\n",
      "current batch loss: 0.667277  [755200/900000]\n",
      "current batch loss: 0.674169  [761600/900000]\n",
      "current batch loss: 0.602978  [768000/900000]\n",
      "current batch loss: 0.605726  [774400/900000]\n",
      "current batch loss: 0.692688  [780800/900000]\n",
      "current batch loss: 0.609393  [787200/900000]\n",
      "current batch loss: 0.670609  [793600/900000]\n",
      "current batch loss: 0.615039  [800000/900000]\n",
      "current batch loss: 0.641290  [806400/900000]\n",
      "current batch loss: 0.668905  [812800/900000]\n",
      "current batch loss: 0.671579  [819200/900000]\n",
      "current batch loss: 0.614975  [825600/900000]\n",
      "current batch loss: 0.584298  [832000/900000]\n",
      "current batch loss: 0.754197  [838400/900000]\n",
      "current batch loss: 0.678599  [844800/900000]\n",
      "current batch loss: 0.577371  [851200/900000]\n",
      "current batch loss: 0.633642  [857600/900000]\n",
      "current batch loss: 0.743398  [864000/900000]\n",
      "current batch loss: 0.667791  [870400/900000]\n",
      "current batch loss: 0.633846  [876800/900000]\n",
      "current batch loss: 0.675617  [883200/900000]\n",
      "current batch loss: 0.638720  [889600/900000]\n",
      "current batch loss: 0.633912  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.640194\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.638613\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 5\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.579074  [    0/900000]\n",
      "current batch loss: 0.668850  [ 6400/900000]\n",
      "current batch loss: 0.629145  [12800/900000]\n",
      "current batch loss: 0.628140  [19200/900000]\n",
      "current batch loss: 0.722375  [25600/900000]\n",
      "current batch loss: 0.603826  [32000/900000]\n",
      "current batch loss: 0.628680  [38400/900000]\n",
      "current batch loss: 0.602079  [44800/900000]\n",
      "current batch loss: 0.735525  [51200/900000]\n",
      "current batch loss: 0.616345  [57600/900000]\n",
      "current batch loss: 0.669995  [64000/900000]\n",
      "current batch loss: 0.620459  [70400/900000]\n",
      "current batch loss: 0.648720  [76800/900000]\n",
      "current batch loss: 0.616337  [83200/900000]\n",
      "current batch loss: 0.621293  [89600/900000]\n",
      "current batch loss: 0.627687  [96000/900000]\n",
      "current batch loss: 0.643591  [102400/900000]\n",
      "current batch loss: 0.686456  [108800/900000]\n",
      "current batch loss: 0.677777  [115200/900000]\n",
      "current batch loss: 0.611592  [121600/900000]\n",
      "current batch loss: 0.681343  [128000/900000]\n",
      "current batch loss: 0.617623  [134400/900000]\n",
      "current batch loss: 0.620475  [140800/900000]\n",
      "current batch loss: 0.645853  [147200/900000]\n",
      "current batch loss: 0.629928  [153600/900000]\n",
      "current batch loss: 0.620120  [160000/900000]\n",
      "current batch loss: 0.678116  [166400/900000]\n",
      "current batch loss: 0.650640  [172800/900000]\n",
      "current batch loss: 0.665471  [179200/900000]\n",
      "current batch loss: 0.655723  [185600/900000]\n",
      "current batch loss: 0.555655  [192000/900000]\n",
      "current batch loss: 0.582326  [198400/900000]\n",
      "current batch loss: 0.711234  [204800/900000]\n",
      "current batch loss: 0.581151  [211200/900000]\n",
      "current batch loss: 0.592018  [217600/900000]\n",
      "current batch loss: 0.591505  [224000/900000]\n",
      "current batch loss: 0.657471  [230400/900000]\n",
      "current batch loss: 0.650486  [236800/900000]\n",
      "current batch loss: 0.619881  [243200/900000]\n",
      "current batch loss: 0.739942  [249600/900000]\n",
      "current batch loss: 0.665216  [256000/900000]\n",
      "current batch loss: 0.653162  [262400/900000]\n",
      "current batch loss: 0.594532  [268800/900000]\n",
      "current batch loss: 0.591935  [275200/900000]\n",
      "current batch loss: 0.705926  [281600/900000]\n",
      "current batch loss: 0.585693  [288000/900000]\n",
      "current batch loss: 0.686040  [294400/900000]\n",
      "current batch loss: 0.599194  [300800/900000]\n",
      "current batch loss: 0.700714  [307200/900000]\n",
      "current batch loss: 0.609133  [313600/900000]\n",
      "current batch loss: 0.620728  [320000/900000]\n",
      "current batch loss: 0.635884  [326400/900000]\n",
      "current batch loss: 0.699995  [332800/900000]\n",
      "current batch loss: 0.669782  [339200/900000]\n",
      "current batch loss: 0.681247  [345600/900000]\n",
      "current batch loss: 0.649514  [352000/900000]\n",
      "current batch loss: 0.564258  [358400/900000]\n",
      "current batch loss: 0.718753  [364800/900000]\n",
      "current batch loss: 0.658007  [371200/900000]\n",
      "current batch loss: 0.734617  [377600/900000]\n",
      "current batch loss: 0.573273  [384000/900000]\n",
      "current batch loss: 0.623039  [390400/900000]\n",
      "current batch loss: 0.642417  [396800/900000]\n",
      "current batch loss: 0.647784  [403200/900000]\n",
      "current batch loss: 0.664110  [409600/900000]\n",
      "current batch loss: 0.619347  [416000/900000]\n",
      "current batch loss: 0.601026  [422400/900000]\n",
      "current batch loss: 0.643010  [428800/900000]\n",
      "current batch loss: 0.584719  [435200/900000]\n",
      "current batch loss: 0.635118  [441600/900000]\n",
      "current batch loss: 0.641015  [448000/900000]\n",
      "current batch loss: 0.697631  [454400/900000]\n",
      "current batch loss: 0.613626  [460800/900000]\n",
      "current batch loss: 0.648079  [467200/900000]\n",
      "current batch loss: 0.633629  [473600/900000]\n",
      "current batch loss: 0.678031  [480000/900000]\n",
      "current batch loss: 0.587603  [486400/900000]\n",
      "current batch loss: 0.592992  [492800/900000]\n",
      "current batch loss: 0.606877  [499200/900000]\n",
      "current batch loss: 0.619997  [505600/900000]\n",
      "current batch loss: 0.610840  [512000/900000]\n",
      "current batch loss: 0.564245  [518400/900000]\n",
      "current batch loss: 0.712804  [524800/900000]\n",
      "current batch loss: 0.629960  [531200/900000]\n",
      "current batch loss: 0.666612  [537600/900000]\n",
      "current batch loss: 0.659312  [544000/900000]\n",
      "current batch loss: 0.646034  [550400/900000]\n",
      "current batch loss: 0.629053  [556800/900000]\n",
      "current batch loss: 0.682740  [563200/900000]\n",
      "current batch loss: 0.620313  [569600/900000]\n",
      "current batch loss: 0.627610  [576000/900000]\n",
      "current batch loss: 0.626798  [582400/900000]\n",
      "current batch loss: 0.712094  [588800/900000]\n",
      "current batch loss: 0.670648  [595200/900000]\n",
      "current batch loss: 0.685650  [601600/900000]\n",
      "current batch loss: 0.677768  [608000/900000]\n",
      "current batch loss: 0.636264  [614400/900000]\n",
      "current batch loss: 0.655700  [620800/900000]\n",
      "current batch loss: 0.577193  [627200/900000]\n",
      "current batch loss: 0.589909  [633600/900000]\n",
      "current batch loss: 0.701659  [640000/900000]\n",
      "current batch loss: 0.589976  [646400/900000]\n",
      "current batch loss: 0.602708  [652800/900000]\n",
      "current batch loss: 0.623733  [659200/900000]\n",
      "current batch loss: 0.656655  [665600/900000]\n",
      "current batch loss: 0.639479  [672000/900000]\n",
      "current batch loss: 0.591428  [678400/900000]\n",
      "current batch loss: 0.687544  [684800/900000]\n",
      "current batch loss: 0.691505  [691200/900000]\n",
      "current batch loss: 0.639250  [697600/900000]\n",
      "current batch loss: 0.603230  [704000/900000]\n",
      "current batch loss: 0.653355  [710400/900000]\n",
      "current batch loss: 0.628488  [716800/900000]\n",
      "current batch loss: 0.633391  [723200/900000]\n",
      "current batch loss: 0.669726  [729600/900000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.677195  [736000/900000]\n",
      "current batch loss: 0.738038  [742400/900000]\n",
      "current batch loss: 0.587983  [748800/900000]\n",
      "current batch loss: 0.615730  [755200/900000]\n",
      "current batch loss: 0.651910  [761600/900000]\n",
      "current batch loss: 0.599650  [768000/900000]\n",
      "current batch loss: 0.615377  [774400/900000]\n",
      "current batch loss: 0.597821  [780800/900000]\n",
      "current batch loss: 0.704545  [787200/900000]\n",
      "current batch loss: 0.624061  [793600/900000]\n",
      "current batch loss: 0.701296  [800000/900000]\n",
      "current batch loss: 0.567436  [806400/900000]\n",
      "current batch loss: 0.670076  [812800/900000]\n",
      "current batch loss: 0.619137  [819200/900000]\n",
      "current batch loss: 0.666178  [825600/900000]\n",
      "current batch loss: 0.632530  [832000/900000]\n",
      "current batch loss: 0.621620  [838400/900000]\n",
      "current batch loss: 0.653826  [844800/900000]\n",
      "current batch loss: 0.648385  [851200/900000]\n",
      "current batch loss: 0.574538  [857600/900000]\n",
      "current batch loss: 0.685150  [864000/900000]\n",
      "current batch loss: 0.647510  [870400/900000]\n",
      "current batch loss: 0.633003  [876800/900000]\n",
      "current batch loss: 0.608901  [883200/900000]\n",
      "current batch loss: 0.611139  [889600/900000]\n",
      "current batch loss: 0.673245  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.638073\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.636728\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 6\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.628877  [    0/900000]\n",
      "current batch loss: 0.702142  [ 6400/900000]\n",
      "current batch loss: 0.678167  [12800/900000]\n",
      "current batch loss: 0.650307  [19200/900000]\n",
      "current batch loss: 0.550697  [25600/900000]\n",
      "current batch loss: 0.623562  [32000/900000]\n",
      "current batch loss: 0.675886  [38400/900000]\n",
      "current batch loss: 0.619145  [44800/900000]\n",
      "current batch loss: 0.572851  [51200/900000]\n",
      "current batch loss: 0.689589  [57600/900000]\n",
      "current batch loss: 0.679669  [64000/900000]\n",
      "current batch loss: 0.714371  [70400/900000]\n",
      "current batch loss: 0.652578  [76800/900000]\n",
      "current batch loss: 0.653605  [83200/900000]\n",
      "current batch loss: 0.631296  [89600/900000]\n",
      "current batch loss: 0.679269  [96000/900000]\n",
      "current batch loss: 0.634966  [102400/900000]\n",
      "current batch loss: 0.703004  [108800/900000]\n",
      "current batch loss: 0.638854  [115200/900000]\n",
      "current batch loss: 0.658598  [121600/900000]\n",
      "current batch loss: 0.664625  [128000/900000]\n",
      "current batch loss: 0.645147  [134400/900000]\n",
      "current batch loss: 0.654354  [140800/900000]\n",
      "current batch loss: 0.593673  [147200/900000]\n",
      "current batch loss: 0.624036  [153600/900000]\n",
      "current batch loss: 0.655507  [160000/900000]\n",
      "current batch loss: 0.593708  [166400/900000]\n",
      "current batch loss: 0.618962  [172800/900000]\n",
      "current batch loss: 0.586114  [179200/900000]\n",
      "current batch loss: 0.631075  [185600/900000]\n",
      "current batch loss: 0.573821  [192000/900000]\n",
      "current batch loss: 0.637661  [198400/900000]\n",
      "current batch loss: 0.670761  [204800/900000]\n",
      "current batch loss: 0.643478  [211200/900000]\n",
      "current batch loss: 0.658917  [217600/900000]\n",
      "current batch loss: 0.677837  [224000/900000]\n",
      "current batch loss: 0.611326  [230400/900000]\n",
      "current batch loss: 0.581722  [236800/900000]\n",
      "current batch loss: 0.659611  [243200/900000]\n",
      "current batch loss: 0.659268  [249600/900000]\n",
      "current batch loss: 0.669415  [256000/900000]\n",
      "current batch loss: 0.575255  [262400/900000]\n",
      "current batch loss: 0.636150  [268800/900000]\n",
      "current batch loss: 0.667264  [275200/900000]\n",
      "current batch loss: 0.608412  [281600/900000]\n",
      "current batch loss: 0.591175  [288000/900000]\n",
      "current batch loss: 0.643997  [294400/900000]\n",
      "current batch loss: 0.620655  [300800/900000]\n",
      "current batch loss: 0.640639  [307200/900000]\n",
      "current batch loss: 0.602405  [313600/900000]\n",
      "current batch loss: 0.722311  [320000/900000]\n",
      "current batch loss: 0.594468  [326400/900000]\n",
      "current batch loss: 0.648977  [332800/900000]\n",
      "current batch loss: 0.640243  [339200/900000]\n",
      "current batch loss: 0.610679  [345600/900000]\n",
      "current batch loss: 0.690347  [352000/900000]\n",
      "current batch loss: 0.627955  [358400/900000]\n",
      "current batch loss: 0.660524  [364800/900000]\n",
      "current batch loss: 0.663718  [371200/900000]\n",
      "current batch loss: 0.597004  [377600/900000]\n",
      "current batch loss: 0.655077  [384000/900000]\n",
      "current batch loss: 0.675412  [390400/900000]\n",
      "current batch loss: 0.718828  [396800/900000]\n",
      "current batch loss: 0.625052  [403200/900000]\n",
      "current batch loss: 0.604566  [409600/900000]\n",
      "current batch loss: 0.637132  [416000/900000]\n",
      "current batch loss: 0.525307  [422400/900000]\n",
      "current batch loss: 0.675198  [428800/900000]\n",
      "current batch loss: 0.560855  [435200/900000]\n",
      "current batch loss: 0.633231  [441600/900000]\n",
      "current batch loss: 0.670723  [448000/900000]\n",
      "current batch loss: 0.589647  [454400/900000]\n",
      "current batch loss: 0.625280  [460800/900000]\n",
      "current batch loss: 0.668523  [467200/900000]\n",
      "current batch loss: 0.669791  [473600/900000]\n",
      "current batch loss: 0.601606  [480000/900000]\n",
      "current batch loss: 0.596319  [486400/900000]\n",
      "current batch loss: 0.661956  [492800/900000]\n",
      "current batch loss: 0.638763  [499200/900000]\n",
      "current batch loss: 0.648902  [505600/900000]\n",
      "current batch loss: 0.651920  [512000/900000]\n",
      "current batch loss: 0.643046  [518400/900000]\n",
      "current batch loss: 0.595305  [524800/900000]\n",
      "current batch loss: 0.632874  [531200/900000]\n",
      "current batch loss: 0.669843  [537600/900000]\n",
      "current batch loss: 0.602277  [544000/900000]\n",
      "current batch loss: 0.640165  [550400/900000]\n",
      "current batch loss: 0.671447  [556800/900000]\n",
      "current batch loss: 0.660053  [563200/900000]\n",
      "current batch loss: 0.537601  [569600/900000]\n",
      "current batch loss: 0.610965  [576000/900000]\n",
      "current batch loss: 0.687487  [582400/900000]\n",
      "current batch loss: 0.583634  [588800/900000]\n",
      "current batch loss: 0.656297  [595200/900000]\n",
      "current batch loss: 0.626131  [601600/900000]\n",
      "current batch loss: 0.676245  [608000/900000]\n",
      "current batch loss: 0.662450  [614400/900000]\n",
      "current batch loss: 0.666644  [620800/900000]\n",
      "current batch loss: 0.645684  [627200/900000]\n",
      "current batch loss: 0.564761  [633600/900000]\n",
      "current batch loss: 0.593899  [640000/900000]\n",
      "current batch loss: 0.658556  [646400/900000]\n",
      "current batch loss: 0.650518  [652800/900000]\n",
      "current batch loss: 0.657256  [659200/900000]\n",
      "current batch loss: 0.583749  [665600/900000]\n",
      "current batch loss: 0.679244  [672000/900000]\n",
      "current batch loss: 0.618163  [678400/900000]\n",
      "current batch loss: 0.597874  [684800/900000]\n",
      "current batch loss: 0.651532  [691200/900000]\n",
      "current batch loss: 0.691184  [697600/900000]\n",
      "current batch loss: 0.683030  [704000/900000]\n",
      "current batch loss: 0.606510  [710400/900000]\n",
      "current batch loss: 0.643932  [716800/900000]\n",
      "current batch loss: 0.678470  [723200/900000]\n",
      "current batch loss: 0.627141  [729600/900000]\n",
      "current batch loss: 0.602121  [736000/900000]\n",
      "current batch loss: 0.669587  [742400/900000]\n",
      "current batch loss: 0.577358  [748800/900000]\n",
      "current batch loss: 0.677149  [755200/900000]\n",
      "current batch loss: 0.667117  [761600/900000]\n",
      "current batch loss: 0.617023  [768000/900000]\n",
      "current batch loss: 0.613976  [774400/900000]\n",
      "current batch loss: 0.693125  [780800/900000]\n",
      "current batch loss: 0.643546  [787200/900000]\n",
      "current batch loss: 0.601665  [793600/900000]\n",
      "current batch loss: 0.582643  [800000/900000]\n",
      "current batch loss: 0.651638  [806400/900000]\n",
      "current batch loss: 0.653914  [812800/900000]\n",
      "current batch loss: 0.618630  [819200/900000]\n",
      "current batch loss: 0.585104  [825600/900000]\n",
      "current batch loss: 0.607598  [832000/900000]\n",
      "current batch loss: 0.615898  [838400/900000]\n",
      "current batch loss: 0.616895  [844800/900000]\n",
      "current batch loss: 0.634237  [851200/900000]\n",
      "current batch loss: 0.625088  [857600/900000]\n",
      "current batch loss: 0.681728  [864000/900000]\n",
      "current batch loss: 0.583451  [870400/900000]\n",
      "current batch loss: 0.668178  [876800/900000]\n",
      "current batch loss: 0.641000  [883200/900000]\n",
      "current batch loss: 0.656915  [889600/900000]\n",
      "current batch loss: 0.603052  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.639691\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.638781\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 7\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.692577  [    0/900000]\n",
      "current batch loss: 0.676876  [ 6400/900000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.660996  [12800/900000]\n",
      "current batch loss: 0.575531  [19200/900000]\n",
      "current batch loss: 0.674701  [25600/900000]\n",
      "current batch loss: 0.656545  [32000/900000]\n",
      "current batch loss: 0.590307  [38400/900000]\n",
      "current batch loss: 0.639006  [44800/900000]\n",
      "current batch loss: 0.638415  [51200/900000]\n",
      "current batch loss: 0.717508  [57600/900000]\n",
      "current batch loss: 0.612967  [64000/900000]\n",
      "current batch loss: 0.642837  [70400/900000]\n",
      "current batch loss: 0.612663  [76800/900000]\n",
      "current batch loss: 0.643891  [83200/900000]\n",
      "current batch loss: 0.586080  [89600/900000]\n",
      "current batch loss: 0.605178  [96000/900000]\n",
      "current batch loss: 0.707315  [102400/900000]\n",
      "current batch loss: 0.683352  [108800/900000]\n",
      "current batch loss: 0.691535  [115200/900000]\n",
      "current batch loss: 0.691078  [121600/900000]\n",
      "current batch loss: 0.591740  [128000/900000]\n",
      "current batch loss: 0.662643  [134400/900000]\n",
      "current batch loss: 0.611720  [140800/900000]\n",
      "current batch loss: 0.522285  [147200/900000]\n",
      "current batch loss: 0.589097  [153600/900000]\n",
      "current batch loss: 0.678164  [160000/900000]\n",
      "current batch loss: 0.620850  [166400/900000]\n",
      "current batch loss: 0.617846  [172800/900000]\n",
      "current batch loss: 0.689791  [179200/900000]\n",
      "current batch loss: 0.627557  [185600/900000]\n",
      "current batch loss: 0.680628  [192000/900000]\n",
      "current batch loss: 0.674001  [198400/900000]\n",
      "current batch loss: 0.632659  [204800/900000]\n",
      "current batch loss: 0.632226  [211200/900000]\n",
      "current batch loss: 0.619750  [217600/900000]\n",
      "current batch loss: 0.743521  [224000/900000]\n",
      "current batch loss: 0.621707  [230400/900000]\n",
      "current batch loss: 0.625440  [236800/900000]\n",
      "current batch loss: 0.689966  [243200/900000]\n",
      "current batch loss: 0.566510  [249600/900000]\n",
      "current batch loss: 0.651383  [256000/900000]\n",
      "current batch loss: 0.687253  [262400/900000]\n",
      "current batch loss: 0.719973  [268800/900000]\n",
      "current batch loss: 0.615046  [275200/900000]\n",
      "current batch loss: 0.587440  [281600/900000]\n",
      "current batch loss: 0.590840  [288000/900000]\n",
      "current batch loss: 0.612699  [294400/900000]\n",
      "current batch loss: 0.643841  [300800/900000]\n",
      "current batch loss: 0.627663  [307200/900000]\n",
      "current batch loss: 0.609081  [313600/900000]\n",
      "current batch loss: 0.631793  [320000/900000]\n",
      "current batch loss: 0.631530  [326400/900000]\n",
      "current batch loss: 0.638131  [332800/900000]\n",
      "current batch loss: 0.542563  [339200/900000]\n",
      "current batch loss: 0.616916  [345600/900000]\n",
      "current batch loss: 0.649354  [352000/900000]\n",
      "current batch loss: 0.655440  [358400/900000]\n",
      "current batch loss: 0.610461  [364800/900000]\n",
      "current batch loss: 0.620605  [371200/900000]\n",
      "current batch loss: 0.648447  [377600/900000]\n",
      "current batch loss: 0.647512  [384000/900000]\n",
      "current batch loss: 0.643557  [390400/900000]\n",
      "current batch loss: 0.661698  [396800/900000]\n",
      "current batch loss: 0.574074  [403200/900000]\n",
      "current batch loss: 0.577026  [409600/900000]\n",
      "current batch loss: 0.646188  [416000/900000]\n",
      "current batch loss: 0.634102  [422400/900000]\n",
      "current batch loss: 0.603203  [428800/900000]\n",
      "current batch loss: 0.661531  [435200/900000]\n",
      "current batch loss: 0.673244  [441600/900000]\n",
      "current batch loss: 0.687370  [448000/900000]\n",
      "current batch loss: 0.735400  [454400/900000]\n",
      "current batch loss: 0.577561  [460800/900000]\n",
      "current batch loss: 0.668149  [467200/900000]\n",
      "current batch loss: 0.684608  [473600/900000]\n",
      "current batch loss: 0.641387  [480000/900000]\n",
      "current batch loss: 0.640860  [486400/900000]\n",
      "current batch loss: 0.638890  [492800/900000]\n",
      "current batch loss: 0.640261  [499200/900000]\n",
      "current batch loss: 0.691416  [505600/900000]\n",
      "current batch loss: 0.602246  [512000/900000]\n",
      "current batch loss: 0.596450  [518400/900000]\n",
      "current batch loss: 0.696239  [524800/900000]\n",
      "current batch loss: 0.656843  [531200/900000]\n",
      "current batch loss: 0.619423  [537600/900000]\n",
      "current batch loss: 0.586311  [544000/900000]\n",
      "current batch loss: 0.603225  [550400/900000]\n",
      "current batch loss: 0.644838  [556800/900000]\n",
      "current batch loss: 0.632634  [563200/900000]\n",
      "current batch loss: 0.588860  [569600/900000]\n",
      "current batch loss: 0.579227  [576000/900000]\n",
      "current batch loss: 0.623143  [582400/900000]\n",
      "current batch loss: 0.628613  [588800/900000]\n",
      "current batch loss: 0.730487  [595200/900000]\n",
      "current batch loss: 0.658136  [601600/900000]\n",
      "current batch loss: 0.660648  [608000/900000]\n",
      "current batch loss: 0.680140  [614400/900000]\n",
      "current batch loss: 0.691167  [620800/900000]\n",
      "current batch loss: 0.631669  [627200/900000]\n",
      "current batch loss: 0.596627  [633600/900000]\n",
      "current batch loss: 0.676375  [640000/900000]\n",
      "current batch loss: 0.626737  [646400/900000]\n",
      "current batch loss: 0.680278  [652800/900000]\n",
      "current batch loss: 0.635792  [659200/900000]\n",
      "current batch loss: 0.597921  [665600/900000]\n",
      "current batch loss: 0.677956  [672000/900000]\n",
      "current batch loss: 0.654177  [678400/900000]\n",
      "current batch loss: 0.660476  [684800/900000]\n",
      "current batch loss: 0.622549  [691200/900000]\n",
      "current batch loss: 0.574428  [697600/900000]\n",
      "current batch loss: 0.629202  [704000/900000]\n",
      "current batch loss: 0.655129  [710400/900000]\n",
      "current batch loss: 0.658918  [716800/900000]\n",
      "current batch loss: 0.617233  [723200/900000]\n",
      "current batch loss: 0.709217  [729600/900000]\n",
      "current batch loss: 0.639513  [736000/900000]\n",
      "current batch loss: 0.622667  [742400/900000]\n",
      "current batch loss: 0.601067  [748800/900000]\n",
      "current batch loss: 0.656816  [755200/900000]\n",
      "current batch loss: 0.628431  [761600/900000]\n",
      "current batch loss: 0.651060  [768000/900000]\n",
      "current batch loss: 0.596919  [774400/900000]\n",
      "current batch loss: 0.599524  [780800/900000]\n",
      "current batch loss: 0.628078  [787200/900000]\n",
      "current batch loss: 0.700844  [793600/900000]\n",
      "current batch loss: 0.655879  [800000/900000]\n",
      "current batch loss: 0.708506  [806400/900000]\n",
      "current batch loss: 0.649611  [812800/900000]\n",
      "current batch loss: 0.615276  [819200/900000]\n",
      "current batch loss: 0.619483  [825600/900000]\n",
      "current batch loss: 0.609017  [832000/900000]\n",
      "current batch loss: 0.646499  [838400/900000]\n",
      "current batch loss: 0.656788  [844800/900000]\n",
      "current batch loss: 0.655830  [851200/900000]\n",
      "current batch loss: 0.613424  [857600/900000]\n",
      "current batch loss: 0.601706  [864000/900000]\n",
      "current batch loss: 0.624677  [870400/900000]\n",
      "current batch loss: 0.664979  [876800/900000]\n",
      "current batch loss: 0.618510  [883200/900000]\n",
      "current batch loss: 0.616286  [889600/900000]\n",
      "current batch loss: 0.659059  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.638028\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.636904\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 8\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.653967  [    0/900000]\n",
      "current batch loss: 0.696531  [ 6400/900000]\n",
      "current batch loss: 0.614863  [12800/900000]\n",
      "current batch loss: 0.654252  [19200/900000]\n",
      "current batch loss: 0.637647  [25600/900000]\n",
      "current batch loss: 0.633575  [32000/900000]\n",
      "current batch loss: 0.695593  [38400/900000]\n",
      "current batch loss: 0.600947  [44800/900000]\n",
      "current batch loss: 0.635818  [51200/900000]\n",
      "current batch loss: 0.603055  [57600/900000]\n",
      "current batch loss: 0.625013  [64000/900000]\n",
      "current batch loss: 0.552432  [70400/900000]\n",
      "current batch loss: 0.624036  [76800/900000]\n",
      "current batch loss: 0.659073  [83200/900000]\n",
      "current batch loss: 0.710734  [89600/900000]\n",
      "current batch loss: 0.645687  [96000/900000]\n",
      "current batch loss: 0.620018  [102400/900000]\n",
      "current batch loss: 0.664896  [108800/900000]\n",
      "current batch loss: 0.622105  [115200/900000]\n",
      "current batch loss: 0.597626  [121600/900000]\n",
      "current batch loss: 0.661005  [128000/900000]\n",
      "current batch loss: 0.609145  [134400/900000]\n",
      "current batch loss: 0.587028  [140800/900000]\n",
      "current batch loss: 0.584432  [147200/900000]\n",
      "current batch loss: 0.729705  [153600/900000]\n",
      "current batch loss: 0.578142  [160000/900000]\n",
      "current batch loss: 0.641387  [166400/900000]\n",
      "current batch loss: 0.580736  [172800/900000]\n",
      "current batch loss: 0.641724  [179200/900000]\n",
      "current batch loss: 0.681581  [185600/900000]\n",
      "current batch loss: 0.675519  [192000/900000]\n",
      "current batch loss: 0.577300  [198400/900000]\n",
      "current batch loss: 0.587718  [204800/900000]\n",
      "current batch loss: 0.577434  [211200/900000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.678436  [217600/900000]\n",
      "current batch loss: 0.599987  [224000/900000]\n",
      "current batch loss: 0.640161  [230400/900000]\n",
      "current batch loss: 0.554594  [236800/900000]\n",
      "current batch loss: 0.639832  [243200/900000]\n",
      "current batch loss: 0.595429  [249600/900000]\n",
      "current batch loss: 0.638633  [256000/900000]\n",
      "current batch loss: 0.588976  [262400/900000]\n",
      "current batch loss: 0.660335  [268800/900000]\n",
      "current batch loss: 0.606487  [275200/900000]\n",
      "current batch loss: 0.669531  [281600/900000]\n",
      "current batch loss: 0.594440  [288000/900000]\n",
      "current batch loss: 0.646929  [294400/900000]\n",
      "current batch loss: 0.643189  [300800/900000]\n",
      "current batch loss: 0.625475  [307200/900000]\n",
      "current batch loss: 0.631208  [313600/900000]\n",
      "current batch loss: 0.666481  [320000/900000]\n",
      "current batch loss: 0.667737  [326400/900000]\n",
      "current batch loss: 0.617795  [332800/900000]\n",
      "current batch loss: 0.673369  [339200/900000]\n",
      "current batch loss: 0.689788  [345600/900000]\n",
      "current batch loss: 0.647821  [352000/900000]\n",
      "current batch loss: 0.606310  [358400/900000]\n",
      "current batch loss: 0.652865  [364800/900000]\n",
      "current batch loss: 0.656749  [371200/900000]\n",
      "current batch loss: 0.584400  [377600/900000]\n",
      "current batch loss: 0.653107  [384000/900000]\n",
      "current batch loss: 0.640406  [390400/900000]\n",
      "current batch loss: 0.601702  [396800/900000]\n",
      "current batch loss: 0.624730  [403200/900000]\n",
      "current batch loss: 0.648202  [409600/900000]\n",
      "current batch loss: 0.585781  [416000/900000]\n",
      "current batch loss: 0.636244  [422400/900000]\n",
      "current batch loss: 0.595989  [428800/900000]\n",
      "current batch loss: 0.656472  [435200/900000]\n",
      "current batch loss: 0.667438  [441600/900000]\n",
      "current batch loss: 0.654998  [448000/900000]\n",
      "current batch loss: 0.611545  [454400/900000]\n",
      "current batch loss: 0.667756  [460800/900000]\n",
      "current batch loss: 0.650110  [467200/900000]\n",
      "current batch loss: 0.633474  [473600/900000]\n",
      "current batch loss: 0.669436  [480000/900000]\n",
      "current batch loss: 0.644483  [486400/900000]\n",
      "current batch loss: 0.660081  [492800/900000]\n",
      "current batch loss: 0.619204  [499200/900000]\n",
      "current batch loss: 0.648341  [505600/900000]\n",
      "current batch loss: 0.563334  [512000/900000]\n",
      "current batch loss: 0.639102  [518400/900000]\n",
      "current batch loss: 0.649396  [524800/900000]\n",
      "current batch loss: 0.585250  [531200/900000]\n",
      "current batch loss: 0.628202  [537600/900000]\n",
      "current batch loss: 0.650056  [544000/900000]\n",
      "current batch loss: 0.664966  [550400/900000]\n",
      "current batch loss: 0.679066  [556800/900000]\n",
      "current batch loss: 0.655927  [563200/900000]\n",
      "current batch loss: 0.643382  [569600/900000]\n",
      "current batch loss: 0.692097  [576000/900000]\n",
      "current batch loss: 0.625519  [582400/900000]\n",
      "current batch loss: 0.641469  [588800/900000]\n",
      "current batch loss: 0.613370  [595200/900000]\n",
      "current batch loss: 0.689424  [601600/900000]\n",
      "current batch loss: 0.700542  [608000/900000]\n",
      "current batch loss: 0.550086  [614400/900000]\n",
      "current batch loss: 0.678342  [620800/900000]\n",
      "current batch loss: 0.609361  [627200/900000]\n",
      "current batch loss: 0.633664  [633600/900000]\n",
      "current batch loss: 0.636051  [640000/900000]\n",
      "current batch loss: 0.691275  [646400/900000]\n",
      "current batch loss: 0.619734  [652800/900000]\n",
      "current batch loss: 0.609700  [659200/900000]\n",
      "current batch loss: 0.688147  [665600/900000]\n",
      "current batch loss: 0.619406  [672000/900000]\n",
      "current batch loss: 0.647717  [678400/900000]\n",
      "current batch loss: 0.669883  [684800/900000]\n",
      "current batch loss: 0.637857  [691200/900000]\n",
      "current batch loss: 0.723639  [697600/900000]\n",
      "current batch loss: 0.653436  [704000/900000]\n",
      "current batch loss: 0.649991  [710400/900000]\n",
      "current batch loss: 0.664683  [716800/900000]\n",
      "current batch loss: 0.691296  [723200/900000]\n",
      "current batch loss: 0.653926  [729600/900000]\n",
      "current batch loss: 0.754497  [736000/900000]\n",
      "current batch loss: 0.664755  [742400/900000]\n",
      "current batch loss: 0.597615  [748800/900000]\n",
      "current batch loss: 0.621606  [755200/900000]\n",
      "current batch loss: 0.649799  [761600/900000]\n",
      "current batch loss: 0.622186  [768000/900000]\n",
      "current batch loss: 0.564467  [774400/900000]\n",
      "current batch loss: 0.647329  [780800/900000]\n",
      "current batch loss: 0.646916  [787200/900000]\n",
      "current batch loss: 0.635213  [793600/900000]\n",
      "current batch loss: 0.551949  [800000/900000]\n",
      "current batch loss: 0.612523  [806400/900000]\n",
      "current batch loss: 0.721130  [812800/900000]\n",
      "current batch loss: 0.712558  [819200/900000]\n",
      "current batch loss: 0.666008  [825600/900000]\n",
      "current batch loss: 0.700949  [832000/900000]\n",
      "current batch loss: 0.605156  [838400/900000]\n",
      "current batch loss: 0.714950  [844800/900000]\n",
      "current batch loss: 0.662663  [851200/900000]\n",
      "current batch loss: 0.690518  [857600/900000]\n",
      "current batch loss: 0.628152  [864000/900000]\n",
      "current batch loss: 0.599243  [870400/900000]\n",
      "current batch loss: 0.707420  [876800/900000]\n",
      "current batch loss: 0.629445  [883200/900000]\n",
      "current batch loss: 0.597678  [889600/900000]\n",
      "current batch loss: 0.609744  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.637506\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.636555\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 9\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.682327  [    0/900000]\n",
      "current batch loss: 0.657213  [ 6400/900000]\n",
      "current batch loss: 0.613204  [12800/900000]\n",
      "current batch loss: 0.717380  [19200/900000]\n",
      "current batch loss: 0.577563  [25600/900000]\n",
      "current batch loss: 0.647852  [32000/900000]\n",
      "current batch loss: 0.549022  [38400/900000]\n",
      "current batch loss: 0.637124  [44800/900000]\n",
      "current batch loss: 0.643421  [51200/900000]\n",
      "current batch loss: 0.635129  [57600/900000]\n",
      "current batch loss: 0.637870  [64000/900000]\n",
      "current batch loss: 0.654548  [70400/900000]\n",
      "current batch loss: 0.585971  [76800/900000]\n",
      "current batch loss: 0.546957  [83200/900000]\n",
      "current batch loss: 0.640352  [89600/900000]\n",
      "current batch loss: 0.614880  [96000/900000]\n",
      "current batch loss: 0.634697  [102400/900000]\n",
      "current batch loss: 0.590398  [108800/900000]\n",
      "current batch loss: 0.648357  [115200/900000]\n",
      "current batch loss: 0.627319  [121600/900000]\n",
      "current batch loss: 0.631521  [128000/900000]\n",
      "current batch loss: 0.602236  [134400/900000]\n",
      "current batch loss: 0.705621  [140800/900000]\n",
      "current batch loss: 0.624844  [147200/900000]\n",
      "current batch loss: 0.589136  [153600/900000]\n",
      "current batch loss: 0.577522  [160000/900000]\n",
      "current batch loss: 0.690184  [166400/900000]\n",
      "current batch loss: 0.696239  [172800/900000]\n",
      "current batch loss: 0.619733  [179200/900000]\n",
      "current batch loss: 0.599122  [185600/900000]\n",
      "current batch loss: 0.552753  [192000/900000]\n",
      "current batch loss: 0.600248  [198400/900000]\n",
      "current batch loss: 0.685932  [204800/900000]\n",
      "current batch loss: 0.649231  [211200/900000]\n",
      "current batch loss: 0.613994  [217600/900000]\n",
      "current batch loss: 0.627761  [224000/900000]\n",
      "current batch loss: 0.665244  [230400/900000]\n",
      "current batch loss: 0.686573  [236800/900000]\n",
      "current batch loss: 0.602079  [243200/900000]\n",
      "current batch loss: 0.598321  [249600/900000]\n",
      "current batch loss: 0.698576  [256000/900000]\n",
      "current batch loss: 0.598518  [262400/900000]\n",
      "current batch loss: 0.606852  [268800/900000]\n",
      "current batch loss: 0.599795  [275200/900000]\n",
      "current batch loss: 0.655846  [281600/900000]\n",
      "current batch loss: 0.634402  [288000/900000]\n",
      "current batch loss: 0.696042  [294400/900000]\n",
      "current batch loss: 0.674919  [300800/900000]\n",
      "current batch loss: 0.583827  [307200/900000]\n",
      "current batch loss: 0.724002  [313600/900000]\n",
      "current batch loss: 0.553522  [320000/900000]\n",
      "current batch loss: 0.572761  [326400/900000]\n",
      "current batch loss: 0.607302  [332800/900000]\n",
      "current batch loss: 0.655299  [339200/900000]\n",
      "current batch loss: 0.668081  [345600/900000]\n",
      "current batch loss: 0.591329  [352000/900000]\n",
      "current batch loss: 0.621608  [358400/900000]\n",
      "current batch loss: 0.651296  [364800/900000]\n",
      "current batch loss: 0.681147  [371200/900000]\n",
      "current batch loss: 0.594217  [377600/900000]\n",
      "current batch loss: 0.541625  [384000/900000]\n",
      "current batch loss: 0.656973  [390400/900000]\n",
      "current batch loss: 0.629465  [396800/900000]\n",
      "current batch loss: 0.681262  [403200/900000]\n",
      "current batch loss: 0.640142  [409600/900000]\n",
      "current batch loss: 0.631965  [416000/900000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.623369  [422400/900000]\n",
      "current batch loss: 0.637272  [428800/900000]\n",
      "current batch loss: 0.634129  [435200/900000]\n",
      "current batch loss: 0.592910  [441600/900000]\n",
      "current batch loss: 0.604203  [448000/900000]\n",
      "current batch loss: 0.669661  [454400/900000]\n",
      "current batch loss: 0.609826  [460800/900000]\n",
      "current batch loss: 0.630709  [467200/900000]\n",
      "current batch loss: 0.674543  [473600/900000]\n",
      "current batch loss: 0.567078  [480000/900000]\n",
      "current batch loss: 0.629997  [486400/900000]\n",
      "current batch loss: 0.677122  [492800/900000]\n",
      "current batch loss: 0.652965  [499200/900000]\n",
      "current batch loss: 0.646131  [505600/900000]\n",
      "current batch loss: 0.656085  [512000/900000]\n",
      "current batch loss: 0.624971  [518400/900000]\n",
      "current batch loss: 0.631743  [524800/900000]\n",
      "current batch loss: 0.559374  [531200/900000]\n",
      "current batch loss: 0.638873  [537600/900000]\n",
      "current batch loss: 0.643829  [544000/900000]\n",
      "current batch loss: 0.617317  [550400/900000]\n",
      "current batch loss: 0.662598  [556800/900000]\n",
      "current batch loss: 0.662481  [563200/900000]\n",
      "current batch loss: 0.657185  [569600/900000]\n",
      "current batch loss: 0.601089  [576000/900000]\n",
      "current batch loss: 0.602180  [582400/900000]\n",
      "current batch loss: 0.641171  [588800/900000]\n",
      "current batch loss: 0.670575  [595200/900000]\n",
      "current batch loss: 0.685964  [601600/900000]\n",
      "current batch loss: 0.684305  [608000/900000]\n",
      "current batch loss: 0.624180  [614400/900000]\n",
      "current batch loss: 0.706791  [620800/900000]\n",
      "current batch loss: 0.706102  [627200/900000]\n",
      "current batch loss: 0.625093  [633600/900000]\n",
      "current batch loss: 0.674616  [640000/900000]\n",
      "current batch loss: 0.646364  [646400/900000]\n",
      "current batch loss: 0.610241  [652800/900000]\n",
      "current batch loss: 0.587899  [659200/900000]\n",
      "current batch loss: 0.619202  [665600/900000]\n",
      "current batch loss: 0.660485  [672000/900000]\n",
      "current batch loss: 0.676735  [678400/900000]\n",
      "current batch loss: 0.620285  [684800/900000]\n",
      "current batch loss: 0.652893  [691200/900000]\n",
      "current batch loss: 0.640622  [697600/900000]\n",
      "current batch loss: 0.716870  [704000/900000]\n",
      "current batch loss: 0.576737  [710400/900000]\n",
      "current batch loss: 0.631346  [716800/900000]\n",
      "current batch loss: 0.613740  [723200/900000]\n",
      "current batch loss: 0.609304  [729600/900000]\n",
      "current batch loss: 0.630091  [736000/900000]\n",
      "current batch loss: 0.629184  [742400/900000]\n",
      "current batch loss: 0.623612  [748800/900000]\n",
      "current batch loss: 0.679746  [755200/900000]\n",
      "current batch loss: 0.672784  [761600/900000]\n",
      "current batch loss: 0.566580  [768000/900000]\n",
      "current batch loss: 0.632133  [774400/900000]\n",
      "current batch loss: 0.606223  [780800/900000]\n",
      "current batch loss: 0.609376  [787200/900000]\n",
      "current batch loss: 0.591539  [793600/900000]\n",
      "current batch loss: 0.619084  [800000/900000]\n",
      "current batch loss: 0.595382  [806400/900000]\n",
      "current batch loss: 0.613597  [812800/900000]\n",
      "current batch loss: 0.602544  [819200/900000]\n",
      "current batch loss: 0.620640  [825600/900000]\n",
      "current batch loss: 0.604980  [832000/900000]\n",
      "current batch loss: 0.642656  [838400/900000]\n",
      "current batch loss: 0.626099  [844800/900000]\n",
      "current batch loss: 0.599643  [851200/900000]\n",
      "current batch loss: 0.686483  [857600/900000]\n",
      "current batch loss: 0.661521  [864000/900000]\n",
      "current batch loss: 0.585872  [870400/900000]\n",
      "current batch loss: 0.674512  [876800/900000]\n",
      "current batch loss: 0.711615  [883200/900000]\n",
      "current batch loss: 0.628607  [889600/900000]\n",
      "current batch loss: 0.587350  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.637172\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.635790\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 10\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.636867  [    0/900000]\n",
      "current batch loss: 0.646656  [ 6400/900000]\n",
      "current batch loss: 0.577716  [12800/900000]\n",
      "current batch loss: 0.671518  [19200/900000]\n",
      "current batch loss: 0.569727  [25600/900000]\n",
      "current batch loss: 0.607674  [32000/900000]\n",
      "current batch loss: 0.580907  [38400/900000]\n",
      "current batch loss: 0.673342  [44800/900000]\n",
      "current batch loss: 0.601166  [51200/900000]\n",
      "current batch loss: 0.705208  [57600/900000]\n",
      "current batch loss: 0.599264  [64000/900000]\n",
      "current batch loss: 0.694302  [70400/900000]\n",
      "current batch loss: 0.611284  [76800/900000]\n",
      "current batch loss: 0.646749  [83200/900000]\n",
      "current batch loss: 0.637635  [89600/900000]\n",
      "current batch loss: 0.639678  [96000/900000]\n",
      "current batch loss: 0.619310  [102400/900000]\n",
      "current batch loss: 0.638925  [108800/900000]\n",
      "current batch loss: 0.602641  [115200/900000]\n",
      "current batch loss: 0.688777  [121600/900000]\n",
      "current batch loss: 0.630261  [128000/900000]\n",
      "current batch loss: 0.649432  [134400/900000]\n",
      "current batch loss: 0.650014  [140800/900000]\n",
      "current batch loss: 0.666211  [147200/900000]\n",
      "current batch loss: 0.657492  [153600/900000]\n",
      "current batch loss: 0.646885  [160000/900000]\n",
      "current batch loss: 0.605772  [166400/900000]\n",
      "current batch loss: 0.647629  [172800/900000]\n",
      "current batch loss: 0.537800  [179200/900000]\n",
      "current batch loss: 0.600382  [185600/900000]\n",
      "current batch loss: 0.656069  [192000/900000]\n",
      "current batch loss: 0.614432  [198400/900000]\n",
      "current batch loss: 0.617419  [204800/900000]\n",
      "current batch loss: 0.694818  [211200/900000]\n",
      "current batch loss: 0.583005  [217600/900000]\n",
      "current batch loss: 0.634751  [224000/900000]\n",
      "current batch loss: 0.667626  [230400/900000]\n",
      "current batch loss: 0.670808  [236800/900000]\n",
      "current batch loss: 0.670826  [243200/900000]\n",
      "current batch loss: 0.609521  [249600/900000]\n",
      "current batch loss: 0.592244  [256000/900000]\n",
      "current batch loss: 0.657256  [262400/900000]\n",
      "current batch loss: 0.573893  [268800/900000]\n",
      "current batch loss: 0.639073  [275200/900000]\n",
      "current batch loss: 0.676121  [281600/900000]\n",
      "current batch loss: 0.518632  [288000/900000]\n",
      "current batch loss: 0.621139  [294400/900000]\n",
      "current batch loss: 0.692073  [300800/900000]\n",
      "current batch loss: 0.606770  [307200/900000]\n",
      "current batch loss: 0.657535  [313600/900000]\n",
      "current batch loss: 0.579949  [320000/900000]\n",
      "current batch loss: 0.580428  [326400/900000]\n",
      "current batch loss: 0.630768  [332800/900000]\n",
      "current batch loss: 0.660361  [339200/900000]\n",
      "current batch loss: 0.650909  [345600/900000]\n",
      "current batch loss: 0.640198  [352000/900000]\n",
      "current batch loss: 0.683591  [358400/900000]\n",
      "current batch loss: 0.665453  [364800/900000]\n",
      "current batch loss: 0.602540  [371200/900000]\n",
      "current batch loss: 0.596580  [377600/900000]\n",
      "current batch loss: 0.688788  [384000/900000]\n",
      "current batch loss: 0.577681  [390400/900000]\n",
      "current batch loss: 0.609132  [396800/900000]\n",
      "current batch loss: 0.582177  [403200/900000]\n",
      "current batch loss: 0.645302  [409600/900000]\n",
      "current batch loss: 0.677123  [416000/900000]\n",
      "current batch loss: 0.690840  [422400/900000]\n",
      "current batch loss: 0.695012  [428800/900000]\n",
      "current batch loss: 0.582512  [435200/900000]\n",
      "current batch loss: 0.714875  [441600/900000]\n",
      "current batch loss: 0.597328  [448000/900000]\n",
      "current batch loss: 0.660991  [454400/900000]\n",
      "current batch loss: 0.678233  [460800/900000]\n",
      "current batch loss: 0.610917  [467200/900000]\n",
      "current batch loss: 0.682310  [473600/900000]\n",
      "current batch loss: 0.647481  [480000/900000]\n",
      "current batch loss: 0.693094  [486400/900000]\n",
      "current batch loss: 0.650213  [492800/900000]\n",
      "current batch loss: 0.630043  [499200/900000]\n",
      "current batch loss: 0.655072  [505600/900000]\n",
      "current batch loss: 0.587161  [512000/900000]\n",
      "current batch loss: 0.623343  [518400/900000]\n",
      "current batch loss: 0.594767  [524800/900000]\n",
      "current batch loss: 0.692910  [531200/900000]\n",
      "current batch loss: 0.662137  [537600/900000]\n",
      "current batch loss: 0.609269  [544000/900000]\n",
      "current batch loss: 0.647849  [550400/900000]\n",
      "current batch loss: 0.654246  [556800/900000]\n",
      "current batch loss: 0.665610  [563200/900000]\n",
      "current batch loss: 0.659912  [569600/900000]\n",
      "current batch loss: 0.578236  [576000/900000]\n",
      "current batch loss: 0.585810  [582400/900000]\n",
      "current batch loss: 0.592704  [588800/900000]\n",
      "current batch loss: 0.622887  [595200/900000]\n",
      "current batch loss: 0.627895  [601600/900000]\n",
      "current batch loss: 0.607881  [608000/900000]\n",
      "current batch loss: 0.686256  [614400/900000]\n",
      "current batch loss: 0.621224  [620800/900000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.639492  [627200/900000]\n",
      "current batch loss: 0.656098  [633600/900000]\n",
      "current batch loss: 0.612811  [640000/900000]\n",
      "current batch loss: 0.706851  [646400/900000]\n",
      "current batch loss: 0.637966  [652800/900000]\n",
      "current batch loss: 0.654135  [659200/900000]\n",
      "current batch loss: 0.612906  [665600/900000]\n",
      "current batch loss: 0.645845  [672000/900000]\n",
      "current batch loss: 0.605117  [678400/900000]\n",
      "current batch loss: 0.604239  [684800/900000]\n",
      "current batch loss: 0.596901  [691200/900000]\n",
      "current batch loss: 0.683852  [697600/900000]\n",
      "current batch loss: 0.696082  [704000/900000]\n",
      "current batch loss: 0.708759  [710400/900000]\n",
      "current batch loss: 0.664226  [716800/900000]\n",
      "current batch loss: 0.689795  [723200/900000]\n",
      "current batch loss: 0.623219  [729600/900000]\n",
      "current batch loss: 0.618996  [736000/900000]\n",
      "current batch loss: 0.598762  [742400/900000]\n",
      "current batch loss: 0.626974  [748800/900000]\n",
      "current batch loss: 0.655111  [755200/900000]\n",
      "current batch loss: 0.626530  [761600/900000]\n",
      "current batch loss: 0.597959  [768000/900000]\n",
      "current batch loss: 0.656243  [774400/900000]\n",
      "current batch loss: 0.624966  [780800/900000]\n",
      "current batch loss: 0.634277  [787200/900000]\n",
      "current batch loss: 0.629886  [793600/900000]\n",
      "current batch loss: 0.645424  [800000/900000]\n",
      "current batch loss: 0.619394  [806400/900000]\n",
      "current batch loss: 0.676648  [812800/900000]\n",
      "current batch loss: 0.677978  [819200/900000]\n",
      "current batch loss: 0.643965  [825600/900000]\n",
      "current batch loss: 0.661426  [832000/900000]\n",
      "current batch loss: 0.620034  [838400/900000]\n",
      "current batch loss: 0.610895  [844800/900000]\n",
      "current batch loss: 0.631595  [851200/900000]\n",
      "current batch loss: 0.658174  [857600/900000]\n",
      "current batch loss: 0.650521  [864000/900000]\n",
      "current batch loss: 0.644860  [870400/900000]\n",
      "current batch loss: 0.631176  [876800/900000]\n",
      "current batch loss: 0.677131  [883200/900000]\n",
      "current batch loss: 0.640388  [889600/900000]\n",
      "current batch loss: 0.633161  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.635333\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.634575\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 11\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.640416  [    0/900000]\n",
      "current batch loss: 0.636326  [ 6400/900000]\n",
      "current batch loss: 0.638048  [12800/900000]\n",
      "current batch loss: 0.647224  [19200/900000]\n",
      "current batch loss: 0.707414  [25600/900000]\n",
      "current batch loss: 0.599728  [32000/900000]\n",
      "current batch loss: 0.706795  [38400/900000]\n",
      "current batch loss: 0.587217  [44800/900000]\n",
      "current batch loss: 0.708655  [51200/900000]\n",
      "current batch loss: 0.599587  [57600/900000]\n",
      "current batch loss: 0.641979  [64000/900000]\n",
      "current batch loss: 0.633212  [70400/900000]\n",
      "current batch loss: 0.608188  [76800/900000]\n",
      "current batch loss: 0.679507  [83200/900000]\n",
      "current batch loss: 0.629023  [89600/900000]\n",
      "current batch loss: 0.566869  [96000/900000]\n",
      "current batch loss: 0.639352  [102400/900000]\n",
      "current batch loss: 0.610611  [108800/900000]\n",
      "current batch loss: 0.626984  [115200/900000]\n",
      "current batch loss: 0.604950  [121600/900000]\n",
      "current batch loss: 0.662717  [128000/900000]\n",
      "current batch loss: 0.605928  [134400/900000]\n",
      "current batch loss: 0.561211  [140800/900000]\n",
      "current batch loss: 0.653491  [147200/900000]\n",
      "current batch loss: 0.608216  [153600/900000]\n",
      "current batch loss: 0.604999  [160000/900000]\n",
      "current batch loss: 0.609855  [166400/900000]\n",
      "current batch loss: 0.640883  [172800/900000]\n",
      "current batch loss: 0.650616  [179200/900000]\n",
      "current batch loss: 0.628103  [185600/900000]\n",
      "current batch loss: 0.600400  [192000/900000]\n",
      "current batch loss: 0.642665  [198400/900000]\n",
      "current batch loss: 0.680428  [204800/900000]\n",
      "current batch loss: 0.655148  [211200/900000]\n",
      "current batch loss: 0.640889  [217600/900000]\n",
      "current batch loss: 0.634486  [224000/900000]\n",
      "current batch loss: 0.594594  [230400/900000]\n",
      "current batch loss: 0.622073  [236800/900000]\n",
      "current batch loss: 0.641803  [243200/900000]\n",
      "current batch loss: 0.636454  [249600/900000]\n",
      "current batch loss: 0.739148  [256000/900000]\n",
      "current batch loss: 0.658379  [262400/900000]\n",
      "current batch loss: 0.632355  [268800/900000]\n",
      "current batch loss: 0.622139  [275200/900000]\n",
      "current batch loss: 0.579676  [281600/900000]\n",
      "current batch loss: 0.645002  [288000/900000]\n",
      "current batch loss: 0.626865  [294400/900000]\n",
      "current batch loss: 0.620830  [300800/900000]\n",
      "current batch loss: 0.644919  [307200/900000]\n",
      "current batch loss: 0.616961  [313600/900000]\n",
      "current batch loss: 0.652073  [320000/900000]\n",
      "current batch loss: 0.665150  [326400/900000]\n",
      "current batch loss: 0.672929  [332800/900000]\n",
      "current batch loss: 0.703785  [339200/900000]\n",
      "current batch loss: 0.648835  [345600/900000]\n",
      "current batch loss: 0.654885  [352000/900000]\n",
      "current batch loss: 0.647620  [358400/900000]\n",
      "current batch loss: 0.593693  [364800/900000]\n",
      "current batch loss: 0.624714  [371200/900000]\n",
      "current batch loss: 0.614513  [377600/900000]\n",
      "current batch loss: 0.565024  [384000/900000]\n",
      "current batch loss: 0.694567  [390400/900000]\n",
      "current batch loss: 0.610292  [396800/900000]\n",
      "current batch loss: 0.658549  [403200/900000]\n",
      "current batch loss: 0.704095  [409600/900000]\n",
      "current batch loss: 0.637672  [416000/900000]\n",
      "current batch loss: 0.597662  [422400/900000]\n",
      "current batch loss: 0.649318  [428800/900000]\n",
      "current batch loss: 0.602920  [435200/900000]\n",
      "current batch loss: 0.560353  [441600/900000]\n",
      "current batch loss: 0.674034  [448000/900000]\n",
      "current batch loss: 0.642788  [454400/900000]\n",
      "current batch loss: 0.702156  [460800/900000]\n",
      "current batch loss: 0.671554  [467200/900000]\n",
      "current batch loss: 0.629756  [473600/900000]\n",
      "current batch loss: 0.652956  [480000/900000]\n",
      "current batch loss: 0.715602  [486400/900000]\n",
      "current batch loss: 0.598748  [492800/900000]\n",
      "current batch loss: 0.663194  [499200/900000]\n",
      "current batch loss: 0.605512  [505600/900000]\n",
      "current batch loss: 0.641156  [512000/900000]\n",
      "current batch loss: 0.562533  [518400/900000]\n",
      "current batch loss: 0.661032  [524800/900000]\n",
      "current batch loss: 0.656481  [531200/900000]\n",
      "current batch loss: 0.624717  [537600/900000]\n",
      "current batch loss: 0.638913  [544000/900000]\n",
      "current batch loss: 0.666069  [550400/900000]\n",
      "current batch loss: 0.664779  [556800/900000]\n",
      "current batch loss: 0.583718  [563200/900000]\n",
      "current batch loss: 0.599931  [569600/900000]\n",
      "current batch loss: 0.654193  [576000/900000]\n",
      "current batch loss: 0.608492  [582400/900000]\n",
      "current batch loss: 0.607133  [588800/900000]\n",
      "current batch loss: 0.593475  [595200/900000]\n",
      "current batch loss: 0.649137  [601600/900000]\n",
      "current batch loss: 0.567934  [608000/900000]\n",
      "current batch loss: 0.621120  [614400/900000]\n",
      "current batch loss: 0.676727  [620800/900000]\n",
      "current batch loss: 0.648169  [627200/900000]\n",
      "current batch loss: 0.620563  [633600/900000]\n",
      "current batch loss: 0.617596  [640000/900000]\n",
      "current batch loss: 0.574418  [646400/900000]\n",
      "current batch loss: 0.645318  [652800/900000]\n",
      "current batch loss: 0.634264  [659200/900000]\n",
      "current batch loss: 0.587396  [665600/900000]\n",
      "current batch loss: 0.585340  [672000/900000]\n",
      "current batch loss: 0.650589  [678400/900000]\n",
      "current batch loss: 0.643923  [684800/900000]\n",
      "current batch loss: 0.583968  [691200/900000]\n",
      "current batch loss: 0.588357  [697600/900000]\n",
      "current batch loss: 0.681492  [704000/900000]\n",
      "current batch loss: 0.643948  [710400/900000]\n",
      "current batch loss: 0.683362  [716800/900000]\n",
      "current batch loss: 0.603383  [723200/900000]\n",
      "current batch loss: 0.651769  [729600/900000]\n",
      "current batch loss: 0.633943  [736000/900000]\n",
      "current batch loss: 0.612912  [742400/900000]\n",
      "current batch loss: 0.625870  [748800/900000]\n",
      "current batch loss: 0.658319  [755200/900000]\n",
      "current batch loss: 0.635588  [761600/900000]\n",
      "current batch loss: 0.636145  [768000/900000]\n",
      "current batch loss: 0.656496  [774400/900000]\n",
      "current batch loss: 0.632884  [780800/900000]\n",
      "current batch loss: 0.625299  [787200/900000]\n",
      "current batch loss: 0.607165  [793600/900000]\n",
      "current batch loss: 0.681639  [800000/900000]\n",
      "current batch loss: 0.628885  [806400/900000]\n",
      "current batch loss: 0.637549  [812800/900000]\n",
      "current batch loss: 0.671304  [819200/900000]\n",
      "current batch loss: 0.597318  [825600/900000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.645986  [832000/900000]\n",
      "current batch loss: 0.618946  [838400/900000]\n",
      "current batch loss: 0.690076  [844800/900000]\n",
      "current batch loss: 0.634044  [851200/900000]\n",
      "current batch loss: 0.725230  [857600/900000]\n",
      "current batch loss: 0.632420  [864000/900000]\n",
      "current batch loss: 0.608438  [870400/900000]\n",
      "current batch loss: 0.737416  [876800/900000]\n",
      "current batch loss: 0.566541  [883200/900000]\n",
      "current batch loss: 0.608354  [889600/900000]\n",
      "current batch loss: 0.635929  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.636237\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.635098\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 12\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.609058  [    0/900000]\n",
      "current batch loss: 0.621542  [ 6400/900000]\n",
      "current batch loss: 0.596003  [12800/900000]\n",
      "current batch loss: 0.652915  [19200/900000]\n",
      "current batch loss: 0.628996  [25600/900000]\n",
      "current batch loss: 0.654118  [32000/900000]\n",
      "current batch loss: 0.586536  [38400/900000]\n",
      "current batch loss: 0.596656  [44800/900000]\n",
      "current batch loss: 0.668852  [51200/900000]\n",
      "current batch loss: 0.620485  [57600/900000]\n",
      "current batch loss: 0.558348  [64000/900000]\n",
      "current batch loss: 0.669026  [70400/900000]\n",
      "current batch loss: 0.618439  [76800/900000]\n",
      "current batch loss: 0.650486  [83200/900000]\n",
      "current batch loss: 0.636954  [89600/900000]\n",
      "current batch loss: 0.625994  [96000/900000]\n",
      "current batch loss: 0.602318  [102400/900000]\n",
      "current batch loss: 0.652404  [108800/900000]\n",
      "current batch loss: 0.643627  [115200/900000]\n",
      "current batch loss: 0.594232  [121600/900000]\n",
      "current batch loss: 0.600373  [128000/900000]\n",
      "current batch loss: 0.608414  [134400/900000]\n",
      "current batch loss: 0.639648  [140800/900000]\n",
      "current batch loss: 0.617950  [147200/900000]\n",
      "current batch loss: 0.642839  [153600/900000]\n",
      "current batch loss: 0.556535  [160000/900000]\n",
      "current batch loss: 0.752061  [166400/900000]\n",
      "current batch loss: 0.640111  [172800/900000]\n",
      "current batch loss: 0.597890  [179200/900000]\n",
      "current batch loss: 0.638346  [185600/900000]\n",
      "current batch loss: 0.673098  [192000/900000]\n",
      "current batch loss: 0.593679  [198400/900000]\n",
      "current batch loss: 0.631197  [204800/900000]\n",
      "current batch loss: 0.604573  [211200/900000]\n",
      "current batch loss: 0.701704  [217600/900000]\n",
      "current batch loss: 0.646490  [224000/900000]\n",
      "current batch loss: 0.682508  [230400/900000]\n",
      "current batch loss: 0.558451  [236800/900000]\n",
      "current batch loss: 0.587944  [243200/900000]\n",
      "current batch loss: 0.643208  [249600/900000]\n",
      "current batch loss: 0.647242  [256000/900000]\n",
      "current batch loss: 0.596182  [262400/900000]\n",
      "current batch loss: 0.722408  [268800/900000]\n",
      "current batch loss: 0.621000  [275200/900000]\n",
      "current batch loss: 0.631770  [281600/900000]\n",
      "current batch loss: 0.681571  [288000/900000]\n",
      "current batch loss: 0.686337  [294400/900000]\n",
      "current batch loss: 0.612288  [300800/900000]\n",
      "current batch loss: 0.655995  [307200/900000]\n",
      "current batch loss: 0.674951  [313600/900000]\n",
      "current batch loss: 0.669082  [320000/900000]\n",
      "current batch loss: 0.630812  [326400/900000]\n",
      "current batch loss: 0.709840  [332800/900000]\n",
      "current batch loss: 0.716565  [339200/900000]\n",
      "current batch loss: 0.681095  [345600/900000]\n",
      "current batch loss: 0.664128  [352000/900000]\n",
      "current batch loss: 0.621325  [358400/900000]\n",
      "current batch loss: 0.719207  [364800/900000]\n",
      "current batch loss: 0.588743  [371200/900000]\n",
      "current batch loss: 0.602809  [377600/900000]\n",
      "current batch loss: 0.633612  [384000/900000]\n",
      "current batch loss: 0.617172  [390400/900000]\n",
      "current batch loss: 0.619567  [396800/900000]\n",
      "current batch loss: 0.613873  [403200/900000]\n",
      "current batch loss: 0.536362  [409600/900000]\n",
      "current batch loss: 0.639351  [416000/900000]\n",
      "current batch loss: 0.617869  [422400/900000]\n",
      "current batch loss: 0.578287  [428800/900000]\n",
      "current batch loss: 0.640290  [435200/900000]\n",
      "current batch loss: 0.651983  [441600/900000]\n",
      "current batch loss: 0.578657  [448000/900000]\n",
      "current batch loss: 0.694343  [454400/900000]\n",
      "current batch loss: 0.647512  [460800/900000]\n",
      "current batch loss: 0.564480  [467200/900000]\n",
      "current batch loss: 0.659628  [473600/900000]\n",
      "current batch loss: 0.624840  [480000/900000]\n",
      "current batch loss: 0.718888  [486400/900000]\n",
      "current batch loss: 0.651597  [492800/900000]\n",
      "current batch loss: 0.651354  [499200/900000]\n",
      "current batch loss: 0.695286  [505600/900000]\n",
      "current batch loss: 0.612204  [512000/900000]\n",
      "current batch loss: 0.650674  [518400/900000]\n",
      "current batch loss: 0.609042  [524800/900000]\n",
      "current batch loss: 0.575695  [531200/900000]\n",
      "current batch loss: 0.624323  [537600/900000]\n",
      "current batch loss: 0.628739  [544000/900000]\n",
      "current batch loss: 0.615000  [550400/900000]\n",
      "current batch loss: 0.665317  [556800/900000]\n",
      "current batch loss: 0.571852  [563200/900000]\n",
      "current batch loss: 0.571335  [569600/900000]\n",
      "current batch loss: 0.720290  [576000/900000]\n",
      "current batch loss: 0.615013  [582400/900000]\n",
      "current batch loss: 0.635929  [588800/900000]\n",
      "current batch loss: 0.613796  [595200/900000]\n",
      "current batch loss: 0.664862  [601600/900000]\n",
      "current batch loss: 0.710888  [608000/900000]\n",
      "current batch loss: 0.725126  [614400/900000]\n",
      "current batch loss: 0.640964  [620800/900000]\n",
      "current batch loss: 0.555504  [627200/900000]\n",
      "current batch loss: 0.581100  [633600/900000]\n",
      "current batch loss: 0.609754  [640000/900000]\n",
      "current batch loss: 0.632524  [646400/900000]\n",
      "current batch loss: 0.607257  [652800/900000]\n",
      "current batch loss: 0.682872  [659200/900000]\n",
      "current batch loss: 0.679358  [665600/900000]\n",
      "current batch loss: 0.615710  [672000/900000]\n",
      "current batch loss: 0.626799  [678400/900000]\n",
      "current batch loss: 0.642773  [684800/900000]\n",
      "current batch loss: 0.648504  [691200/900000]\n",
      "current batch loss: 0.620404  [697600/900000]\n",
      "current batch loss: 0.697230  [704000/900000]\n",
      "current batch loss: 0.598318  [710400/900000]\n",
      "current batch loss: 0.635127  [716800/900000]\n",
      "current batch loss: 0.641431  [723200/900000]\n",
      "current batch loss: 0.619899  [729600/900000]\n",
      "current batch loss: 0.582060  [736000/900000]\n",
      "current batch loss: 0.629154  [742400/900000]\n",
      "current batch loss: 0.671031  [748800/900000]\n",
      "current batch loss: 0.605247  [755200/900000]\n",
      "current batch loss: 0.702674  [761600/900000]\n",
      "current batch loss: 0.655010  [768000/900000]\n",
      "current batch loss: 0.617381  [774400/900000]\n",
      "current batch loss: 0.662329  [780800/900000]\n",
      "current batch loss: 0.631186  [787200/900000]\n",
      "current batch loss: 0.651516  [793600/900000]\n",
      "current batch loss: 0.625461  [800000/900000]\n",
      "current batch loss: 0.682417  [806400/900000]\n",
      "current batch loss: 0.673837  [812800/900000]\n",
      "current batch loss: 0.589609  [819200/900000]\n",
      "current batch loss: 0.565538  [825600/900000]\n",
      "current batch loss: 0.615090  [832000/900000]\n",
      "current batch loss: 0.610258  [838400/900000]\n",
      "current batch loss: 0.585833  [844800/900000]\n",
      "current batch loss: 0.598827  [851200/900000]\n",
      "current batch loss: 0.647633  [857600/900000]\n",
      "current batch loss: 0.608539  [864000/900000]\n",
      "current batch loss: 0.607048  [870400/900000]\n",
      "current batch loss: 0.624611  [876800/900000]\n",
      "current batch loss: 0.601531  [883200/900000]\n",
      "current batch loss: 0.750596  [889600/900000]\n",
      "current batch loss: 0.595642  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.638462\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.637382\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 13\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.676187  [    0/900000]\n",
      "current batch loss: 0.627855  [ 6400/900000]\n",
      "current batch loss: 0.605654  [12800/900000]\n",
      "current batch loss: 0.608919  [19200/900000]\n",
      "current batch loss: 0.618123  [25600/900000]\n",
      "current batch loss: 0.640881  [32000/900000]\n",
      "current batch loss: 0.546822  [38400/900000]\n",
      "current batch loss: 0.568000  [44800/900000]\n",
      "current batch loss: 0.579046  [51200/900000]\n",
      "current batch loss: 0.615747  [57600/900000]\n",
      "current batch loss: 0.615329  [64000/900000]\n",
      "current batch loss: 0.668021  [70400/900000]\n",
      "current batch loss: 0.656881  [76800/900000]\n",
      "current batch loss: 0.608590  [83200/900000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.635928  [89600/900000]\n",
      "current batch loss: 0.655502  [96000/900000]\n",
      "current batch loss: 0.705398  [102400/900000]\n",
      "current batch loss: 0.564955  [108800/900000]\n",
      "current batch loss: 0.773874  [115200/900000]\n",
      "current batch loss: 0.634274  [121600/900000]\n",
      "current batch loss: 0.631190  [128000/900000]\n",
      "current batch loss: 0.565450  [134400/900000]\n",
      "current batch loss: 0.597924  [140800/900000]\n",
      "current batch loss: 0.616849  [147200/900000]\n",
      "current batch loss: 0.680572  [153600/900000]\n",
      "current batch loss: 0.638314  [160000/900000]\n",
      "current batch loss: 0.598969  [166400/900000]\n",
      "current batch loss: 0.631149  [172800/900000]\n",
      "current batch loss: 0.616078  [179200/900000]\n",
      "current batch loss: 0.630327  [185600/900000]\n",
      "current batch loss: 0.643353  [192000/900000]\n",
      "current batch loss: 0.589888  [198400/900000]\n",
      "current batch loss: 0.601004  [204800/900000]\n",
      "current batch loss: 0.580193  [211200/900000]\n",
      "current batch loss: 0.700726  [217600/900000]\n",
      "current batch loss: 0.643517  [224000/900000]\n",
      "current batch loss: 0.582563  [230400/900000]\n",
      "current batch loss: 0.604823  [236800/900000]\n",
      "current batch loss: 0.640497  [243200/900000]\n",
      "current batch loss: 0.670109  [249600/900000]\n",
      "current batch loss: 0.626666  [256000/900000]\n",
      "current batch loss: 0.636245  [262400/900000]\n",
      "current batch loss: 0.617406  [268800/900000]\n",
      "current batch loss: 0.635118  [275200/900000]\n",
      "current batch loss: 0.620453  [281600/900000]\n",
      "current batch loss: 0.631437  [288000/900000]\n",
      "current batch loss: 0.657661  [294400/900000]\n",
      "current batch loss: 0.645551  [300800/900000]\n",
      "current batch loss: 0.613541  [307200/900000]\n",
      "current batch loss: 0.675669  [313600/900000]\n",
      "current batch loss: 0.688143  [320000/900000]\n",
      "current batch loss: 0.630397  [326400/900000]\n",
      "current batch loss: 0.622826  [332800/900000]\n",
      "current batch loss: 0.605172  [339200/900000]\n",
      "current batch loss: 0.639740  [345600/900000]\n",
      "current batch loss: 0.702174  [352000/900000]\n",
      "current batch loss: 0.651134  [358400/900000]\n",
      "current batch loss: 0.635222  [364800/900000]\n",
      "current batch loss: 0.561124  [371200/900000]\n",
      "current batch loss: 0.635348  [377600/900000]\n",
      "current batch loss: 0.722676  [384000/900000]\n",
      "current batch loss: 0.596021  [390400/900000]\n",
      "current batch loss: 0.569583  [396800/900000]\n",
      "current batch loss: 0.675956  [403200/900000]\n",
      "current batch loss: 0.562361  [409600/900000]\n",
      "current batch loss: 0.547926  [416000/900000]\n",
      "current batch loss: 0.655129  [422400/900000]\n",
      "current batch loss: 0.617964  [428800/900000]\n",
      "current batch loss: 0.600774  [435200/900000]\n",
      "current batch loss: 0.645789  [441600/900000]\n",
      "current batch loss: 0.596843  [448000/900000]\n",
      "current batch loss: 0.639220  [454400/900000]\n",
      "current batch loss: 0.630524  [460800/900000]\n",
      "current batch loss: 0.643515  [467200/900000]\n",
      "current batch loss: 0.604120  [473600/900000]\n",
      "current batch loss: 0.653512  [480000/900000]\n",
      "current batch loss: 0.633199  [486400/900000]\n",
      "current batch loss: 0.650725  [492800/900000]\n",
      "current batch loss: 0.607046  [499200/900000]\n",
      "current batch loss: 0.661208  [505600/900000]\n",
      "current batch loss: 0.597694  [512000/900000]\n",
      "current batch loss: 0.602132  [518400/900000]\n",
      "current batch loss: 0.563443  [524800/900000]\n",
      "current batch loss: 0.618590  [531200/900000]\n",
      "current batch loss: 0.675635  [537600/900000]\n",
      "current batch loss: 0.712723  [544000/900000]\n",
      "current batch loss: 0.614011  [550400/900000]\n",
      "current batch loss: 0.656670  [556800/900000]\n",
      "current batch loss: 0.596633  [563200/900000]\n",
      "current batch loss: 0.649305  [569600/900000]\n",
      "current batch loss: 0.555108  [576000/900000]\n",
      "current batch loss: 0.558260  [582400/900000]\n",
      "current batch loss: 0.658970  [588800/900000]\n",
      "current batch loss: 0.656014  [595200/900000]\n",
      "current batch loss: 0.645786  [601600/900000]\n",
      "current batch loss: 0.671366  [608000/900000]\n",
      "current batch loss: 0.713341  [614400/900000]\n",
      "current batch loss: 0.610411  [620800/900000]\n",
      "current batch loss: 0.630063  [627200/900000]\n",
      "current batch loss: 0.679786  [633600/900000]\n",
      "current batch loss: 0.742399  [640000/900000]\n",
      "current batch loss: 0.660276  [646400/900000]\n",
      "current batch loss: 0.665085  [652800/900000]\n",
      "current batch loss: 0.658523  [659200/900000]\n",
      "current batch loss: 0.657316  [665600/900000]\n",
      "current batch loss: 0.662095  [672000/900000]\n",
      "current batch loss: 0.621473  [678400/900000]\n",
      "current batch loss: 0.623676  [684800/900000]\n",
      "current batch loss: 0.585708  [691200/900000]\n",
      "current batch loss: 0.561406  [697600/900000]\n",
      "current batch loss: 0.652583  [704000/900000]\n",
      "current batch loss: 0.594483  [710400/900000]\n",
      "current batch loss: 0.586879  [716800/900000]\n",
      "current batch loss: 0.695840  [723200/900000]\n",
      "current batch loss: 0.593356  [729600/900000]\n",
      "current batch loss: 0.626206  [736000/900000]\n",
      "current batch loss: 0.639886  [742400/900000]\n",
      "current batch loss: 0.608914  [748800/900000]\n",
      "current batch loss: 0.659632  [755200/900000]\n",
      "current batch loss: 0.604150  [761600/900000]\n",
      "current batch loss: 0.569907  [768000/900000]\n",
      "current batch loss: 0.612537  [774400/900000]\n",
      "current batch loss: 0.686837  [780800/900000]\n",
      "current batch loss: 0.603207  [787200/900000]\n",
      "current batch loss: 0.601019  [793600/900000]\n",
      "current batch loss: 0.742249  [800000/900000]\n",
      "current batch loss: 0.616780  [806400/900000]\n",
      "current batch loss: 0.626456  [812800/900000]\n",
      "current batch loss: 0.607252  [819200/900000]\n",
      "current batch loss: 0.658447  [825600/900000]\n",
      "current batch loss: 0.630394  [832000/900000]\n",
      "current batch loss: 0.618344  [838400/900000]\n",
      "current batch loss: 0.695534  [844800/900000]\n",
      "current batch loss: 0.659380  [851200/900000]\n",
      "current batch loss: 0.646647  [857600/900000]\n",
      "current batch loss: 0.661325  [864000/900000]\n",
      "current batch loss: 0.669311  [870400/900000]\n",
      "current batch loss: 0.653786  [876800/900000]\n",
      "current batch loss: 0.648106  [883200/900000]\n",
      "current batch loss: 0.642739  [889600/900000]\n",
      "current batch loss: 0.598952  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.635897\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.634905\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 14\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.657382  [    0/900000]\n",
      "current batch loss: 0.586962  [ 6400/900000]\n",
      "current batch loss: 0.613679  [12800/900000]\n",
      "current batch loss: 0.723966  [19200/900000]\n",
      "current batch loss: 0.630298  [25600/900000]\n",
      "current batch loss: 0.558525  [32000/900000]\n",
      "current batch loss: 0.677206  [38400/900000]\n",
      "current batch loss: 0.648280  [44800/900000]\n",
      "current batch loss: 0.650805  [51200/900000]\n",
      "current batch loss: 0.559501  [57600/900000]\n",
      "current batch loss: 0.606229  [64000/900000]\n",
      "current batch loss: 0.636483  [70400/900000]\n",
      "current batch loss: 0.660682  [76800/900000]\n",
      "current batch loss: 0.557561  [83200/900000]\n",
      "current batch loss: 0.584120  [89600/900000]\n",
      "current batch loss: 0.588228  [96000/900000]\n",
      "current batch loss: 0.633776  [102400/900000]\n",
      "current batch loss: 0.650853  [108800/900000]\n",
      "current batch loss: 0.648819  [115200/900000]\n",
      "current batch loss: 0.598769  [121600/900000]\n",
      "current batch loss: 0.651409  [128000/900000]\n",
      "current batch loss: 0.634884  [134400/900000]\n",
      "current batch loss: 0.645045  [140800/900000]\n",
      "current batch loss: 0.605467  [147200/900000]\n",
      "current batch loss: 0.621632  [153600/900000]\n",
      "current batch loss: 0.628617  [160000/900000]\n",
      "current batch loss: 0.581940  [166400/900000]\n",
      "current batch loss: 0.705726  [172800/900000]\n",
      "current batch loss: 0.631498  [179200/900000]\n",
      "current batch loss: 0.647483  [185600/900000]\n",
      "current batch loss: 0.594588  [192000/900000]\n",
      "current batch loss: 0.646519  [198400/900000]\n",
      "current batch loss: 0.607407  [204800/900000]\n",
      "current batch loss: 0.632990  [211200/900000]\n",
      "current batch loss: 0.633736  [217600/900000]\n",
      "current batch loss: 0.562425  [224000/900000]\n",
      "current batch loss: 0.657740  [230400/900000]\n",
      "current batch loss: 0.559986  [236800/900000]\n",
      "current batch loss: 0.620711  [243200/900000]\n",
      "current batch loss: 0.642402  [249600/900000]\n",
      "current batch loss: 0.585851  [256000/900000]\n",
      "current batch loss: 0.632969  [262400/900000]\n",
      "current batch loss: 0.643398  [268800/900000]\n",
      "current batch loss: 0.630777  [275200/900000]\n",
      "current batch loss: 0.680230  [281600/900000]\n",
      "current batch loss: 0.592464  [288000/900000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.556378  [294400/900000]\n",
      "current batch loss: 0.672798  [300800/900000]\n",
      "current batch loss: 0.648322  [307200/900000]\n",
      "current batch loss: 0.585453  [313600/900000]\n",
      "current batch loss: 0.586947  [320000/900000]\n",
      "current batch loss: 0.629632  [326400/900000]\n",
      "current batch loss: 0.594458  [332800/900000]\n",
      "current batch loss: 0.605754  [339200/900000]\n",
      "current batch loss: 0.574241  [345600/900000]\n",
      "current batch loss: 0.600899  [352000/900000]\n",
      "current batch loss: 0.671500  [358400/900000]\n",
      "current batch loss: 0.633459  [364800/900000]\n",
      "current batch loss: 0.637639  [371200/900000]\n",
      "current batch loss: 0.624486  [377600/900000]\n",
      "current batch loss: 0.641779  [384000/900000]\n",
      "current batch loss: 0.612836  [390400/900000]\n",
      "current batch loss: 0.687444  [396800/900000]\n",
      "current batch loss: 0.593604  [403200/900000]\n",
      "current batch loss: 0.679997  [409600/900000]\n",
      "current batch loss: 0.672572  [416000/900000]\n",
      "current batch loss: 0.690047  [422400/900000]\n",
      "current batch loss: 0.601801  [428800/900000]\n",
      "current batch loss: 0.627432  [435200/900000]\n",
      "current batch loss: 0.633426  [441600/900000]\n",
      "current batch loss: 0.602392  [448000/900000]\n",
      "current batch loss: 0.671347  [454400/900000]\n",
      "current batch loss: 0.564058  [460800/900000]\n",
      "current batch loss: 0.639272  [467200/900000]\n",
      "current batch loss: 0.570799  [473600/900000]\n",
      "current batch loss: 0.671813  [480000/900000]\n",
      "current batch loss: 0.656024  [486400/900000]\n",
      "current batch loss: 0.608222  [492800/900000]\n",
      "current batch loss: 0.647701  [499200/900000]\n",
      "current batch loss: 0.616227  [505600/900000]\n",
      "current batch loss: 0.648097  [512000/900000]\n",
      "current batch loss: 0.628241  [518400/900000]\n",
      "current batch loss: 0.549206  [524800/900000]\n",
      "current batch loss: 0.630347  [531200/900000]\n",
      "current batch loss: 0.728940  [537600/900000]\n",
      "current batch loss: 0.684415  [544000/900000]\n",
      "current batch loss: 0.608708  [550400/900000]\n",
      "current batch loss: 0.627942  [556800/900000]\n",
      "current batch loss: 0.669862  [563200/900000]\n",
      "current batch loss: 0.621535  [569600/900000]\n",
      "current batch loss: 0.629667  [576000/900000]\n",
      "current batch loss: 0.651986  [582400/900000]\n",
      "current batch loss: 0.710150  [588800/900000]\n",
      "current batch loss: 0.670332  [595200/900000]\n",
      "current batch loss: 0.641054  [601600/900000]\n",
      "current batch loss: 0.593158  [608000/900000]\n",
      "current batch loss: 0.664387  [614400/900000]\n",
      "current batch loss: 0.659124  [620800/900000]\n",
      "current batch loss: 0.636334  [627200/900000]\n",
      "current batch loss: 0.602162  [633600/900000]\n",
      "current batch loss: 0.589825  [640000/900000]\n",
      "current batch loss: 0.678815  [646400/900000]\n",
      "current batch loss: 0.592037  [652800/900000]\n",
      "current batch loss: 0.650626  [659200/900000]\n",
      "current batch loss: 0.577335  [665600/900000]\n",
      "current batch loss: 0.632301  [672000/900000]\n",
      "current batch loss: 0.649571  [678400/900000]\n",
      "current batch loss: 0.632632  [684800/900000]\n",
      "current batch loss: 0.626067  [691200/900000]\n",
      "current batch loss: 0.624150  [697600/900000]\n",
      "current batch loss: 0.648490  [704000/900000]\n",
      "current batch loss: 0.682182  [710400/900000]\n",
      "current batch loss: 0.615591  [716800/900000]\n",
      "current batch loss: 0.618190  [723200/900000]\n",
      "current batch loss: 0.628558  [729600/900000]\n",
      "current batch loss: 0.684263  [736000/900000]\n",
      "current batch loss: 0.691037  [742400/900000]\n",
      "current batch loss: 0.618683  [748800/900000]\n",
      "current batch loss: 0.680981  [755200/900000]\n",
      "current batch loss: 0.583673  [761600/900000]\n",
      "current batch loss: 0.642791  [768000/900000]\n",
      "current batch loss: 0.586296  [774400/900000]\n",
      "current batch loss: 0.649542  [780800/900000]\n",
      "current batch loss: 0.651765  [787200/900000]\n",
      "current batch loss: 0.649114  [793600/900000]\n",
      "current batch loss: 0.598661  [800000/900000]\n",
      "current batch loss: 0.673845  [806400/900000]\n",
      "current batch loss: 0.625607  [812800/900000]\n",
      "current batch loss: 0.593361  [819200/900000]\n",
      "current batch loss: 0.675569  [825600/900000]\n",
      "current batch loss: 0.619433  [832000/900000]\n",
      "current batch loss: 0.677286  [838400/900000]\n",
      "current batch loss: 0.599978  [844800/900000]\n",
      "current batch loss: 0.649469  [851200/900000]\n",
      "current batch loss: 0.549645  [857600/900000]\n",
      "current batch loss: 0.671280  [864000/900000]\n",
      "current batch loss: 0.566962  [870400/900000]\n",
      "current batch loss: 0.644713  [876800/900000]\n",
      "current batch loss: 0.709997  [883200/900000]\n",
      "current batch loss: 0.672536  [889600/900000]\n",
      "current batch loss: 0.636772  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.634590\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.633795\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 15\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.706495  [    0/900000]\n",
      "current batch loss: 0.647815  [ 6400/900000]\n",
      "current batch loss: 0.652315  [12800/900000]\n",
      "current batch loss: 0.631649  [19200/900000]\n",
      "current batch loss: 0.606418  [25600/900000]\n",
      "current batch loss: 0.605258  [32000/900000]\n",
      "current batch loss: 0.529539  [38400/900000]\n",
      "current batch loss: 0.656771  [44800/900000]\n",
      "current batch loss: 0.699867  [51200/900000]\n",
      "current batch loss: 0.632802  [57600/900000]\n",
      "current batch loss: 0.693036  [64000/900000]\n",
      "current batch loss: 0.594763  [70400/900000]\n",
      "current batch loss: 0.590965  [76800/900000]\n",
      "current batch loss: 0.627261  [83200/900000]\n",
      "current batch loss: 0.738503  [89600/900000]\n",
      "current batch loss: 0.566920  [96000/900000]\n",
      "current batch loss: 0.642625  [102400/900000]\n",
      "current batch loss: 0.669972  [108800/900000]\n",
      "current batch loss: 0.648035  [115200/900000]\n",
      "current batch loss: 0.590177  [121600/900000]\n",
      "current batch loss: 0.623171  [128000/900000]\n",
      "current batch loss: 0.630082  [134400/900000]\n",
      "current batch loss: 0.585667  [140800/900000]\n",
      "current batch loss: 0.683669  [147200/900000]\n",
      "current batch loss: 0.594940  [153600/900000]\n",
      "current batch loss: 0.603835  [160000/900000]\n",
      "current batch loss: 0.641876  [166400/900000]\n",
      "current batch loss: 0.512429  [172800/900000]\n",
      "current batch loss: 0.587343  [179200/900000]\n",
      "current batch loss: 0.628447  [185600/900000]\n",
      "current batch loss: 0.616908  [192000/900000]\n",
      "current batch loss: 0.573372  [198400/900000]\n",
      "current batch loss: 0.643767  [204800/900000]\n",
      "current batch loss: 0.590082  [211200/900000]\n",
      "current batch loss: 0.578031  [217600/900000]\n",
      "current batch loss: 0.645464  [224000/900000]\n",
      "current batch loss: 0.704082  [230400/900000]\n",
      "current batch loss: 0.650429  [236800/900000]\n",
      "current batch loss: 0.631174  [243200/900000]\n",
      "current batch loss: 0.631244  [249600/900000]\n",
      "current batch loss: 0.625942  [256000/900000]\n",
      "current batch loss: 0.658222  [262400/900000]\n",
      "current batch loss: 0.612877  [268800/900000]\n",
      "current batch loss: 0.597101  [275200/900000]\n",
      "current batch loss: 0.665078  [281600/900000]\n",
      "current batch loss: 0.622674  [288000/900000]\n",
      "current batch loss: 0.612399  [294400/900000]\n",
      "current batch loss: 0.676493  [300800/900000]\n",
      "current batch loss: 0.624938  [307200/900000]\n",
      "current batch loss: 0.627120  [313600/900000]\n",
      "current batch loss: 0.592641  [320000/900000]\n",
      "current batch loss: 0.599641  [326400/900000]\n",
      "current batch loss: 0.690212  [332800/900000]\n",
      "current batch loss: 0.587942  [339200/900000]\n",
      "current batch loss: 0.571432  [345600/900000]\n",
      "current batch loss: 0.708847  [352000/900000]\n",
      "current batch loss: 0.629500  [358400/900000]\n",
      "current batch loss: 0.633491  [364800/900000]\n",
      "current batch loss: 0.565630  [371200/900000]\n",
      "current batch loss: 0.729544  [377600/900000]\n",
      "current batch loss: 0.643846  [384000/900000]\n",
      "current batch loss: 0.580584  [390400/900000]\n",
      "current batch loss: 0.666388  [396800/900000]\n",
      "current batch loss: 0.580786  [403200/900000]\n",
      "current batch loss: 0.637087  [409600/900000]\n",
      "current batch loss: 0.605615  [416000/900000]\n",
      "current batch loss: 0.625483  [422400/900000]\n",
      "current batch loss: 0.660323  [428800/900000]\n",
      "current batch loss: 0.687763  [435200/900000]\n",
      "current batch loss: 0.605347  [441600/900000]\n",
      "current batch loss: 0.624435  [448000/900000]\n",
      "current batch loss: 0.617911  [454400/900000]\n",
      "current batch loss: 0.604906  [460800/900000]\n",
      "current batch loss: 0.638145  [467200/900000]\n",
      "current batch loss: 0.639194  [473600/900000]\n",
      "current batch loss: 0.631007  [480000/900000]\n",
      "current batch loss: 0.626716  [486400/900000]\n",
      "current batch loss: 0.594285  [492800/900000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.635851  [499200/900000]\n",
      "current batch loss: 0.676225  [505600/900000]\n",
      "current batch loss: 0.603111  [512000/900000]\n",
      "current batch loss: 0.641665  [518400/900000]\n",
      "current batch loss: 0.661459  [524800/900000]\n",
      "current batch loss: 0.640863  [531200/900000]\n",
      "current batch loss: 0.607023  [537600/900000]\n",
      "current batch loss: 0.637975  [544000/900000]\n",
      "current batch loss: 0.683819  [550400/900000]\n",
      "current batch loss: 0.588153  [556800/900000]\n",
      "current batch loss: 0.626523  [563200/900000]\n",
      "current batch loss: 0.645025  [569600/900000]\n",
      "current batch loss: 0.646732  [576000/900000]\n",
      "current batch loss: 0.613351  [582400/900000]\n",
      "current batch loss: 0.642735  [588800/900000]\n",
      "current batch loss: 0.627755  [595200/900000]\n",
      "current batch loss: 0.573385  [601600/900000]\n",
      "current batch loss: 0.718274  [608000/900000]\n",
      "current batch loss: 0.596833  [614400/900000]\n",
      "current batch loss: 0.614967  [620800/900000]\n",
      "current batch loss: 0.623732  [627200/900000]\n",
      "current batch loss: 0.626591  [633600/900000]\n",
      "current batch loss: 0.584766  [640000/900000]\n",
      "current batch loss: 0.657973  [646400/900000]\n",
      "current batch loss: 0.650157  [652800/900000]\n",
      "current batch loss: 0.472541  [659200/900000]\n",
      "current batch loss: 0.685474  [665600/900000]\n",
      "current batch loss: 0.638958  [672000/900000]\n",
      "current batch loss: 0.597017  [678400/900000]\n",
      "current batch loss: 0.712740  [684800/900000]\n",
      "current batch loss: 0.662613  [691200/900000]\n",
      "current batch loss: 0.582614  [697600/900000]\n",
      "current batch loss: 0.527668  [704000/900000]\n",
      "current batch loss: 0.617996  [710400/900000]\n",
      "current batch loss: 0.657975  [716800/900000]\n",
      "current batch loss: 0.592891  [723200/900000]\n",
      "current batch loss: 0.736802  [729600/900000]\n",
      "current batch loss: 0.654456  [736000/900000]\n",
      "current batch loss: 0.675561  [742400/900000]\n",
      "current batch loss: 0.637420  [748800/900000]\n",
      "current batch loss: 0.616767  [755200/900000]\n",
      "current batch loss: 0.680738  [761600/900000]\n",
      "current batch loss: 0.645526  [768000/900000]\n",
      "current batch loss: 0.662693  [774400/900000]\n",
      "current batch loss: 0.664227  [780800/900000]\n",
      "current batch loss: 0.682677  [787200/900000]\n",
      "current batch loss: 0.621612  [793600/900000]\n",
      "current batch loss: 0.578514  [800000/900000]\n",
      "current batch loss: 0.657383  [806400/900000]\n",
      "current batch loss: 0.693992  [812800/900000]\n",
      "current batch loss: 0.575838  [819200/900000]\n",
      "current batch loss: 0.619302  [825600/900000]\n",
      "current batch loss: 0.658885  [832000/900000]\n",
      "current batch loss: 0.632804  [838400/900000]\n",
      "current batch loss: 0.604309  [844800/900000]\n",
      "current batch loss: 0.589319  [851200/900000]\n",
      "current batch loss: 0.620209  [857600/900000]\n",
      "current batch loss: 0.628088  [864000/900000]\n",
      "current batch loss: 0.708319  [870400/900000]\n",
      "current batch loss: 0.714188  [876800/900000]\n",
      "current batch loss: 0.639528  [883200/900000]\n",
      "current batch loss: 0.661595  [889600/900000]\n",
      "current batch loss: 0.672570  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.632758\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.631670\n",
      "-----------------------------------------------\n",
      "|\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# a useful function to present things clearer\n",
    "def separator():\n",
    "    print( \"-----------------------------------------------\" )\n",
    "\n",
    "epochs = 15\n",
    "\n",
    "# re-initialise the model and the optimizer\n",
    "model = cwolaNet( 4, 64 ).to(device)\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam( model.parameters(), lr=learning_rate )\n",
    "separator()\n",
    "print( \"model architecture \")\n",
    "separator()\n",
    "print( model )\n",
    "\n",
    "# track train and val losses\n",
    "trn_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for t in range(epochs):\n",
    "    separator()\n",
    "    print( f\"Epoch {t+1}\" )\n",
    "    separator()\n",
    "    train_epoch( trn_dataloader, model, loss_fn, optimizer )\n",
    "    separator()\n",
    "    trn_loss = trn_pass( trn_dataloader, model, loss_fn )\n",
    "    trn_losses.append( trn_loss )\n",
    "    separator()\n",
    "    val_loss = val_pass( val_dataloader, model, loss_fn )\n",
    "    val_losses.append( val_loss )\n",
    "    separator()\n",
    "    print( \"|\" )\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's plot the training and validation losses to see how they look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots( 1, 1, figsize=(7,5) )\n",
    "\n",
    "c1 = 'tab:red'\n",
    "c2 = 'tab:green'\n",
    "\n",
    "axs.plot( trn_losses, label=\"train loss\", color=c1 )\n",
    "axs.plot( val_losses, label=\"val   loss\", color=c2 )\n",
    "\n",
    "axs.set_yscale('log')\n",
    "\n",
    "axs.set_xlabel( \"epoch\", fontproperties=axislabelfont )\n",
    "axs.set_ylabel( \"Binary CE\", fontproperties=axislabelfont )\n",
    "\n",
    "xticks = [ int(x) for x in axs.get_xticks() ]\n",
    "axs.set_xticklabels( xticks, fontproperties=tickfont )\n",
    "\n",
    "yticks = axs.get_yticks()\n",
    "axs.set_yticklabels( yticks, fontproperties=tickfont )\n",
    "\n",
    "axs.legend( loc='best', prop=tickfont )\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to look at how good this classifier is at tagging the signal events.  We do this in the exact same way as we do for a supervised classifier.  Note that while the classifier is trained on bin labels, the performance is evaluated on truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model( X_test_p ).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "def closest_point(array, tpr_p=0.3):\n",
    "    dist = ((array-tpr_p)**2)\n",
    "    return np.argmin(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(14,5))\n",
    "\n",
    "fpr, tpr, th = roc_curve( y_test, test_pred )\n",
    "auc_score = roc_auc_score( y_test, test_pred )\n",
    "rnd_class = np.linspace(0, 1, 100)\n",
    "\n",
    "ax[0].plot(fpr, tpr, label='AUC = {:.2f}'.format(auc_score) )\n",
    "ax[0].plot(rnd_class, rnd_class, '--', label='Rnd classifier')\n",
    "ax[1].plot(tpr, 1/fpr, label='AUC = {:.2f}\\n $1/\\epsilon_{{bkg}}$(0.3) = {:.0f}'.format(auc_score, 1/fpr[closest_point(tpr, tpr_p=0.3)]))\n",
    "ax[1].plot(rnd_class, 1/rnd_class, '--', label='Rnd classifier')\n",
    "ax[1].set_yscale('log')\n",
    "\n",
    "ax[0].set_xlabel('$\\epsilon_{bkg}$ - FPR', fontproperties=axislabelfont)\n",
    "ax[0].set_ylabel('$\\epsilon_{s}$ - TPR', fontproperties=axislabelfont)\n",
    "\n",
    "ax[1].set_xlabel('$\\epsilon_{s}$ - TPR', fontproperties=axislabelfont)\n",
    "ax[1].set_ylabel('1/$\\epsilon_{bkg}$ - Inverse FPR', fontproperties=axislabelfont)\n",
    "for i in range(len(ax)):\n",
    "    ax[i].legend(prop=axislabelfont)\n",
    "    ax[i].tick_params(labelsize=axisfontsize)\n",
    "    ax[i].grid('on')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results aren't bad given that we did not use truth labels in the training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However this dataset has quite a lot of signal events for an anomaly detection problem, we call this signal-to-background ratio the S/B.  And in the case above it is 10%!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81725, 818275.0, '9.99%')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.sum(), (1.0-y_train).sum(), str( round( ( y_train.sum() / (1.0-y_train).sum() )*100, 2 ) ) + \"%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will lower this to see how well the classifier performs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lowering the S/B $\\rightarrow$ less signal events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's re-define our dataset so that we can specify the S/B we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cwola_data_sb( Dataset ):\n",
    "    \n",
    "    def __init__( self, data, labels,  bins, sb ):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.bins = bins\n",
    "        self.sb = sb\n",
    "        \n",
    "        if self.sb>0.098:\n",
    "            raise Exception( \"S/B is too large, choose <= 0.098\" )\n",
    "        \n",
    "        self.nbkg = int( (1.0-self.labels).sum() )\n",
    "        self.nsig = int( self.labels.sum() )\n",
    "        self.nsig_new = int( round( self.sb * self.nbkg ) )\n",
    "        self.nsig_del = int( self.nsig - self.nsig_new )\n",
    "        self.del_indx = np.where( self.labels == 1.0 )[0][0:self.nsig_del]\n",
    "        self.data_cut = np.delete( self.data, self.del_indx, axis=0 )\n",
    "        self.labels_cut = np.delete( self.labels, self.del_indx, axis=0 )\n",
    "        self.bins_cut = np.delete( self.bins, self.del_indx, axis=0 )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_cut)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_cut[idx], self.labels_cut[idx], self.bins_cut[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can re-define our datasets and dataloaders with the lower S/B.\n",
    "\n",
    "Note that we don't need to lower the S/B for testing, since this dataset is only used to get the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dataset_sb5pc = cwola_data_sb( X_train_p, y_train_p, b_train_p, 0.05 )\n",
    "val_dataset_sb5pc = cwola_data_sb( X_val_p, y_val_p, b_val_p, 0.05 )\n",
    "tst_dataset = cwola_data_sb( X_test_p, y_test_p, b_test_p, 0.098 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dataloader_sb5pc = DataLoader( trn_dataset_sb5pc, batch_size=64, shuffle=True )\n",
    "val_dataloader_sb5pc = DataLoader( val_dataset_sb5pc, batch_size=64, shuffle=True )\n",
    "tst_dataloader = DataLoader( tst_dataset, batch_size=64, shuffle=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train S/B: 5.0%\n",
      "val S/B: 5.0%\n",
      "test S/B: 9.800000190734863%\n"
     ]
    }
   ],
   "source": [
    "print( \"train S/B: \" + str( np.round( ( trn_dataset_sb5pc.labels_cut.sum() / (1.0-trn_dataset_sb5pc.labels_cut).sum() )*100, 2 ).item() ) + \"%\" )\n",
    "print( \"val S/B: \" + str( np.round( ( val_dataset_sb5pc.labels_cut.sum() / (1.0-val_dataset_sb5pc.labels_cut).sum() )*100, 2 ).item() ) + \"%\" )\n",
    "print( \"test S/B: \" + str( np.round( ( tst_dataset.labels_cut.sum() / (1.0-tst_dataset.labels_cut).sum() )*100, 2 ).item() ) + \"%\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([859189, 4]), torch.Size([859189, 1]), torch.Size([859189, 1]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_dataset_sb5pc.data_cut.shape, trn_dataset_sb5pc.labels_cut.shape, trn_dataset_sb5pc.bins_cut.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S/B = 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train S/B: 5.0%\n",
      "val S/B: 5.0%\n",
      "-----------------------------------------------\n",
      "model architecture \n",
      "-----------------------------------------------\n",
      "cwolaNet(\n",
      "  (layer1): Linear(in_features=4, out_features=64, bias=True)\n",
      "  (relu_1): ReLU()\n",
      "  (layer2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (relu_2): ReLU()\n",
      "  (layer3): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "-----------------------------------------------\n",
      "Epoch 1\n",
      "-----------------------------------------------\n",
      "current batch loss: 1.596361  [    0/859189]\n",
      "current batch loss: 0.666138  [ 6400/859189]\n",
      "current batch loss: 0.756707  [12800/859189]\n",
      "current batch loss: 0.692432  [19200/859189]\n",
      "current batch loss: 0.697127  [25600/859189]\n",
      "current batch loss: 0.747289  [32000/859189]\n",
      "current batch loss: 0.663066  [38400/859189]\n",
      "current batch loss: 0.701795  [44800/859189]\n",
      "current batch loss: 0.668573  [51200/859189]\n",
      "current batch loss: 0.659585  [57600/859189]\n",
      "current batch loss: 0.698174  [64000/859189]\n",
      "current batch loss: 0.684341  [70400/859189]\n",
      "current batch loss: 0.629601  [76800/859189]\n",
      "current batch loss: 0.700846  [83200/859189]\n",
      "current batch loss: 0.691900  [89600/859189]\n",
      "current batch loss: 0.613565  [96000/859189]\n",
      "current batch loss: 0.583889  [102400/859189]\n",
      "current batch loss: 0.679642  [108800/859189]\n",
      "current batch loss: 0.755159  [115200/859189]\n",
      "current batch loss: 0.731371  [121600/859189]\n",
      "current batch loss: 0.677944  [128000/859189]\n",
      "current batch loss: 0.682533  [134400/859189]\n",
      "current batch loss: 0.693702  [140800/859189]\n",
      "current batch loss: 0.642683  [147200/859189]\n",
      "current batch loss: 0.680131  [153600/859189]\n",
      "current batch loss: 0.641964  [160000/859189]\n",
      "current batch loss: 0.639970  [166400/859189]\n",
      "current batch loss: 0.664901  [172800/859189]\n",
      "current batch loss: 0.694363  [179200/859189]\n",
      "current batch loss: 0.705335  [185600/859189]\n",
      "current batch loss: 0.734360  [192000/859189]\n",
      "current batch loss: 0.652263  [198400/859189]\n",
      "current batch loss: 0.649517  [204800/859189]\n",
      "current batch loss: 0.640154  [211200/859189]\n",
      "current batch loss: 0.751591  [217600/859189]\n",
      "current batch loss: 0.628730  [224000/859189]\n",
      "current batch loss: 0.629033  [230400/859189]\n",
      "current batch loss: 0.611166  [236800/859189]\n",
      "current batch loss: 0.626941  [243200/859189]\n",
      "current batch loss: 0.713563  [249600/859189]\n",
      "current batch loss: 0.648278  [256000/859189]\n",
      "current batch loss: 0.706969  [262400/859189]\n",
      "current batch loss: 0.640638  [268800/859189]\n",
      "current batch loss: 0.649819  [275200/859189]\n",
      "current batch loss: 0.642104  [281600/859189]\n",
      "current batch loss: 0.615370  [288000/859189]\n",
      "current batch loss: 0.687614  [294400/859189]\n",
      "current batch loss: 0.676821  [300800/859189]\n",
      "current batch loss: 0.633469  [307200/859189]\n",
      "current batch loss: 0.636370  [313600/859189]\n",
      "current batch loss: 0.659091  [320000/859189]\n",
      "current batch loss: 0.633406  [326400/859189]\n",
      "current batch loss: 0.701413  [332800/859189]\n",
      "current batch loss: 0.625232  [339200/859189]\n",
      "current batch loss: 0.701262  [345600/859189]\n",
      "current batch loss: 0.667504  [352000/859189]\n",
      "current batch loss: 0.651664  [358400/859189]\n",
      "current batch loss: 0.657952  [364800/859189]\n",
      "current batch loss: 0.607810  [371200/859189]\n",
      "current batch loss: 0.619743  [377600/859189]\n",
      "current batch loss: 0.649990  [384000/859189]\n",
      "current batch loss: 0.660335  [390400/859189]\n",
      "current batch loss: 0.626734  [396800/859189]\n",
      "current batch loss: 0.615373  [403200/859189]\n",
      "current batch loss: 0.635158  [409600/859189]\n",
      "current batch loss: 0.682306  [416000/859189]\n",
      "current batch loss: 0.654427  [422400/859189]\n",
      "current batch loss: 0.731138  [428800/859189]\n",
      "current batch loss: 0.629387  [435200/859189]\n",
      "current batch loss: 0.653195  [441600/859189]\n",
      "current batch loss: 0.610508  [448000/859189]\n",
      "current batch loss: 0.663513  [454400/859189]\n",
      "current batch loss: 0.644060  [460800/859189]\n",
      "current batch loss: 0.681096  [467200/859189]\n",
      "current batch loss: 0.666659  [473600/859189]\n",
      "current batch loss: 0.644411  [480000/859189]\n",
      "current batch loss: 0.665589  [486400/859189]\n",
      "current batch loss: 0.687775  [492800/859189]\n",
      "current batch loss: 0.614954  [499200/859189]\n",
      "current batch loss: 0.700972  [505600/859189]\n",
      "current batch loss: 0.637048  [512000/859189]\n",
      "current batch loss: 0.646682  [518400/859189]\n",
      "current batch loss: 0.597370  [524800/859189]\n",
      "current batch loss: 0.678179  [531200/859189]\n",
      "current batch loss: 0.652552  [537600/859189]\n",
      "current batch loss: 0.653478  [544000/859189]\n",
      "current batch loss: 0.681999  [550400/859189]\n",
      "current batch loss: 0.663206  [556800/859189]\n",
      "current batch loss: 0.681294  [563200/859189]\n",
      "current batch loss: 0.653854  [569600/859189]\n",
      "current batch loss: 0.703264  [576000/859189]\n",
      "current batch loss: 0.654798  [582400/859189]\n",
      "current batch loss: 0.655614  [588800/859189]\n",
      "current batch loss: 0.644716  [595200/859189]\n",
      "current batch loss: 0.631457  [601600/859189]\n",
      "current batch loss: 0.643665  [608000/859189]\n",
      "current batch loss: 0.597682  [614400/859189]\n",
      "current batch loss: 0.678934  [620800/859189]\n",
      "current batch loss: 0.672215  [627200/859189]\n",
      "current batch loss: 0.650954  [633600/859189]\n",
      "current batch loss: 0.664415  [640000/859189]\n",
      "current batch loss: 0.683332  [646400/859189]\n",
      "current batch loss: 0.660855  [652800/859189]\n",
      "current batch loss: 0.645776  [659200/859189]\n",
      "current batch loss: 0.648934  [665600/859189]\n",
      "current batch loss: 0.694837  [672000/859189]\n",
      "current batch loss: 0.716102  [678400/859189]\n",
      "current batch loss: 0.645596  [684800/859189]\n",
      "current batch loss: 0.628360  [691200/859189]\n",
      "current batch loss: 0.696320  [697600/859189]\n",
      "current batch loss: 0.594816  [704000/859189]\n",
      "current batch loss: 0.695697  [710400/859189]\n",
      "current batch loss: 0.740527  [716800/859189]\n",
      "current batch loss: 0.645798  [723200/859189]\n",
      "current batch loss: 0.614525  [729600/859189]\n",
      "current batch loss: 0.645811  [736000/859189]\n",
      "current batch loss: 0.593468  [742400/859189]\n",
      "current batch loss: 0.674165  [748800/859189]\n",
      "current batch loss: 0.660225  [755200/859189]\n",
      "current batch loss: 0.661693  [761600/859189]\n",
      "current batch loss: 0.667283  [768000/859189]\n",
      "current batch loss: 0.667937  [774400/859189]\n",
      "current batch loss: 0.692160  [780800/859189]\n",
      "current batch loss: 0.647514  [787200/859189]\n",
      "current batch loss: 0.681814  [793600/859189]\n",
      "current batch loss: 0.655595  [800000/859189]\n",
      "current batch loss: 0.646487  [806400/859189]\n",
      "current batch loss: 0.711517  [812800/859189]\n",
      "current batch loss: 0.610267  [819200/859189]\n",
      "current batch loss: 0.623803  [825600/859189]\n",
      "current batch loss: 0.604387  [832000/859189]\n",
      "current batch loss: 0.605206  [838400/859189]\n",
      "current batch loss: 0.688432  [844800/859189]\n",
      "current batch loss: 0.692010  [851200/859189]\n",
      "current batch loss: 0.635472  [857600/859189]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.652586\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.651709\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 2\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.659864  [    0/859189]\n",
      "current batch loss: 0.669690  [ 6400/859189]\n",
      "current batch loss: 0.674080  [12800/859189]\n",
      "current batch loss: 0.705338  [19200/859189]\n",
      "current batch loss: 0.718550  [25600/859189]\n",
      "current batch loss: 0.674616  [32000/859189]\n",
      "current batch loss: 0.648827  [38400/859189]\n",
      "current batch loss: 0.630713  [44800/859189]\n",
      "current batch loss: 0.660284  [51200/859189]\n",
      "current batch loss: 0.707080  [57600/859189]\n",
      "current batch loss: 0.633878  [64000/859189]\n",
      "current batch loss: 0.686518  [70400/859189]\n",
      "current batch loss: 0.628435  [76800/859189]\n",
      "current batch loss: 0.669408  [83200/859189]\n",
      "current batch loss: 0.628314  [89600/859189]\n",
      "current batch loss: 0.697391  [96000/859189]\n",
      "current batch loss: 0.655159  [102400/859189]\n",
      "current batch loss: 0.648566  [108800/859189]\n",
      "current batch loss: 0.599475  [115200/859189]\n",
      "current batch loss: 0.636991  [121600/859189]\n",
      "current batch loss: 0.698482  [128000/859189]\n",
      "current batch loss: 0.701541  [134400/859189]\n",
      "current batch loss: 0.724545  [140800/859189]\n",
      "current batch loss: 0.737296  [147200/859189]\n",
      "current batch loss: 0.705817  [153600/859189]\n",
      "current batch loss: 0.637512  [160000/859189]\n",
      "current batch loss: 0.592855  [166400/859189]\n",
      "current batch loss: 0.679756  [172800/859189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.661018  [179200/859189]\n",
      "current batch loss: 0.593712  [185600/859189]\n",
      "current batch loss: 0.754460  [192000/859189]\n",
      "current batch loss: 0.689322  [198400/859189]\n",
      "current batch loss: 0.651354  [204800/859189]\n",
      "current batch loss: 0.604223  [211200/859189]\n",
      "current batch loss: 0.631398  [217600/859189]\n",
      "current batch loss: 0.638226  [224000/859189]\n",
      "current batch loss: 0.693133  [230400/859189]\n",
      "current batch loss: 0.592211  [236800/859189]\n",
      "current batch loss: 0.637351  [243200/859189]\n",
      "current batch loss: 0.658600  [249600/859189]\n",
      "current batch loss: 0.627523  [256000/859189]\n",
      "current batch loss: 0.662118  [262400/859189]\n",
      "current batch loss: 0.614945  [268800/859189]\n",
      "current batch loss: 0.575369  [275200/859189]\n",
      "current batch loss: 0.739450  [281600/859189]\n",
      "current batch loss: 0.643759  [288000/859189]\n",
      "current batch loss: 0.647126  [294400/859189]\n",
      "current batch loss: 0.599073  [300800/859189]\n",
      "current batch loss: 0.671400  [307200/859189]\n",
      "current batch loss: 0.622052  [313600/859189]\n",
      "current batch loss: 0.624563  [320000/859189]\n",
      "current batch loss: 0.669139  [326400/859189]\n",
      "current batch loss: 0.672717  [332800/859189]\n",
      "current batch loss: 0.607468  [339200/859189]\n",
      "current batch loss: 0.658473  [345600/859189]\n",
      "current batch loss: 0.659754  [352000/859189]\n",
      "current batch loss: 0.673685  [358400/859189]\n",
      "current batch loss: 0.623218  [364800/859189]\n",
      "current batch loss: 0.618410  [371200/859189]\n",
      "current batch loss: 0.616186  [377600/859189]\n",
      "current batch loss: 0.694742  [384000/859189]\n",
      "current batch loss: 0.616173  [390400/859189]\n",
      "current batch loss: 0.620880  [396800/859189]\n",
      "current batch loss: 0.635474  [403200/859189]\n",
      "current batch loss: 0.670799  [409600/859189]\n",
      "current batch loss: 0.614658  [416000/859189]\n",
      "current batch loss: 0.624870  [422400/859189]\n",
      "current batch loss: 0.639731  [428800/859189]\n",
      "current batch loss: 0.570243  [435200/859189]\n",
      "current batch loss: 0.628575  [441600/859189]\n",
      "current batch loss: 0.697233  [448000/859189]\n",
      "current batch loss: 0.693236  [454400/859189]\n",
      "current batch loss: 0.699278  [460800/859189]\n",
      "current batch loss: 0.653462  [467200/859189]\n",
      "current batch loss: 0.660759  [473600/859189]\n",
      "current batch loss: 0.630011  [480000/859189]\n",
      "current batch loss: 0.643418  [486400/859189]\n",
      "current batch loss: 0.660889  [492800/859189]\n",
      "current batch loss: 0.664647  [499200/859189]\n",
      "current batch loss: 0.701243  [505600/859189]\n",
      "current batch loss: 0.670809  [512000/859189]\n",
      "current batch loss: 0.639002  [518400/859189]\n",
      "current batch loss: 0.643666  [524800/859189]\n",
      "current batch loss: 0.625173  [531200/859189]\n",
      "current batch loss: 0.645180  [537600/859189]\n",
      "current batch loss: 0.668487  [544000/859189]\n",
      "current batch loss: 0.620277  [550400/859189]\n",
      "current batch loss: 0.626596  [556800/859189]\n",
      "current batch loss: 0.661509  [563200/859189]\n",
      "current batch loss: 0.640621  [569600/859189]\n",
      "current batch loss: 0.683427  [576000/859189]\n",
      "current batch loss: 0.671871  [582400/859189]\n",
      "current batch loss: 0.644114  [588800/859189]\n",
      "current batch loss: 0.655151  [595200/859189]\n",
      "current batch loss: 0.637708  [601600/859189]\n",
      "current batch loss: 0.704252  [608000/859189]\n",
      "current batch loss: 0.704298  [614400/859189]\n",
      "current batch loss: 0.586441  [620800/859189]\n",
      "current batch loss: 0.649246  [627200/859189]\n",
      "current batch loss: 0.657727  [633600/859189]\n",
      "current batch loss: 0.646310  [640000/859189]\n",
      "current batch loss: 0.589688  [646400/859189]\n",
      "current batch loss: 0.683614  [652800/859189]\n",
      "current batch loss: 0.651166  [659200/859189]\n",
      "current batch loss: 0.634819  [665600/859189]\n",
      "current batch loss: 0.630870  [672000/859189]\n",
      "current batch loss: 0.663999  [678400/859189]\n",
      "current batch loss: 0.672417  [684800/859189]\n",
      "current batch loss: 0.641040  [691200/859189]\n",
      "current batch loss: 0.647341  [697600/859189]\n",
      "current batch loss: 0.687711  [704000/859189]\n",
      "current batch loss: 0.681546  [710400/859189]\n",
      "current batch loss: 0.698174  [716800/859189]\n",
      "current batch loss: 0.613868  [723200/859189]\n",
      "current batch loss: 0.651040  [729600/859189]\n",
      "current batch loss: 0.654219  [736000/859189]\n",
      "current batch loss: 0.679005  [742400/859189]\n",
      "current batch loss: 0.623498  [748800/859189]\n",
      "current batch loss: 0.701425  [755200/859189]\n",
      "current batch loss: 0.607547  [761600/859189]\n",
      "current batch loss: 0.675316  [768000/859189]\n",
      "current batch loss: 0.588103  [774400/859189]\n",
      "current batch loss: 0.627186  [780800/859189]\n",
      "current batch loss: 0.692858  [787200/859189]\n",
      "current batch loss: 0.692591  [793600/859189]\n",
      "current batch loss: 0.609683  [800000/859189]\n",
      "current batch loss: 0.638131  [806400/859189]\n",
      "current batch loss: 0.636499  [812800/859189]\n",
      "current batch loss: 0.598983  [819200/859189]\n",
      "current batch loss: 0.593597  [825600/859189]\n",
      "current batch loss: 0.670889  [832000/859189]\n",
      "current batch loss: 0.581516  [838400/859189]\n",
      "current batch loss: 0.662556  [844800/859189]\n",
      "current batch loss: 0.591544  [851200/859189]\n",
      "current batch loss: 0.689597  [857600/859189]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.651579\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.650955\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 3\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.619838  [    0/859189]\n",
      "current batch loss: 0.641938  [ 6400/859189]\n",
      "current batch loss: 0.643885  [12800/859189]\n",
      "current batch loss: 0.658846  [19200/859189]\n",
      "current batch loss: 0.687362  [25600/859189]\n",
      "current batch loss: 0.672577  [32000/859189]\n",
      "current batch loss: 0.661065  [38400/859189]\n",
      "current batch loss: 0.623946  [44800/859189]\n",
      "current batch loss: 0.627837  [51200/859189]\n",
      "current batch loss: 0.550711  [57600/859189]\n",
      "current batch loss: 0.656827  [64000/859189]\n",
      "current batch loss: 0.675677  [70400/859189]\n",
      "current batch loss: 0.649292  [76800/859189]\n",
      "current batch loss: 0.691709  [83200/859189]\n",
      "current batch loss: 0.701555  [89600/859189]\n",
      "current batch loss: 0.658482  [96000/859189]\n",
      "current batch loss: 0.660530  [102400/859189]\n",
      "current batch loss: 0.685457  [108800/859189]\n",
      "current batch loss: 0.621517  [115200/859189]\n",
      "current batch loss: 0.657317  [121600/859189]\n",
      "current batch loss: 0.664040  [128000/859189]\n",
      "current batch loss: 0.604291  [134400/859189]\n",
      "current batch loss: 0.625584  [140800/859189]\n",
      "current batch loss: 0.621106  [147200/859189]\n",
      "current batch loss: 0.675343  [153600/859189]\n",
      "current batch loss: 0.639934  [160000/859189]\n",
      "current batch loss: 0.674580  [166400/859189]\n",
      "current batch loss: 0.617051  [172800/859189]\n",
      "current batch loss: 0.624360  [179200/859189]\n",
      "current batch loss: 0.642347  [185600/859189]\n",
      "current batch loss: 0.657692  [192000/859189]\n",
      "current batch loss: 0.728660  [198400/859189]\n",
      "current batch loss: 0.677398  [204800/859189]\n",
      "current batch loss: 0.630898  [211200/859189]\n",
      "current batch loss: 0.639521  [217600/859189]\n",
      "current batch loss: 0.654895  [224000/859189]\n",
      "current batch loss: 0.731956  [230400/859189]\n",
      "current batch loss: 0.621873  [236800/859189]\n",
      "current batch loss: 0.677948  [243200/859189]\n",
      "current batch loss: 0.711518  [249600/859189]\n",
      "current batch loss: 0.639558  [256000/859189]\n",
      "current batch loss: 0.683898  [262400/859189]\n",
      "current batch loss: 0.609181  [268800/859189]\n",
      "current batch loss: 0.649157  [275200/859189]\n",
      "current batch loss: 0.650185  [281600/859189]\n",
      "current batch loss: 0.680240  [288000/859189]\n",
      "current batch loss: 0.682466  [294400/859189]\n",
      "current batch loss: 0.669825  [300800/859189]\n",
      "current batch loss: 0.640074  [307200/859189]\n",
      "current batch loss: 0.647751  [313600/859189]\n",
      "current batch loss: 0.630682  [320000/859189]\n",
      "current batch loss: 0.688518  [326400/859189]\n",
      "current batch loss: 0.660747  [332800/859189]\n",
      "current batch loss: 0.598139  [339200/859189]\n",
      "current batch loss: 0.670189  [345600/859189]\n",
      "current batch loss: 0.719948  [352000/859189]\n",
      "current batch loss: 0.675625  [358400/859189]\n",
      "current batch loss: 0.670673  [364800/859189]\n",
      "current batch loss: 0.612834  [371200/859189]\n",
      "current batch loss: 0.682988  [377600/859189]\n",
      "current batch loss: 0.691579  [384000/859189]\n",
      "current batch loss: 0.664147  [390400/859189]\n",
      "current batch loss: 0.679130  [396800/859189]\n",
      "current batch loss: 0.617054  [403200/859189]\n",
      "current batch loss: 0.745344  [409600/859189]\n",
      "current batch loss: 0.651065  [416000/859189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.598076  [422400/859189]\n",
      "current batch loss: 0.696429  [428800/859189]\n",
      "current batch loss: 0.627629  [435200/859189]\n",
      "current batch loss: 0.618278  [441600/859189]\n",
      "current batch loss: 0.618109  [448000/859189]\n",
      "current batch loss: 0.627683  [454400/859189]\n",
      "current batch loss: 0.623711  [460800/859189]\n",
      "current batch loss: 0.614366  [467200/859189]\n",
      "current batch loss: 0.624000  [473600/859189]\n",
      "current batch loss: 0.662182  [480000/859189]\n",
      "current batch loss: 0.663195  [486400/859189]\n",
      "current batch loss: 0.654448  [492800/859189]\n",
      "current batch loss: 0.668126  [499200/859189]\n",
      "current batch loss: 0.708343  [505600/859189]\n",
      "current batch loss: 0.639751  [512000/859189]\n",
      "current batch loss: 0.632477  [518400/859189]\n",
      "current batch loss: 0.706379  [524800/859189]\n",
      "current batch loss: 0.676077  [531200/859189]\n",
      "current batch loss: 0.678666  [537600/859189]\n",
      "current batch loss: 0.625320  [544000/859189]\n",
      "current batch loss: 0.641415  [550400/859189]\n",
      "current batch loss: 0.658851  [556800/859189]\n",
      "current batch loss: 0.695494  [563200/859189]\n",
      "current batch loss: 0.683589  [569600/859189]\n",
      "current batch loss: 0.650368  [576000/859189]\n",
      "current batch loss: 0.661432  [582400/859189]\n",
      "current batch loss: 0.710443  [588800/859189]\n",
      "current batch loss: 0.665351  [595200/859189]\n",
      "current batch loss: 0.656006  [601600/859189]\n",
      "current batch loss: 0.693729  [608000/859189]\n",
      "current batch loss: 0.622601  [614400/859189]\n",
      "current batch loss: 0.634592  [620800/859189]\n",
      "current batch loss: 0.624108  [627200/859189]\n",
      "current batch loss: 0.634789  [633600/859189]\n",
      "current batch loss: 0.648508  [640000/859189]\n",
      "current batch loss: 0.626471  [646400/859189]\n",
      "current batch loss: 0.601837  [652800/859189]\n",
      "current batch loss: 0.673236  [659200/859189]\n",
      "current batch loss: 0.652232  [665600/859189]\n",
      "current batch loss: 0.671749  [672000/859189]\n",
      "current batch loss: 0.597739  [678400/859189]\n",
      "current batch loss: 0.554668  [684800/859189]\n",
      "current batch loss: 0.642772  [691200/859189]\n",
      "current batch loss: 0.636099  [697600/859189]\n",
      "current batch loss: 0.691725  [704000/859189]\n",
      "current batch loss: 0.637735  [710400/859189]\n",
      "current batch loss: 0.711965  [716800/859189]\n",
      "current batch loss: 0.658652  [723200/859189]\n",
      "current batch loss: 0.686626  [729600/859189]\n",
      "current batch loss: 0.655351  [736000/859189]\n",
      "current batch loss: 0.673542  [742400/859189]\n",
      "current batch loss: 0.653060  [748800/859189]\n",
      "current batch loss: 0.681850  [755200/859189]\n",
      "current batch loss: 0.663261  [761600/859189]\n",
      "current batch loss: 0.635301  [768000/859189]\n",
      "current batch loss: 0.687353  [774400/859189]\n",
      "current batch loss: 0.647187  [780800/859189]\n",
      "current batch loss: 0.677829  [787200/859189]\n",
      "current batch loss: 0.696332  [793600/859189]\n",
      "current batch loss: 0.655373  [800000/859189]\n",
      "current batch loss: 0.635077  [806400/859189]\n",
      "current batch loss: 0.644920  [812800/859189]\n",
      "current batch loss: 0.623584  [819200/859189]\n",
      "current batch loss: 0.630646  [825600/859189]\n",
      "current batch loss: 0.633020  [832000/859189]\n",
      "current batch loss: 0.607775  [838400/859189]\n",
      "current batch loss: 0.656305  [844800/859189]\n",
      "current batch loss: 0.547525  [851200/859189]\n",
      "current batch loss: 0.679276  [857600/859189]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.651067\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.650207\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 4\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.659036  [    0/859189]\n",
      "current batch loss: 0.714763  [ 6400/859189]\n",
      "current batch loss: 0.683874  [12800/859189]\n",
      "current batch loss: 0.618106  [19200/859189]\n",
      "current batch loss: 0.691665  [25600/859189]\n",
      "current batch loss: 0.626200  [32000/859189]\n",
      "current batch loss: 0.674585  [38400/859189]\n",
      "current batch loss: 0.623771  [44800/859189]\n",
      "current batch loss: 0.638006  [51200/859189]\n",
      "current batch loss: 0.713249  [57600/859189]\n",
      "current batch loss: 0.645057  [64000/859189]\n",
      "current batch loss: 0.611341  [70400/859189]\n",
      "current batch loss: 0.683286  [76800/859189]\n",
      "current batch loss: 0.594356  [83200/859189]\n",
      "current batch loss: 0.723498  [89600/859189]\n",
      "current batch loss: 0.672045  [96000/859189]\n",
      "current batch loss: 0.666051  [102400/859189]\n",
      "current batch loss: 0.638934  [108800/859189]\n",
      "current batch loss: 0.570975  [115200/859189]\n",
      "current batch loss: 0.591686  [121600/859189]\n",
      "current batch loss: 0.738121  [128000/859189]\n",
      "current batch loss: 0.628157  [134400/859189]\n",
      "current batch loss: 0.600857  [140800/859189]\n",
      "current batch loss: 0.613773  [147200/859189]\n",
      "current batch loss: 0.599545  [153600/859189]\n",
      "current batch loss: 0.628130  [160000/859189]\n",
      "current batch loss: 0.628994  [166400/859189]\n",
      "current batch loss: 0.696622  [172800/859189]\n",
      "current batch loss: 0.681279  [179200/859189]\n",
      "current batch loss: 0.592050  [185600/859189]\n",
      "current batch loss: 0.662158  [192000/859189]\n",
      "current batch loss: 0.695322  [198400/859189]\n",
      "current batch loss: 0.630323  [204800/859189]\n",
      "current batch loss: 0.639477  [211200/859189]\n",
      "current batch loss: 0.687820  [217600/859189]\n",
      "current batch loss: 0.574943  [224000/859189]\n",
      "current batch loss: 0.644543  [230400/859189]\n",
      "current batch loss: 0.669622  [236800/859189]\n",
      "current batch loss: 0.622857  [243200/859189]\n",
      "current batch loss: 0.614813  [249600/859189]\n",
      "current batch loss: 0.705478  [256000/859189]\n",
      "current batch loss: 0.583859  [262400/859189]\n",
      "current batch loss: 0.654731  [268800/859189]\n",
      "current batch loss: 0.543635  [275200/859189]\n",
      "current batch loss: 0.660642  [281600/859189]\n",
      "current batch loss: 0.683281  [288000/859189]\n",
      "current batch loss: 0.699140  [294400/859189]\n",
      "current batch loss: 0.655961  [300800/859189]\n",
      "current batch loss: 0.673435  [307200/859189]\n",
      "current batch loss: 0.703385  [313600/859189]\n",
      "current batch loss: 0.713187  [320000/859189]\n",
      "current batch loss: 0.628726  [326400/859189]\n",
      "current batch loss: 0.601316  [332800/859189]\n",
      "current batch loss: 0.723081  [339200/859189]\n",
      "current batch loss: 0.675010  [345600/859189]\n",
      "current batch loss: 0.626763  [352000/859189]\n",
      "current batch loss: 0.658965  [358400/859189]\n",
      "current batch loss: 0.606438  [364800/859189]\n",
      "current batch loss: 0.647416  [371200/859189]\n",
      "current batch loss: 0.669373  [377600/859189]\n",
      "current batch loss: 0.639881  [384000/859189]\n",
      "current batch loss: 0.622440  [390400/859189]\n",
      "current batch loss: 0.639697  [396800/859189]\n",
      "current batch loss: 0.699313  [403200/859189]\n",
      "current batch loss: 0.651031  [409600/859189]\n",
      "current batch loss: 0.642019  [416000/859189]\n",
      "current batch loss: 0.589557  [422400/859189]\n",
      "current batch loss: 0.676471  [428800/859189]\n",
      "current batch loss: 0.701782  [435200/859189]\n",
      "current batch loss: 0.598897  [441600/859189]\n",
      "current batch loss: 0.721909  [448000/859189]\n",
      "current batch loss: 0.623312  [454400/859189]\n",
      "current batch loss: 0.685632  [460800/859189]\n",
      "current batch loss: 0.629086  [467200/859189]\n",
      "current batch loss: 0.655152  [473600/859189]\n",
      "current batch loss: 0.668996  [480000/859189]\n",
      "current batch loss: 0.650111  [486400/859189]\n",
      "current batch loss: 0.705784  [492800/859189]\n",
      "current batch loss: 0.658192  [499200/859189]\n",
      "current batch loss: 0.601883  [505600/859189]\n",
      "current batch loss: 0.619341  [512000/859189]\n",
      "current batch loss: 0.641595  [518400/859189]\n",
      "current batch loss: 0.612482  [524800/859189]\n",
      "current batch loss: 0.657017  [531200/859189]\n",
      "current batch loss: 0.656777  [537600/859189]\n",
      "current batch loss: 0.713887  [544000/859189]\n",
      "current batch loss: 0.611556  [550400/859189]\n",
      "current batch loss: 0.648072  [556800/859189]\n",
      "current batch loss: 0.567705  [563200/859189]\n",
      "current batch loss: 0.644805  [569600/859189]\n",
      "current batch loss: 0.625333  [576000/859189]\n",
      "current batch loss: 0.670505  [582400/859189]\n",
      "current batch loss: 0.658565  [588800/859189]\n",
      "current batch loss: 0.677449  [595200/859189]\n",
      "current batch loss: 0.695366  [601600/859189]\n",
      "current batch loss: 0.681852  [608000/859189]\n",
      "current batch loss: 0.629874  [614400/859189]\n",
      "current batch loss: 0.644878  [620800/859189]\n",
      "current batch loss: 0.633217  [627200/859189]\n",
      "current batch loss: 0.575203  [633600/859189]\n",
      "current batch loss: 0.652682  [640000/859189]\n",
      "current batch loss: 0.627901  [646400/859189]\n",
      "current batch loss: 0.638141  [652800/859189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.653063  [659200/859189]\n",
      "current batch loss: 0.627621  [665600/859189]\n",
      "current batch loss: 0.653965  [672000/859189]\n",
      "current batch loss: 0.593562  [678400/859189]\n",
      "current batch loss: 0.632030  [684800/859189]\n",
      "current batch loss: 0.621558  [691200/859189]\n",
      "current batch loss: 0.658025  [697600/859189]\n",
      "current batch loss: 0.609226  [704000/859189]\n",
      "current batch loss: 0.653391  [710400/859189]\n",
      "current batch loss: 0.704024  [716800/859189]\n",
      "current batch loss: 0.619264  [723200/859189]\n",
      "current batch loss: 0.637422  [729600/859189]\n",
      "current batch loss: 0.660591  [736000/859189]\n",
      "current batch loss: 0.629166  [742400/859189]\n",
      "current batch loss: 0.623532  [748800/859189]\n",
      "current batch loss: 0.652107  [755200/859189]\n",
      "current batch loss: 0.684021  [761600/859189]\n",
      "current batch loss: 0.657105  [768000/859189]\n",
      "current batch loss: 0.633847  [774400/859189]\n",
      "current batch loss: 0.648806  [780800/859189]\n",
      "current batch loss: 0.670494  [787200/859189]\n",
      "current batch loss: 0.652230  [793600/859189]\n",
      "current batch loss: 0.640832  [800000/859189]\n",
      "current batch loss: 0.647791  [806400/859189]\n",
      "current batch loss: 0.642645  [812800/859189]\n",
      "current batch loss: 0.682243  [819200/859189]\n",
      "current batch loss: 0.694950  [825600/859189]\n",
      "current batch loss: 0.656047  [832000/859189]\n",
      "current batch loss: 0.679491  [838400/859189]\n",
      "current batch loss: 0.701697  [844800/859189]\n",
      "current batch loss: 0.680992  [851200/859189]\n",
      "current batch loss: 0.618512  [857600/859189]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.650798\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.650151\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 5\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.687038  [    0/859189]\n",
      "current batch loss: 0.652767  [ 6400/859189]\n",
      "current batch loss: 0.657005  [12800/859189]\n",
      "current batch loss: 0.644818  [19200/859189]\n",
      "current batch loss: 0.646234  [25600/859189]\n",
      "current batch loss: 0.676980  [32000/859189]\n",
      "current batch loss: 0.674317  [38400/859189]\n",
      "current batch loss: 0.618144  [44800/859189]\n",
      "current batch loss: 0.620468  [51200/859189]\n",
      "current batch loss: 0.695810  [57600/859189]\n",
      "current batch loss: 0.628579  [64000/859189]\n",
      "current batch loss: 0.603249  [70400/859189]\n",
      "current batch loss: 0.632172  [76800/859189]\n",
      "current batch loss: 0.635364  [83200/859189]\n",
      "current batch loss: 0.640453  [89600/859189]\n",
      "current batch loss: 0.625938  [96000/859189]\n",
      "current batch loss: 0.730702  [102400/859189]\n",
      "current batch loss: 0.694193  [108800/859189]\n",
      "current batch loss: 0.657748  [115200/859189]\n",
      "current batch loss: 0.666967  [121600/859189]\n",
      "current batch loss: 0.662570  [128000/859189]\n",
      "current batch loss: 0.684146  [134400/859189]\n",
      "current batch loss: 0.610483  [140800/859189]\n",
      "current batch loss: 0.633530  [147200/859189]\n",
      "current batch loss: 0.654051  [153600/859189]\n",
      "current batch loss: 0.689880  [160000/859189]\n",
      "current batch loss: 0.637822  [166400/859189]\n",
      "current batch loss: 0.633659  [172800/859189]\n",
      "current batch loss: 0.655983  [179200/859189]\n",
      "current batch loss: 0.659639  [185600/859189]\n",
      "current batch loss: 0.695275  [192000/859189]\n",
      "current batch loss: 0.630511  [198400/859189]\n",
      "current batch loss: 0.627036  [204800/859189]\n",
      "current batch loss: 0.660424  [211200/859189]\n",
      "current batch loss: 0.691502  [217600/859189]\n",
      "current batch loss: 0.605383  [224000/859189]\n",
      "current batch loss: 0.603822  [230400/859189]\n",
      "current batch loss: 0.615240  [236800/859189]\n",
      "current batch loss: 0.573898  [243200/859189]\n",
      "current batch loss: 0.690671  [249600/859189]\n",
      "current batch loss: 0.574135  [256000/859189]\n",
      "current batch loss: 0.683054  [262400/859189]\n",
      "current batch loss: 0.588602  [268800/859189]\n",
      "current batch loss: 0.658802  [275200/859189]\n",
      "current batch loss: 0.684432  [281600/859189]\n",
      "current batch loss: 0.671013  [288000/859189]\n",
      "current batch loss: 0.679679  [294400/859189]\n",
      "current batch loss: 0.680812  [300800/859189]\n",
      "current batch loss: 0.625673  [307200/859189]\n",
      "current batch loss: 0.660227  [313600/859189]\n",
      "current batch loss: 0.669359  [320000/859189]\n",
      "current batch loss: 0.633920  [326400/859189]\n",
      "current batch loss: 0.664562  [332800/859189]\n",
      "current batch loss: 0.670080  [339200/859189]\n",
      "current batch loss: 0.615010  [345600/859189]\n",
      "current batch loss: 0.636001  [352000/859189]\n",
      "current batch loss: 0.669858  [358400/859189]\n",
      "current batch loss: 0.561454  [364800/859189]\n",
      "current batch loss: 0.650254  [371200/859189]\n",
      "current batch loss: 0.655771  [377600/859189]\n",
      "current batch loss: 0.765339  [384000/859189]\n",
      "current batch loss: 0.638380  [390400/859189]\n",
      "current batch loss: 0.674129  [396800/859189]\n",
      "current batch loss: 0.616970  [403200/859189]\n",
      "current batch loss: 0.626231  [409600/859189]\n",
      "current batch loss: 0.617181  [416000/859189]\n",
      "current batch loss: 0.631271  [422400/859189]\n",
      "current batch loss: 0.606979  [428800/859189]\n",
      "current batch loss: 0.659501  [435200/859189]\n",
      "current batch loss: 0.676051  [441600/859189]\n",
      "current batch loss: 0.639170  [448000/859189]\n",
      "current batch loss: 0.650328  [454400/859189]\n",
      "current batch loss: 0.687477  [460800/859189]\n",
      "current batch loss: 0.679643  [467200/859189]\n",
      "current batch loss: 0.631414  [473600/859189]\n",
      "current batch loss: 0.629625  [480000/859189]\n",
      "current batch loss: 0.697609  [486400/859189]\n",
      "current batch loss: 0.637314  [492800/859189]\n",
      "current batch loss: 0.632086  [499200/859189]\n",
      "current batch loss: 0.654850  [505600/859189]\n",
      "current batch loss: 0.599074  [512000/859189]\n",
      "current batch loss: 0.693121  [518400/859189]\n",
      "current batch loss: 0.600710  [524800/859189]\n",
      "current batch loss: 0.651755  [531200/859189]\n",
      "current batch loss: 0.672370  [537600/859189]\n",
      "current batch loss: 0.576589  [544000/859189]\n",
      "current batch loss: 0.597487  [550400/859189]\n",
      "current batch loss: 0.616987  [556800/859189]\n",
      "current batch loss: 0.658423  [563200/859189]\n",
      "current batch loss: 0.673379  [569600/859189]\n",
      "current batch loss: 0.664929  [576000/859189]\n",
      "current batch loss: 0.616594  [582400/859189]\n",
      "current batch loss: 0.635216  [588800/859189]\n",
      "current batch loss: 0.672164  [595200/859189]\n",
      "current batch loss: 0.663758  [601600/859189]\n",
      "current batch loss: 0.630784  [608000/859189]\n",
      "current batch loss: 0.690131  [614400/859189]\n",
      "current batch loss: 0.665866  [620800/859189]\n",
      "current batch loss: 0.754961  [627200/859189]\n",
      "current batch loss: 0.718649  [633600/859189]\n",
      "current batch loss: 0.613597  [640000/859189]\n",
      "current batch loss: 0.647682  [646400/859189]\n",
      "current batch loss: 0.640881  [652800/859189]\n",
      "current batch loss: 0.632205  [659200/859189]\n",
      "current batch loss: 0.678052  [665600/859189]\n",
      "current batch loss: 0.643052  [672000/859189]\n",
      "current batch loss: 0.654038  [678400/859189]\n",
      "current batch loss: 0.632054  [684800/859189]\n",
      "current batch loss: 0.692127  [691200/859189]\n",
      "current batch loss: 0.656096  [697600/859189]\n",
      "current batch loss: 0.658041  [704000/859189]\n",
      "current batch loss: 0.636954  [710400/859189]\n",
      "current batch loss: 0.693705  [716800/859189]\n",
      "current batch loss: 0.654019  [723200/859189]\n",
      "current batch loss: 0.658560  [729600/859189]\n",
      "current batch loss: 0.660936  [736000/859189]\n",
      "current batch loss: 0.640408  [742400/859189]\n",
      "current batch loss: 0.708111  [748800/859189]\n",
      "current batch loss: 0.661090  [755200/859189]\n",
      "current batch loss: 0.646195  [761600/859189]\n",
      "current batch loss: 0.709332  [768000/859189]\n",
      "current batch loss: 0.625717  [774400/859189]\n",
      "current batch loss: 0.606825  [780800/859189]\n",
      "current batch loss: 0.647519  [787200/859189]\n",
      "current batch loss: 0.575005  [793600/859189]\n",
      "current batch loss: 0.631291  [800000/859189]\n",
      "current batch loss: 0.604964  [806400/859189]\n",
      "current batch loss: 0.642189  [812800/859189]\n",
      "current batch loss: 0.647441  [819200/859189]\n",
      "current batch loss: 0.645859  [825600/859189]\n",
      "current batch loss: 0.664822  [832000/859189]\n",
      "current batch loss: 0.654339  [838400/859189]\n",
      "current batch loss: 0.651450  [844800/859189]\n",
      "current batch loss: 0.752019  [851200/859189]\n",
      "current batch loss: 0.638131  [857600/859189]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.650256\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.649605\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 6\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.650638  [    0/859189]\n",
      "current batch loss: 0.709063  [ 6400/859189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.679781  [12800/859189]\n",
      "current batch loss: 0.661520  [19200/859189]\n",
      "current batch loss: 0.633860  [25600/859189]\n",
      "current batch loss: 0.660506  [32000/859189]\n",
      "current batch loss: 0.618847  [38400/859189]\n",
      "current batch loss: 0.669307  [44800/859189]\n",
      "current batch loss: 0.612893  [51200/859189]\n",
      "current batch loss: 0.632969  [57600/859189]\n",
      "current batch loss: 0.722240  [64000/859189]\n",
      "current batch loss: 0.658347  [70400/859189]\n",
      "current batch loss: 0.646259  [76800/859189]\n",
      "current batch loss: 0.651750  [83200/859189]\n",
      "current batch loss: 0.687510  [89600/859189]\n",
      "current batch loss: 0.635639  [96000/859189]\n",
      "current batch loss: 0.654761  [102400/859189]\n",
      "current batch loss: 0.696777  [108800/859189]\n",
      "current batch loss: 0.632540  [115200/859189]\n",
      "current batch loss: 0.629003  [121600/859189]\n",
      "current batch loss: 0.661050  [128000/859189]\n",
      "current batch loss: 0.720799  [134400/859189]\n",
      "current batch loss: 0.660418  [140800/859189]\n",
      "current batch loss: 0.625629  [147200/859189]\n",
      "current batch loss: 0.577466  [153600/859189]\n",
      "current batch loss: 0.637714  [160000/859189]\n",
      "current batch loss: 0.660467  [166400/859189]\n",
      "current batch loss: 0.703025  [172800/859189]\n",
      "current batch loss: 0.629954  [179200/859189]\n",
      "current batch loss: 0.633260  [185600/859189]\n",
      "current batch loss: 0.617378  [192000/859189]\n",
      "current batch loss: 0.692435  [198400/859189]\n",
      "current batch loss: 0.626072  [204800/859189]\n",
      "current batch loss: 0.661201  [211200/859189]\n",
      "current batch loss: 0.650405  [217600/859189]\n",
      "current batch loss: 0.702553  [224000/859189]\n",
      "current batch loss: 0.627209  [230400/859189]\n",
      "current batch loss: 0.635772  [236800/859189]\n",
      "current batch loss: 0.654858  [243200/859189]\n",
      "current batch loss: 0.630581  [249600/859189]\n",
      "current batch loss: 0.678606  [256000/859189]\n",
      "current batch loss: 0.640234  [262400/859189]\n",
      "current batch loss: 0.633263  [268800/859189]\n",
      "current batch loss: 0.616506  [275200/859189]\n",
      "current batch loss: 0.718418  [281600/859189]\n",
      "current batch loss: 0.644003  [288000/859189]\n",
      "current batch loss: 0.703391  [294400/859189]\n",
      "current batch loss: 0.695491  [300800/859189]\n",
      "current batch loss: 0.617398  [307200/859189]\n",
      "current batch loss: 0.603972  [313600/859189]\n",
      "current batch loss: 0.610655  [320000/859189]\n",
      "current batch loss: 0.653063  [326400/859189]\n",
      "current batch loss: 0.668752  [332800/859189]\n",
      "current batch loss: 0.597267  [339200/859189]\n",
      "current batch loss: 0.622536  [345600/859189]\n",
      "current batch loss: 0.651386  [352000/859189]\n",
      "current batch loss: 0.662726  [358400/859189]\n",
      "current batch loss: 0.661974  [364800/859189]\n",
      "current batch loss: 0.615215  [371200/859189]\n",
      "current batch loss: 0.671054  [377600/859189]\n",
      "current batch loss: 0.650155  [384000/859189]\n",
      "current batch loss: 0.593689  [390400/859189]\n",
      "current batch loss: 0.649284  [396800/859189]\n",
      "current batch loss: 0.630518  [403200/859189]\n",
      "current batch loss: 0.647513  [409600/859189]\n",
      "current batch loss: 0.641731  [416000/859189]\n",
      "current batch loss: 0.681356  [422400/859189]\n",
      "current batch loss: 0.581010  [428800/859189]\n",
      "current batch loss: 0.670683  [435200/859189]\n",
      "current batch loss: 0.631538  [441600/859189]\n",
      "current batch loss: 0.615720  [448000/859189]\n",
      "current batch loss: 0.653708  [454400/859189]\n",
      "current batch loss: 0.615705  [460800/859189]\n",
      "current batch loss: 0.657746  [467200/859189]\n",
      "current batch loss: 0.676851  [473600/859189]\n",
      "current batch loss: 0.667801  [480000/859189]\n",
      "current batch loss: 0.640511  [486400/859189]\n",
      "current batch loss: 0.645192  [492800/859189]\n",
      "current batch loss: 0.634442  [499200/859189]\n",
      "current batch loss: 0.652846  [505600/859189]\n",
      "current batch loss: 0.629352  [512000/859189]\n",
      "current batch loss: 0.644802  [518400/859189]\n",
      "current batch loss: 0.682336  [524800/859189]\n",
      "current batch loss: 0.602228  [531200/859189]\n",
      "current batch loss: 0.638341  [537600/859189]\n",
      "current batch loss: 0.658040  [544000/859189]\n",
      "current batch loss: 0.721180  [550400/859189]\n",
      "current batch loss: 0.605841  [556800/859189]\n",
      "current batch loss: 0.741535  [563200/859189]\n",
      "current batch loss: 0.663873  [569600/859189]\n",
      "current batch loss: 0.633875  [576000/859189]\n",
      "current batch loss: 0.696760  [582400/859189]\n",
      "current batch loss: 0.635046  [588800/859189]\n",
      "current batch loss: 0.613266  [595200/859189]\n",
      "current batch loss: 0.627581  [601600/859189]\n",
      "current batch loss: 0.630305  [608000/859189]\n",
      "current batch loss: 0.673893  [614400/859189]\n",
      "current batch loss: 0.671957  [620800/859189]\n",
      "current batch loss: 0.620782  [627200/859189]\n",
      "current batch loss: 0.689588  [633600/859189]\n",
      "current batch loss: 0.626439  [640000/859189]\n",
      "current batch loss: 0.651924  [646400/859189]\n",
      "current batch loss: 0.615600  [652800/859189]\n",
      "current batch loss: 0.633668  [659200/859189]\n",
      "current batch loss: 0.615105  [665600/859189]\n",
      "current batch loss: 0.682006  [672000/859189]\n",
      "current batch loss: 0.646830  [678400/859189]\n",
      "current batch loss: 0.663053  [684800/859189]\n",
      "current batch loss: 0.599681  [691200/859189]\n",
      "current batch loss: 0.694070  [697600/859189]\n",
      "current batch loss: 0.669097  [704000/859189]\n",
      "current batch loss: 0.670895  [710400/859189]\n",
      "current batch loss: 0.638190  [716800/859189]\n",
      "current batch loss: 0.598544  [723200/859189]\n",
      "current batch loss: 0.681048  [729600/859189]\n",
      "current batch loss: 0.685235  [736000/859189]\n",
      "current batch loss: 0.640974  [742400/859189]\n",
      "current batch loss: 0.683880  [748800/859189]\n",
      "current batch loss: 0.617602  [755200/859189]\n",
      "current batch loss: 0.655882  [761600/859189]\n",
      "current batch loss: 0.659657  [768000/859189]\n",
      "current batch loss: 0.662449  [774400/859189]\n",
      "current batch loss: 0.647966  [780800/859189]\n",
      "current batch loss: 0.543217  [787200/859189]\n",
      "current batch loss: 0.657319  [793600/859189]\n",
      "current batch loss: 0.581758  [800000/859189]\n",
      "current batch loss: 0.586159  [806400/859189]\n",
      "current batch loss: 0.667203  [812800/859189]\n",
      "current batch loss: 0.689826  [819200/859189]\n",
      "current batch loss: 0.650062  [825600/859189]\n",
      "current batch loss: 0.591518  [832000/859189]\n",
      "current batch loss: 0.644316  [838400/859189]\n",
      "current batch loss: 0.655120  [844800/859189]\n",
      "current batch loss: 0.648348  [851200/859189]\n",
      "current batch loss: 0.615403  [857600/859189]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.650220\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.649734\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 7\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.646612  [    0/859189]\n",
      "current batch loss: 0.611119  [ 6400/859189]\n",
      "current batch loss: 0.616069  [12800/859189]\n",
      "current batch loss: 0.600402  [19200/859189]\n",
      "current batch loss: 0.624387  [25600/859189]\n",
      "current batch loss: 0.606418  [32000/859189]\n",
      "current batch loss: 0.626370  [38400/859189]\n",
      "current batch loss: 0.629166  [44800/859189]\n",
      "current batch loss: 0.668014  [51200/859189]\n",
      "current batch loss: 0.632889  [57600/859189]\n",
      "current batch loss: 0.619133  [64000/859189]\n",
      "current batch loss: 0.600103  [70400/859189]\n",
      "current batch loss: 0.661406  [76800/859189]\n",
      "current batch loss: 0.569935  [83200/859189]\n",
      "current batch loss: 0.607289  [89600/859189]\n",
      "current batch loss: 0.621607  [96000/859189]\n",
      "current batch loss: 0.677254  [102400/859189]\n",
      "current batch loss: 0.638325  [108800/859189]\n",
      "current batch loss: 0.693895  [115200/859189]\n",
      "current batch loss: 0.646918  [121600/859189]\n",
      "current batch loss: 0.656002  [128000/859189]\n",
      "current batch loss: 0.669518  [134400/859189]\n",
      "current batch loss: 0.691437  [140800/859189]\n",
      "current batch loss: 0.674648  [147200/859189]\n",
      "current batch loss: 0.683424  [153600/859189]\n",
      "current batch loss: 0.657508  [160000/859189]\n",
      "current batch loss: 0.598237  [166400/859189]\n",
      "current batch loss: 0.597076  [172800/859189]\n",
      "current batch loss: 0.668558  [179200/859189]\n",
      "current batch loss: 0.628789  [185600/859189]\n",
      "current batch loss: 0.625981  [192000/859189]\n",
      "current batch loss: 0.626218  [198400/859189]\n",
      "current batch loss: 0.619936  [204800/859189]\n",
      "current batch loss: 0.657659  [211200/859189]\n",
      "current batch loss: 0.704442  [217600/859189]\n",
      "current batch loss: 0.565206  [224000/859189]\n",
      "current batch loss: 0.623933  [230400/859189]\n",
      "current batch loss: 0.659377  [236800/859189]\n",
      "current batch loss: 0.677398  [243200/859189]\n",
      "current batch loss: 0.621338  [249600/859189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.638950  [256000/859189]\n",
      "current batch loss: 0.682486  [262400/859189]\n",
      "current batch loss: 0.620352  [268800/859189]\n",
      "current batch loss: 0.649982  [275200/859189]\n",
      "current batch loss: 0.689718  [281600/859189]\n",
      "current batch loss: 0.688180  [288000/859189]\n",
      "current batch loss: 0.676195  [294400/859189]\n",
      "current batch loss: 0.670494  [300800/859189]\n",
      "current batch loss: 0.687761  [307200/859189]\n",
      "current batch loss: 0.636803  [313600/859189]\n",
      "current batch loss: 0.720385  [320000/859189]\n",
      "current batch loss: 0.732815  [326400/859189]\n",
      "current batch loss: 0.704986  [332800/859189]\n",
      "current batch loss: 0.671591  [339200/859189]\n",
      "current batch loss: 0.674982  [345600/859189]\n",
      "current batch loss: 0.642600  [352000/859189]\n",
      "current batch loss: 0.637387  [358400/859189]\n",
      "current batch loss: 0.700284  [364800/859189]\n",
      "current batch loss: 0.624117  [371200/859189]\n",
      "current batch loss: 0.646808  [377600/859189]\n",
      "current batch loss: 0.645611  [384000/859189]\n",
      "current batch loss: 0.691598  [390400/859189]\n",
      "current batch loss: 0.600280  [396800/859189]\n",
      "current batch loss: 0.584223  [403200/859189]\n",
      "current batch loss: 0.613975  [409600/859189]\n",
      "current batch loss: 0.648369  [416000/859189]\n",
      "current batch loss: 0.607077  [422400/859189]\n",
      "current batch loss: 0.679536  [428800/859189]\n",
      "current batch loss: 0.646148  [435200/859189]\n",
      "current batch loss: 0.655935  [441600/859189]\n",
      "current batch loss: 0.742097  [448000/859189]\n",
      "current batch loss: 0.656998  [454400/859189]\n",
      "current batch loss: 0.690694  [460800/859189]\n",
      "current batch loss: 0.625293  [467200/859189]\n",
      "current batch loss: 0.653879  [473600/859189]\n",
      "current batch loss: 0.611488  [480000/859189]\n",
      "current batch loss: 0.687978  [486400/859189]\n",
      "current batch loss: 0.627317  [492800/859189]\n",
      "current batch loss: 0.651017  [499200/859189]\n",
      "current batch loss: 0.755625  [505600/859189]\n",
      "current batch loss: 0.628763  [512000/859189]\n",
      "current batch loss: 0.588563  [518400/859189]\n",
      "current batch loss: 0.652090  [524800/859189]\n",
      "current batch loss: 0.704639  [531200/859189]\n",
      "current batch loss: 0.603037  [537600/859189]\n",
      "current batch loss: 0.676868  [544000/859189]\n",
      "current batch loss: 0.602822  [550400/859189]\n",
      "current batch loss: 0.647516  [556800/859189]\n",
      "current batch loss: 0.663291  [563200/859189]\n",
      "current batch loss: 0.675749  [569600/859189]\n",
      "current batch loss: 0.673914  [576000/859189]\n",
      "current batch loss: 0.700911  [582400/859189]\n",
      "current batch loss: 0.720868  [588800/859189]\n",
      "current batch loss: 0.594845  [595200/859189]\n",
      "current batch loss: 0.685399  [601600/859189]\n",
      "current batch loss: 0.693525  [608000/859189]\n",
      "current batch loss: 0.691323  [614400/859189]\n",
      "current batch loss: 0.655026  [620800/859189]\n",
      "current batch loss: 0.679833  [627200/859189]\n",
      "current batch loss: 0.744742  [633600/859189]\n",
      "current batch loss: 0.671671  [640000/859189]\n",
      "current batch loss: 0.631830  [646400/859189]\n",
      "current batch loss: 0.705711  [652800/859189]\n",
      "current batch loss: 0.562736  [659200/859189]\n",
      "current batch loss: 0.684845  [665600/859189]\n",
      "current batch loss: 0.616061  [672000/859189]\n",
      "current batch loss: 0.638068  [678400/859189]\n",
      "current batch loss: 0.573462  [684800/859189]\n",
      "current batch loss: 0.669236  [691200/859189]\n",
      "current batch loss: 0.623763  [697600/859189]\n",
      "current batch loss: 0.684795  [704000/859189]\n",
      "current batch loss: 0.626534  [710400/859189]\n",
      "current batch loss: 0.627901  [716800/859189]\n",
      "current batch loss: 0.605841  [723200/859189]\n",
      "current batch loss: 0.638208  [729600/859189]\n",
      "current batch loss: 0.666859  [736000/859189]\n",
      "current batch loss: 0.621722  [742400/859189]\n",
      "current batch loss: 0.647249  [748800/859189]\n",
      "current batch loss: 0.737924  [755200/859189]\n",
      "current batch loss: 0.624573  [761600/859189]\n",
      "current batch loss: 0.691253  [768000/859189]\n",
      "current batch loss: 0.669959  [774400/859189]\n",
      "current batch loss: 0.610472  [780800/859189]\n",
      "current batch loss: 0.660192  [787200/859189]\n",
      "current batch loss: 0.768934  [793600/859189]\n",
      "current batch loss: 0.638684  [800000/859189]\n",
      "current batch loss: 0.710601  [806400/859189]\n",
      "current batch loss: 0.653845  [812800/859189]\n",
      "current batch loss: 0.675657  [819200/859189]\n",
      "current batch loss: 0.618714  [825600/859189]\n",
      "current batch loss: 0.692265  [832000/859189]\n",
      "current batch loss: 0.646910  [838400/859189]\n",
      "current batch loss: 0.673389  [844800/859189]\n",
      "current batch loss: 0.616475  [851200/859189]\n",
      "current batch loss: 0.666916  [857600/859189]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.650303\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.649795\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 8\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.614672  [    0/859189]\n",
      "current batch loss: 0.628830  [ 6400/859189]\n",
      "current batch loss: 0.643282  [12800/859189]\n",
      "current batch loss: 0.637197  [19200/859189]\n",
      "current batch loss: 0.654123  [25600/859189]\n",
      "current batch loss: 0.620547  [32000/859189]\n",
      "current batch loss: 0.636980  [38400/859189]\n",
      "current batch loss: 0.607326  [44800/859189]\n",
      "current batch loss: 0.683803  [51200/859189]\n",
      "current batch loss: 0.623415  [57600/859189]\n",
      "current batch loss: 0.637142  [64000/859189]\n",
      "current batch loss: 0.638684  [70400/859189]\n",
      "current batch loss: 0.611820  [76800/859189]\n",
      "current batch loss: 0.659018  [83200/859189]\n",
      "current batch loss: 0.704123  [89600/859189]\n",
      "current batch loss: 0.708645  [96000/859189]\n",
      "current batch loss: 0.635775  [102400/859189]\n",
      "current batch loss: 0.606512  [108800/859189]\n",
      "current batch loss: 0.693666  [115200/859189]\n",
      "current batch loss: 0.666988  [121600/859189]\n",
      "current batch loss: 0.680816  [128000/859189]\n",
      "current batch loss: 0.674394  [134400/859189]\n",
      "current batch loss: 0.716282  [140800/859189]\n",
      "current batch loss: 0.635373  [147200/859189]\n",
      "current batch loss: 0.672008  [153600/859189]\n",
      "current batch loss: 0.658352  [160000/859189]\n",
      "current batch loss: 0.650403  [166400/859189]\n",
      "current batch loss: 0.676969  [172800/859189]\n",
      "current batch loss: 0.700067  [179200/859189]\n",
      "current batch loss: 0.720988  [185600/859189]\n",
      "current batch loss: 0.600879  [192000/859189]\n",
      "current batch loss: 0.674118  [198400/859189]\n",
      "current batch loss: 0.639080  [204800/859189]\n",
      "current batch loss: 0.596222  [211200/859189]\n",
      "current batch loss: 0.622752  [217600/859189]\n",
      "current batch loss: 0.668236  [224000/859189]\n",
      "current batch loss: 0.600384  [230400/859189]\n",
      "current batch loss: 0.635513  [236800/859189]\n",
      "current batch loss: 0.704792  [243200/859189]\n",
      "current batch loss: 0.655865  [249600/859189]\n",
      "current batch loss: 0.650870  [256000/859189]\n",
      "current batch loss: 0.709561  [262400/859189]\n",
      "current batch loss: 0.630001  [268800/859189]\n",
      "current batch loss: 0.592024  [275200/859189]\n",
      "current batch loss: 0.584900  [281600/859189]\n",
      "current batch loss: 0.631844  [288000/859189]\n",
      "current batch loss: 0.644864  [294400/859189]\n",
      "current batch loss: 0.655394  [300800/859189]\n",
      "current batch loss: 0.637152  [307200/859189]\n",
      "current batch loss: 0.638157  [313600/859189]\n",
      "current batch loss: 0.593879  [320000/859189]\n",
      "current batch loss: 0.638633  [326400/859189]\n",
      "current batch loss: 0.656427  [332800/859189]\n",
      "current batch loss: 0.647868  [339200/859189]\n",
      "current batch loss: 0.687678  [345600/859189]\n",
      "current batch loss: 0.634524  [352000/859189]\n",
      "current batch loss: 0.711586  [358400/859189]\n",
      "current batch loss: 0.657905  [364800/859189]\n",
      "current batch loss: 0.635592  [371200/859189]\n",
      "current batch loss: 0.629827  [377600/859189]\n",
      "current batch loss: 0.751359  [384000/859189]\n",
      "current batch loss: 0.649230  [390400/859189]\n",
      "current batch loss: 0.611965  [396800/859189]\n",
      "current batch loss: 0.665058  [403200/859189]\n",
      "current batch loss: 0.587526  [409600/859189]\n",
      "current batch loss: 0.687744  [416000/859189]\n",
      "current batch loss: 0.671770  [422400/859189]\n",
      "current batch loss: 0.620688  [428800/859189]\n",
      "current batch loss: 0.629946  [435200/859189]\n",
      "current batch loss: 0.687265  [441600/859189]\n",
      "current batch loss: 0.613933  [448000/859189]\n",
      "current batch loss: 0.671618  [454400/859189]\n",
      "current batch loss: 0.630958  [460800/859189]\n",
      "current batch loss: 0.642698  [467200/859189]\n",
      "current batch loss: 0.652935  [473600/859189]\n",
      "current batch loss: 0.594843  [480000/859189]\n",
      "current batch loss: 0.673710  [486400/859189]\n",
      "current batch loss: 0.635877  [492800/859189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.603055  [499200/859189]\n",
      "current batch loss: 0.663706  [505600/859189]\n",
      "current batch loss: 0.644677  [512000/859189]\n",
      "current batch loss: 0.701543  [518400/859189]\n",
      "current batch loss: 0.636951  [524800/859189]\n",
      "current batch loss: 0.693195  [531200/859189]\n",
      "current batch loss: 0.648783  [537600/859189]\n",
      "current batch loss: 0.677993  [544000/859189]\n",
      "current batch loss: 0.605047  [550400/859189]\n",
      "current batch loss: 0.637004  [556800/859189]\n",
      "current batch loss: 0.683131  [563200/859189]\n",
      "current batch loss: 0.601073  [569600/859189]\n",
      "current batch loss: 0.708407  [576000/859189]\n",
      "current batch loss: 0.666099  [582400/859189]\n",
      "current batch loss: 0.619170  [588800/859189]\n",
      "current batch loss: 0.641441  [595200/859189]\n",
      "current batch loss: 0.617308  [601600/859189]\n",
      "current batch loss: 0.640818  [608000/859189]\n",
      "current batch loss: 0.666969  [614400/859189]\n",
      "current batch loss: 0.690383  [620800/859189]\n",
      "current batch loss: 0.642328  [627200/859189]\n",
      "current batch loss: 0.693455  [633600/859189]\n",
      "current batch loss: 0.622980  [640000/859189]\n",
      "current batch loss: 0.629745  [646400/859189]\n",
      "current batch loss: 0.651030  [652800/859189]\n",
      "current batch loss: 0.651924  [659200/859189]\n",
      "current batch loss: 0.699592  [665600/859189]\n",
      "current batch loss: 0.623438  [672000/859189]\n",
      "current batch loss: 0.662160  [678400/859189]\n",
      "current batch loss: 0.665628  [684800/859189]\n",
      "current batch loss: 0.629973  [691200/859189]\n",
      "current batch loss: 0.641707  [697600/859189]\n",
      "current batch loss: 0.636493  [704000/859189]\n",
      "current batch loss: 0.675029  [710400/859189]\n",
      "current batch loss: 0.624189  [716800/859189]\n",
      "current batch loss: 0.656307  [723200/859189]\n",
      "current batch loss: 0.639080  [729600/859189]\n",
      "current batch loss: 0.599342  [736000/859189]\n",
      "current batch loss: 0.716924  [742400/859189]\n",
      "current batch loss: 0.618416  [748800/859189]\n",
      "current batch loss: 0.696537  [755200/859189]\n",
      "current batch loss: 0.668992  [761600/859189]\n",
      "current batch loss: 0.604573  [768000/859189]\n",
      "current batch loss: 0.691458  [774400/859189]\n",
      "current batch loss: 0.600058  [780800/859189]\n",
      "current batch loss: 0.645824  [787200/859189]\n",
      "current batch loss: 0.653049  [793600/859189]\n",
      "current batch loss: 0.587237  [800000/859189]\n",
      "current batch loss: 0.609145  [806400/859189]\n",
      "current batch loss: 0.659686  [812800/859189]\n",
      "current batch loss: 0.644751  [819200/859189]\n",
      "current batch loss: 0.686051  [825600/859189]\n",
      "current batch loss: 0.616215  [832000/859189]\n",
      "current batch loss: 0.727312  [838400/859189]\n",
      "current batch loss: 0.563314  [844800/859189]\n",
      "current batch loss: 0.588537  [851200/859189]\n",
      "current batch loss: 0.655187  [857600/859189]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.649637\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.648905\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 9\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.639564  [    0/859189]\n",
      "current batch loss: 0.592533  [ 6400/859189]\n",
      "current batch loss: 0.652892  [12800/859189]\n",
      "current batch loss: 0.668404  [19200/859189]\n",
      "current batch loss: 0.724136  [25600/859189]\n",
      "current batch loss: 0.652542  [32000/859189]\n",
      "current batch loss: 0.727664  [38400/859189]\n",
      "current batch loss: 0.618700  [44800/859189]\n",
      "current batch loss: 0.705299  [51200/859189]\n",
      "current batch loss: 0.698696  [57600/859189]\n",
      "current batch loss: 0.645633  [64000/859189]\n",
      "current batch loss: 0.676740  [70400/859189]\n",
      "current batch loss: 0.706628  [76800/859189]\n",
      "current batch loss: 0.612318  [83200/859189]\n",
      "current batch loss: 0.697300  [89600/859189]\n",
      "current batch loss: 0.688140  [96000/859189]\n",
      "current batch loss: 0.636691  [102400/859189]\n",
      "current batch loss: 0.696168  [108800/859189]\n",
      "current batch loss: 0.666967  [115200/859189]\n",
      "current batch loss: 0.694541  [121600/859189]\n",
      "current batch loss: 0.652449  [128000/859189]\n",
      "current batch loss: 0.638657  [134400/859189]\n",
      "current batch loss: 0.713242  [140800/859189]\n",
      "current batch loss: 0.651677  [147200/859189]\n",
      "current batch loss: 0.671604  [153600/859189]\n",
      "current batch loss: 0.648140  [160000/859189]\n",
      "current batch loss: 0.644126  [166400/859189]\n",
      "current batch loss: 0.652654  [172800/859189]\n",
      "current batch loss: 0.661330  [179200/859189]\n",
      "current batch loss: 0.632286  [185600/859189]\n",
      "current batch loss: 0.682504  [192000/859189]\n",
      "current batch loss: 0.615610  [198400/859189]\n",
      "current batch loss: 0.648373  [204800/859189]\n",
      "current batch loss: 0.594188  [211200/859189]\n",
      "current batch loss: 0.611508  [217600/859189]\n",
      "current batch loss: 0.653855  [224000/859189]\n",
      "current batch loss: 0.683088  [230400/859189]\n",
      "current batch loss: 0.622429  [236800/859189]\n",
      "current batch loss: 0.628621  [243200/859189]\n",
      "current batch loss: 0.680069  [249600/859189]\n",
      "current batch loss: 0.662162  [256000/859189]\n",
      "current batch loss: 0.647019  [262400/859189]\n",
      "current batch loss: 0.657447  [268800/859189]\n",
      "current batch loss: 0.622622  [275200/859189]\n",
      "current batch loss: 0.639297  [281600/859189]\n",
      "current batch loss: 0.659490  [288000/859189]\n",
      "current batch loss: 0.688845  [294400/859189]\n",
      "current batch loss: 0.677004  [300800/859189]\n",
      "current batch loss: 0.636113  [307200/859189]\n",
      "current batch loss: 0.654846  [313600/859189]\n",
      "current batch loss: 0.603278  [320000/859189]\n",
      "current batch loss: 0.632384  [326400/859189]\n",
      "current batch loss: 0.651798  [332800/859189]\n",
      "current batch loss: 0.677146  [339200/859189]\n",
      "current batch loss: 0.616386  [345600/859189]\n",
      "current batch loss: 0.622783  [352000/859189]\n",
      "current batch loss: 0.665549  [358400/859189]\n",
      "current batch loss: 0.673339  [364800/859189]\n",
      "current batch loss: 0.663509  [371200/859189]\n",
      "current batch loss: 0.673605  [377600/859189]\n",
      "current batch loss: 0.734617  [384000/859189]\n",
      "current batch loss: 0.693262  [390400/859189]\n",
      "current batch loss: 0.643412  [396800/859189]\n",
      "current batch loss: 0.726187  [403200/859189]\n",
      "current batch loss: 0.628311  [409600/859189]\n",
      "current batch loss: 0.668388  [416000/859189]\n",
      "current batch loss: 0.646072  [422400/859189]\n",
      "current batch loss: 0.756173  [428800/859189]\n",
      "current batch loss: 0.647472  [435200/859189]\n",
      "current batch loss: 0.692865  [441600/859189]\n",
      "current batch loss: 0.655287  [448000/859189]\n",
      "current batch loss: 0.620386  [454400/859189]\n",
      "current batch loss: 0.652991  [460800/859189]\n",
      "current batch loss: 0.632421  [467200/859189]\n",
      "current batch loss: 0.698719  [473600/859189]\n",
      "current batch loss: 0.635276  [480000/859189]\n",
      "current batch loss: 0.667033  [486400/859189]\n",
      "current batch loss: 0.656839  [492800/859189]\n",
      "current batch loss: 0.589893  [499200/859189]\n",
      "current batch loss: 0.693815  [505600/859189]\n",
      "current batch loss: 0.690963  [512000/859189]\n",
      "current batch loss: 0.588898  [518400/859189]\n",
      "current batch loss: 0.750029  [524800/859189]\n",
      "current batch loss: 0.648595  [531200/859189]\n",
      "current batch loss: 0.628538  [537600/859189]\n",
      "current batch loss: 0.672737  [544000/859189]\n",
      "current batch loss: 0.653236  [550400/859189]\n",
      "current batch loss: 0.605999  [556800/859189]\n",
      "current batch loss: 0.556877  [563200/859189]\n",
      "current batch loss: 0.616195  [569600/859189]\n",
      "current batch loss: 0.665564  [576000/859189]\n",
      "current batch loss: 0.618129  [582400/859189]\n",
      "current batch loss: 0.615863  [588800/859189]\n",
      "current batch loss: 0.642899  [595200/859189]\n",
      "current batch loss: 0.675323  [601600/859189]\n",
      "current batch loss: 0.711582  [608000/859189]\n",
      "current batch loss: 0.601909  [614400/859189]\n",
      "current batch loss: 0.652969  [620800/859189]\n",
      "current batch loss: 0.657616  [627200/859189]\n",
      "current batch loss: 0.683604  [633600/859189]\n",
      "current batch loss: 0.646197  [640000/859189]\n",
      "current batch loss: 0.608405  [646400/859189]\n",
      "current batch loss: 0.703421  [652800/859189]\n",
      "current batch loss: 0.648742  [659200/859189]\n",
      "current batch loss: 0.664657  [665600/859189]\n",
      "current batch loss: 0.682318  [672000/859189]\n",
      "current batch loss: 0.614304  [678400/859189]\n",
      "current batch loss: 0.602893  [684800/859189]\n",
      "current batch loss: 0.642980  [691200/859189]\n",
      "current batch loss: 0.754175  [697600/859189]\n",
      "current batch loss: 0.688864  [704000/859189]\n",
      "current batch loss: 0.597350  [710400/859189]\n",
      "current batch loss: 0.666821  [716800/859189]\n",
      "current batch loss: 0.645439  [723200/859189]\n",
      "current batch loss: 0.665499  [729600/859189]\n",
      "current batch loss: 0.671373  [736000/859189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.630453  [742400/859189]\n",
      "current batch loss: 0.638058  [748800/859189]\n",
      "current batch loss: 0.681675  [755200/859189]\n",
      "current batch loss: 0.624365  [761600/859189]\n",
      "current batch loss: 0.656940  [768000/859189]\n",
      "current batch loss: 0.604273  [774400/859189]\n",
      "current batch loss: 0.572577  [780800/859189]\n",
      "current batch loss: 0.639063  [787200/859189]\n",
      "current batch loss: 0.637788  [793600/859189]\n",
      "current batch loss: 0.607889  [800000/859189]\n",
      "current batch loss: 0.694275  [806400/859189]\n",
      "current batch loss: 0.583148  [812800/859189]\n",
      "current batch loss: 0.661098  [819200/859189]\n",
      "current batch loss: 0.553133  [825600/859189]\n",
      "current batch loss: 0.601651  [832000/859189]\n",
      "current batch loss: 0.566014  [838400/859189]\n",
      "current batch loss: 0.743499  [844800/859189]\n",
      "current batch loss: 0.661556  [851200/859189]\n",
      "current batch loss: 0.656501  [857600/859189]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.649604\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.649068\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 10\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.602916  [    0/859189]\n",
      "current batch loss: 0.660906  [ 6400/859189]\n",
      "current batch loss: 0.668922  [12800/859189]\n",
      "current batch loss: 0.630571  [19200/859189]\n",
      "current batch loss: 0.641951  [25600/859189]\n",
      "current batch loss: 0.629795  [32000/859189]\n",
      "current batch loss: 0.647331  [38400/859189]\n",
      "current batch loss: 0.671350  [44800/859189]\n",
      "current batch loss: 0.657396  [51200/859189]\n",
      "current batch loss: 0.639409  [57600/859189]\n",
      "current batch loss: 0.661191  [64000/859189]\n",
      "current batch loss: 0.594649  [70400/859189]\n",
      "current batch loss: 0.630086  [76800/859189]\n",
      "current batch loss: 0.693416  [83200/859189]\n",
      "current batch loss: 0.652018  [89600/859189]\n",
      "current batch loss: 0.701492  [96000/859189]\n",
      "current batch loss: 0.641837  [102400/859189]\n",
      "current batch loss: 0.667378  [108800/859189]\n",
      "current batch loss: 0.655258  [115200/859189]\n",
      "current batch loss: 0.699707  [121600/859189]\n",
      "current batch loss: 0.673467  [128000/859189]\n",
      "current batch loss: 0.649649  [134400/859189]\n",
      "current batch loss: 0.647440  [140800/859189]\n",
      "current batch loss: 0.679395  [147200/859189]\n",
      "current batch loss: 0.661008  [153600/859189]\n",
      "current batch loss: 0.651106  [160000/859189]\n",
      "current batch loss: 0.649453  [166400/859189]\n",
      "current batch loss: 0.633069  [172800/859189]\n",
      "current batch loss: 0.606029  [179200/859189]\n",
      "current batch loss: 0.668514  [185600/859189]\n",
      "current batch loss: 0.614613  [192000/859189]\n",
      "current batch loss: 0.645553  [198400/859189]\n",
      "current batch loss: 0.639512  [204800/859189]\n",
      "current batch loss: 0.583201  [211200/859189]\n",
      "current batch loss: 0.592553  [217600/859189]\n",
      "current batch loss: 0.705562  [224000/859189]\n",
      "current batch loss: 0.649034  [230400/859189]\n",
      "current batch loss: 0.637541  [236800/859189]\n",
      "current batch loss: 0.751170  [243200/859189]\n",
      "current batch loss: 0.636662  [249600/859189]\n",
      "current batch loss: 0.619542  [256000/859189]\n",
      "current batch loss: 0.640196  [262400/859189]\n",
      "current batch loss: 0.633299  [268800/859189]\n",
      "current batch loss: 0.655727  [275200/859189]\n",
      "current batch loss: 0.616396  [281600/859189]\n",
      "current batch loss: 0.663186  [288000/859189]\n",
      "current batch loss: 0.663408  [294400/859189]\n",
      "current batch loss: 0.669916  [300800/859189]\n",
      "current batch loss: 0.645021  [307200/859189]\n",
      "current batch loss: 0.637349  [313600/859189]\n",
      "current batch loss: 0.697461  [320000/859189]\n",
      "current batch loss: 0.634806  [326400/859189]\n",
      "current batch loss: 0.658889  [332800/859189]\n",
      "current batch loss: 0.694469  [339200/859189]\n",
      "current batch loss: 0.608519  [345600/859189]\n",
      "current batch loss: 0.551291  [352000/859189]\n",
      "current batch loss: 0.636260  [358400/859189]\n",
      "current batch loss: 0.688499  [364800/859189]\n",
      "current batch loss: 0.644577  [371200/859189]\n",
      "current batch loss: 0.621717  [377600/859189]\n",
      "current batch loss: 0.695243  [384000/859189]\n",
      "current batch loss: 0.692031  [390400/859189]\n",
      "current batch loss: 0.679579  [396800/859189]\n",
      "current batch loss: 0.638180  [403200/859189]\n",
      "current batch loss: 0.668708  [409600/859189]\n",
      "current batch loss: 0.719972  [416000/859189]\n",
      "current batch loss: 0.631793  [422400/859189]\n",
      "current batch loss: 0.661391  [428800/859189]\n",
      "current batch loss: 0.645285  [435200/859189]\n",
      "current batch loss: 0.692053  [441600/859189]\n",
      "current batch loss: 0.646512  [448000/859189]\n",
      "current batch loss: 0.658599  [454400/859189]\n",
      "current batch loss: 0.643090  [460800/859189]\n",
      "current batch loss: 0.619883  [467200/859189]\n",
      "current batch loss: 0.632439  [473600/859189]\n",
      "current batch loss: 0.620639  [480000/859189]\n",
      "current batch loss: 0.662068  [486400/859189]\n",
      "current batch loss: 0.639786  [492800/859189]\n",
      "current batch loss: 0.629925  [499200/859189]\n",
      "current batch loss: 0.593147  [505600/859189]\n",
      "current batch loss: 0.666798  [512000/859189]\n",
      "current batch loss: 0.601663  [518400/859189]\n",
      "current batch loss: 0.684831  [524800/859189]\n",
      "current batch loss: 0.726341  [531200/859189]\n",
      "current batch loss: 0.607839  [537600/859189]\n",
      "current batch loss: 0.723037  [544000/859189]\n",
      "current batch loss: 0.639569  [550400/859189]\n",
      "current batch loss: 0.647351  [556800/859189]\n",
      "current batch loss: 0.657131  [563200/859189]\n",
      "current batch loss: 0.680523  [569600/859189]\n",
      "current batch loss: 0.662385  [576000/859189]\n",
      "current batch loss: 0.660636  [582400/859189]\n",
      "current batch loss: 0.571517  [588800/859189]\n",
      "current batch loss: 0.613553  [595200/859189]\n",
      "current batch loss: 0.683046  [601600/859189]\n",
      "current batch loss: 0.631732  [608000/859189]\n",
      "current batch loss: 0.641249  [614400/859189]\n",
      "current batch loss: 0.662569  [620800/859189]\n",
      "current batch loss: 0.651458  [627200/859189]\n",
      "current batch loss: 0.624642  [633600/859189]\n",
      "current batch loss: 0.649667  [640000/859189]\n",
      "current batch loss: 0.651472  [646400/859189]\n",
      "current batch loss: 0.642290  [652800/859189]\n",
      "current batch loss: 0.566428  [659200/859189]\n",
      "current batch loss: 0.635760  [665600/859189]\n",
      "current batch loss: 0.620742  [672000/859189]\n",
      "current batch loss: 0.696056  [678400/859189]\n",
      "current batch loss: 0.671204  [684800/859189]\n",
      "current batch loss: 0.667935  [691200/859189]\n",
      "current batch loss: 0.684335  [697600/859189]\n",
      "current batch loss: 0.610730  [704000/859189]\n",
      "current batch loss: 0.661245  [710400/859189]\n",
      "current batch loss: 0.691407  [716800/859189]\n",
      "current batch loss: 0.620392  [723200/859189]\n",
      "current batch loss: 0.654685  [729600/859189]\n",
      "current batch loss: 0.608781  [736000/859189]\n",
      "current batch loss: 0.727043  [742400/859189]\n",
      "current batch loss: 0.719597  [748800/859189]\n",
      "current batch loss: 0.652949  [755200/859189]\n",
      "current batch loss: 0.657630  [761600/859189]\n",
      "current batch loss: 0.656094  [768000/859189]\n",
      "current batch loss: 0.615756  [774400/859189]\n",
      "current batch loss: 0.678881  [780800/859189]\n",
      "current batch loss: 0.657659  [787200/859189]\n",
      "current batch loss: 0.661744  [793600/859189]\n",
      "current batch loss: 0.696578  [800000/859189]\n",
      "current batch loss: 0.641013  [806400/859189]\n",
      "current batch loss: 0.594989  [812800/859189]\n",
      "current batch loss: 0.678931  [819200/859189]\n",
      "current batch loss: 0.632825  [825600/859189]\n",
      "current batch loss: 0.643110  [832000/859189]\n",
      "current batch loss: 0.630898  [838400/859189]\n",
      "current batch loss: 0.621182  [844800/859189]\n",
      "current batch loss: 0.671410  [851200/859189]\n",
      "current batch loss: 0.696920  [857600/859189]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.649798\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.649341\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 11\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.619001  [    0/859189]\n",
      "current batch loss: 0.617771  [ 6400/859189]\n",
      "current batch loss: 0.656679  [12800/859189]\n",
      "current batch loss: 0.631000  [19200/859189]\n",
      "current batch loss: 0.626575  [25600/859189]\n",
      "current batch loss: 0.618571  [32000/859189]\n",
      "current batch loss: 0.752059  [38400/859189]\n",
      "current batch loss: 0.675462  [44800/859189]\n",
      "current batch loss: 0.642482  [51200/859189]\n",
      "current batch loss: 0.537990  [57600/859189]\n",
      "current batch loss: 0.685797  [64000/859189]\n",
      "current batch loss: 0.644924  [70400/859189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.633915  [76800/859189]\n",
      "current batch loss: 0.590044  [83200/859189]\n",
      "current batch loss: 0.646856  [89600/859189]\n",
      "current batch loss: 0.680629  [96000/859189]\n",
      "current batch loss: 0.624960  [102400/859189]\n",
      "current batch loss: 0.572929  [108800/859189]\n",
      "current batch loss: 0.676541  [115200/859189]\n",
      "current batch loss: 0.655222  [121600/859189]\n",
      "current batch loss: 0.640885  [128000/859189]\n",
      "current batch loss: 0.609571  [134400/859189]\n",
      "current batch loss: 0.711928  [140800/859189]\n",
      "current batch loss: 0.709977  [147200/859189]\n",
      "current batch loss: 0.620571  [153600/859189]\n",
      "current batch loss: 0.650155  [160000/859189]\n",
      "current batch loss: 0.664517  [166400/859189]\n",
      "current batch loss: 0.664822  [172800/859189]\n",
      "current batch loss: 0.667407  [179200/859189]\n",
      "current batch loss: 0.678712  [185600/859189]\n",
      "current batch loss: 0.627039  [192000/859189]\n",
      "current batch loss: 0.585029  [198400/859189]\n",
      "current batch loss: 0.637679  [204800/859189]\n",
      "current batch loss: 0.676399  [211200/859189]\n",
      "current batch loss: 0.627079  [217600/859189]\n",
      "current batch loss: 0.673055  [224000/859189]\n",
      "current batch loss: 0.657129  [230400/859189]\n",
      "current batch loss: 0.682928  [236800/859189]\n",
      "current batch loss: 0.705013  [243200/859189]\n",
      "current batch loss: 0.552797  [249600/859189]\n",
      "current batch loss: 0.691093  [256000/859189]\n",
      "current batch loss: 0.656032  [262400/859189]\n",
      "current batch loss: 0.599462  [268800/859189]\n",
      "current batch loss: 0.661286  [275200/859189]\n",
      "current batch loss: 0.606566  [281600/859189]\n",
      "current batch loss: 0.676262  [288000/859189]\n",
      "current batch loss: 0.689103  [294400/859189]\n",
      "current batch loss: 0.613366  [300800/859189]\n",
      "current batch loss: 0.679411  [307200/859189]\n",
      "current batch loss: 0.628699  [313600/859189]\n",
      "current batch loss: 0.725981  [320000/859189]\n",
      "current batch loss: 0.619677  [326400/859189]\n",
      "current batch loss: 0.666873  [332800/859189]\n",
      "current batch loss: 0.669292  [339200/859189]\n",
      "current batch loss: 0.683295  [345600/859189]\n",
      "current batch loss: 0.664339  [352000/859189]\n",
      "current batch loss: 0.633649  [358400/859189]\n",
      "current batch loss: 0.614432  [364800/859189]\n",
      "current batch loss: 0.674491  [371200/859189]\n",
      "current batch loss: 0.630881  [377600/859189]\n",
      "current batch loss: 0.713550  [384000/859189]\n",
      "current batch loss: 0.654404  [390400/859189]\n",
      "current batch loss: 0.585121  [396800/859189]\n",
      "current batch loss: 0.613674  [403200/859189]\n",
      "current batch loss: 0.697908  [409600/859189]\n",
      "current batch loss: 0.611117  [416000/859189]\n",
      "current batch loss: 0.704429  [422400/859189]\n",
      "current batch loss: 0.657350  [428800/859189]\n",
      "current batch loss: 0.632080  [435200/859189]\n",
      "current batch loss: 0.678173  [441600/859189]\n",
      "current batch loss: 0.658835  [448000/859189]\n",
      "current batch loss: 0.683581  [454400/859189]\n",
      "current batch loss: 0.638610  [460800/859189]\n",
      "current batch loss: 0.661916  [467200/859189]\n",
      "current batch loss: 0.633489  [473600/859189]\n",
      "current batch loss: 0.614082  [480000/859189]\n",
      "current batch loss: 0.680974  [486400/859189]\n",
      "current batch loss: 0.627263  [492800/859189]\n",
      "current batch loss: 0.564198  [499200/859189]\n",
      "current batch loss: 0.629605  [505600/859189]\n",
      "current batch loss: 0.650225  [512000/859189]\n",
      "current batch loss: 0.629691  [518400/859189]\n",
      "current batch loss: 0.656437  [524800/859189]\n",
      "current batch loss: 0.669577  [531200/859189]\n",
      "current batch loss: 0.640980  [537600/859189]\n",
      "current batch loss: 0.650393  [544000/859189]\n",
      "current batch loss: 0.659326  [550400/859189]\n",
      "current batch loss: 0.737624  [556800/859189]\n",
      "current batch loss: 0.614106  [563200/859189]\n",
      "current batch loss: 0.618941  [569600/859189]\n",
      "current batch loss: 0.676424  [576000/859189]\n",
      "current batch loss: 0.619054  [582400/859189]\n",
      "current batch loss: 0.639976  [588800/859189]\n",
      "current batch loss: 0.652505  [595200/859189]\n",
      "current batch loss: 0.595946  [601600/859189]\n",
      "current batch loss: 0.608675  [608000/859189]\n",
      "current batch loss: 0.660082  [614400/859189]\n",
      "current batch loss: 0.661938  [620800/859189]\n",
      "current batch loss: 0.620920  [627200/859189]\n",
      "current batch loss: 0.634188  [633600/859189]\n",
      "current batch loss: 0.660325  [640000/859189]\n",
      "current batch loss: 0.671400  [646400/859189]\n",
      "current batch loss: 0.709960  [652800/859189]\n",
      "current batch loss: 0.634387  [659200/859189]\n",
      "current batch loss: 0.750976  [665600/859189]\n",
      "current batch loss: 0.694609  [672000/859189]\n",
      "current batch loss: 0.645117  [678400/859189]\n",
      "current batch loss: 0.646352  [684800/859189]\n",
      "current batch loss: 0.675094  [691200/859189]\n",
      "current batch loss: 0.536936  [697600/859189]\n",
      "current batch loss: 0.642823  [704000/859189]\n",
      "current batch loss: 0.640702  [710400/859189]\n",
      "current batch loss: 0.603076  [716800/859189]\n",
      "current batch loss: 0.627370  [723200/859189]\n",
      "current batch loss: 0.590823  [729600/859189]\n",
      "current batch loss: 0.670256  [736000/859189]\n",
      "current batch loss: 0.644859  [742400/859189]\n",
      "current batch loss: 0.625962  [748800/859189]\n",
      "current batch loss: 0.632022  [755200/859189]\n",
      "current batch loss: 0.726093  [761600/859189]\n",
      "current batch loss: 0.683037  [768000/859189]\n",
      "current batch loss: 0.646565  [774400/859189]\n",
      "current batch loss: 0.673992  [780800/859189]\n",
      "current batch loss: 0.665665  [787200/859189]\n",
      "current batch loss: 0.706679  [793600/859189]\n",
      "current batch loss: 0.584769  [800000/859189]\n",
      "current batch loss: 0.724552  [806400/859189]\n",
      "current batch loss: 0.633352  [812800/859189]\n",
      "current batch loss: 0.689030  [819200/859189]\n",
      "current batch loss: 0.584842  [825600/859189]\n",
      "current batch loss: 0.616413  [832000/859189]\n",
      "current batch loss: 0.556032  [838400/859189]\n",
      "current batch loss: 0.625184  [844800/859189]\n",
      "current batch loss: 0.660839  [851200/859189]\n",
      "current batch loss: 0.699412  [857600/859189]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.649129\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.648667\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 12\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.612042  [    0/859189]\n",
      "current batch loss: 0.592749  [ 6400/859189]\n",
      "current batch loss: 0.644926  [12800/859189]\n",
      "current batch loss: 0.620351  [19200/859189]\n",
      "current batch loss: 0.663688  [25600/859189]\n",
      "current batch loss: 0.671552  [32000/859189]\n",
      "current batch loss: 0.670192  [38400/859189]\n",
      "current batch loss: 0.663267  [44800/859189]\n",
      "current batch loss: 0.611046  [51200/859189]\n",
      "current batch loss: 0.643128  [57600/859189]\n",
      "current batch loss: 0.569590  [64000/859189]\n",
      "current batch loss: 0.711312  [70400/859189]\n",
      "current batch loss: 0.661739  [76800/859189]\n",
      "current batch loss: 0.646194  [83200/859189]\n",
      "current batch loss: 0.619616  [89600/859189]\n",
      "current batch loss: 0.627662  [96000/859189]\n",
      "current batch loss: 0.615299  [102400/859189]\n",
      "current batch loss: 0.651029  [108800/859189]\n",
      "current batch loss: 0.654983  [115200/859189]\n",
      "current batch loss: 0.672438  [121600/859189]\n",
      "current batch loss: 0.575269  [128000/859189]\n",
      "current batch loss: 0.594523  [134400/859189]\n",
      "current batch loss: 0.685091  [140800/859189]\n",
      "current batch loss: 0.639682  [147200/859189]\n",
      "current batch loss: 0.638474  [153600/859189]\n",
      "current batch loss: 0.609312  [160000/859189]\n",
      "current batch loss: 0.611005  [166400/859189]\n",
      "current batch loss: 0.610657  [172800/859189]\n",
      "current batch loss: 0.694683  [179200/859189]\n",
      "current batch loss: 0.643077  [185600/859189]\n",
      "current batch loss: 0.630371  [192000/859189]\n",
      "current batch loss: 0.656016  [198400/859189]\n",
      "current batch loss: 0.582015  [204800/859189]\n",
      "current batch loss: 0.664015  [211200/859189]\n",
      "current batch loss: 0.666258  [217600/859189]\n",
      "current batch loss: 0.674693  [224000/859189]\n",
      "current batch loss: 0.623531  [230400/859189]\n",
      "current batch loss: 0.636357  [236800/859189]\n",
      "current batch loss: 0.634766  [243200/859189]\n",
      "current batch loss: 0.697649  [249600/859189]\n",
      "current batch loss: 0.633605  [256000/859189]\n",
      "current batch loss: 0.708802  [262400/859189]\n",
      "current batch loss: 0.615193  [268800/859189]\n",
      "current batch loss: 0.686028  [275200/859189]\n",
      "current batch loss: 0.621544  [281600/859189]\n",
      "current batch loss: 0.644769  [288000/859189]\n",
      "current batch loss: 0.578661  [294400/859189]\n",
      "current batch loss: 0.688017  [300800/859189]\n",
      "current batch loss: 0.654399  [307200/859189]\n",
      "current batch loss: 0.664012  [313600/859189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.670805  [320000/859189]\n",
      "current batch loss: 0.775169  [326400/859189]\n",
      "current batch loss: 0.644679  [332800/859189]\n",
      "current batch loss: 0.686056  [339200/859189]\n",
      "current batch loss: 0.610987  [345600/859189]\n",
      "current batch loss: 0.673007  [352000/859189]\n",
      "current batch loss: 0.652028  [358400/859189]\n",
      "current batch loss: 0.595746  [364800/859189]\n",
      "current batch loss: 0.683961  [371200/859189]\n",
      "current batch loss: 0.672336  [377600/859189]\n",
      "current batch loss: 0.633647  [384000/859189]\n",
      "current batch loss: 0.632763  [390400/859189]\n",
      "current batch loss: 0.673912  [396800/859189]\n",
      "current batch loss: 0.632324  [403200/859189]\n",
      "current batch loss: 0.686901  [409600/859189]\n",
      "current batch loss: 0.630999  [416000/859189]\n",
      "current batch loss: 0.649120  [422400/859189]\n",
      "current batch loss: 0.667073  [428800/859189]\n",
      "current batch loss: 0.679165  [435200/859189]\n",
      "current batch loss: 0.596368  [441600/859189]\n",
      "current batch loss: 0.613560  [448000/859189]\n",
      "current batch loss: 0.679697  [454400/859189]\n",
      "current batch loss: 0.748053  [460800/859189]\n",
      "current batch loss: 0.605555  [467200/859189]\n",
      "current batch loss: 0.692021  [473600/859189]\n",
      "current batch loss: 0.630136  [480000/859189]\n",
      "current batch loss: 0.663014  [486400/859189]\n",
      "current batch loss: 0.635047  [492800/859189]\n",
      "current batch loss: 0.678813  [499200/859189]\n",
      "current batch loss: 0.629388  [505600/859189]\n",
      "current batch loss: 0.615372  [512000/859189]\n",
      "current batch loss: 0.619850  [518400/859189]\n",
      "current batch loss: 0.672632  [524800/859189]\n",
      "current batch loss: 0.670407  [531200/859189]\n",
      "current batch loss: 0.627664  [537600/859189]\n",
      "current batch loss: 0.630017  [544000/859189]\n",
      "current batch loss: 0.676374  [550400/859189]\n",
      "current batch loss: 0.625337  [556800/859189]\n",
      "current batch loss: 0.649080  [563200/859189]\n",
      "current batch loss: 0.673731  [569600/859189]\n",
      "current batch loss: 0.633497  [576000/859189]\n",
      "current batch loss: 0.617267  [582400/859189]\n",
      "current batch loss: 0.652710  [588800/859189]\n",
      "current batch loss: 0.652036  [595200/859189]\n",
      "current batch loss: 0.646453  [601600/859189]\n",
      "current batch loss: 0.618790  [608000/859189]\n",
      "current batch loss: 0.614903  [614400/859189]\n",
      "current batch loss: 0.590009  [620800/859189]\n",
      "current batch loss: 0.685664  [627200/859189]\n",
      "current batch loss: 0.655311  [633600/859189]\n",
      "current batch loss: 0.675125  [640000/859189]\n",
      "current batch loss: 0.617824  [646400/859189]\n",
      "current batch loss: 0.594918  [652800/859189]\n",
      "current batch loss: 0.620431  [659200/859189]\n",
      "current batch loss: 0.618421  [665600/859189]\n",
      "current batch loss: 0.607445  [672000/859189]\n",
      "current batch loss: 0.628774  [678400/859189]\n",
      "current batch loss: 0.676540  [684800/859189]\n",
      "current batch loss: 0.660951  [691200/859189]\n",
      "current batch loss: 0.615047  [697600/859189]\n",
      "current batch loss: 0.661821  [704000/859189]\n",
      "current batch loss: 0.658369  [710400/859189]\n",
      "current batch loss: 0.687362  [716800/859189]\n",
      "current batch loss: 0.687736  [723200/859189]\n",
      "current batch loss: 0.649208  [729600/859189]\n",
      "current batch loss: 0.649108  [736000/859189]\n",
      "current batch loss: 0.679701  [742400/859189]\n",
      "current batch loss: 0.699430  [748800/859189]\n",
      "current batch loss: 0.630751  [755200/859189]\n",
      "current batch loss: 0.612196  [761600/859189]\n",
      "current batch loss: 0.648208  [768000/859189]\n",
      "current batch loss: 0.637615  [774400/859189]\n",
      "current batch loss: 0.681127  [780800/859189]\n",
      "current batch loss: 0.631333  [787200/859189]\n",
      "current batch loss: 0.641892  [793600/859189]\n",
      "current batch loss: 0.599428  [800000/859189]\n",
      "current batch loss: 0.591697  [806400/859189]\n",
      "current batch loss: 0.575371  [812800/859189]\n",
      "current batch loss: 0.696413  [819200/859189]\n",
      "current batch loss: 0.621764  [825600/859189]\n",
      "current batch loss: 0.694569  [832000/859189]\n",
      "current batch loss: 0.704951  [838400/859189]\n",
      "current batch loss: 0.644023  [844800/859189]\n",
      "current batch loss: 0.712906  [851200/859189]\n",
      "current batch loss: 0.608258  [857600/859189]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.648804\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.647975\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 13\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.669725  [    0/859189]\n",
      "current batch loss: 0.645110  [ 6400/859189]\n",
      "current batch loss: 0.614631  [12800/859189]\n",
      "current batch loss: 0.589242  [19200/859189]\n",
      "current batch loss: 0.628393  [25600/859189]\n",
      "current batch loss: 0.679583  [32000/859189]\n",
      "current batch loss: 0.627610  [38400/859189]\n",
      "current batch loss: 0.706413  [44800/859189]\n",
      "current batch loss: 0.656880  [51200/859189]\n",
      "current batch loss: 0.658855  [57600/859189]\n",
      "current batch loss: 0.612594  [64000/859189]\n",
      "current batch loss: 0.647535  [70400/859189]\n",
      "current batch loss: 0.666222  [76800/859189]\n",
      "current batch loss: 0.606124  [83200/859189]\n",
      "current batch loss: 0.658590  [89600/859189]\n",
      "current batch loss: 0.623524  [96000/859189]\n",
      "current batch loss: 0.659582  [102400/859189]\n",
      "current batch loss: 0.704479  [108800/859189]\n",
      "current batch loss: 0.661260  [115200/859189]\n",
      "current batch loss: 0.591066  [121600/859189]\n",
      "current batch loss: 0.654215  [128000/859189]\n",
      "current batch loss: 0.684818  [134400/859189]\n",
      "current batch loss: 0.656907  [140800/859189]\n",
      "current batch loss: 0.642672  [147200/859189]\n",
      "current batch loss: 0.627232  [153600/859189]\n",
      "current batch loss: 0.607498  [160000/859189]\n",
      "current batch loss: 0.696166  [166400/859189]\n",
      "current batch loss: 0.634341  [172800/859189]\n",
      "current batch loss: 0.665684  [179200/859189]\n",
      "current batch loss: 0.652911  [185600/859189]\n",
      "current batch loss: 0.686520  [192000/859189]\n",
      "current batch loss: 0.635805  [198400/859189]\n",
      "current batch loss: 0.667289  [204800/859189]\n",
      "current batch loss: 0.754207  [211200/859189]\n",
      "current batch loss: 0.634865  [217600/859189]\n",
      "current batch loss: 0.652798  [224000/859189]\n",
      "current batch loss: 0.680715  [230400/859189]\n",
      "current batch loss: 0.668078  [236800/859189]\n",
      "current batch loss: 0.702498  [243200/859189]\n",
      "current batch loss: 0.608634  [249600/859189]\n",
      "current batch loss: 0.600692  [256000/859189]\n",
      "current batch loss: 0.635382  [262400/859189]\n",
      "current batch loss: 0.591900  [268800/859189]\n",
      "current batch loss: 0.633356  [275200/859189]\n",
      "current batch loss: 0.653040  [281600/859189]\n",
      "current batch loss: 0.628266  [288000/859189]\n",
      "current batch loss: 0.631551  [294400/859189]\n",
      "current batch loss: 0.610782  [300800/859189]\n",
      "current batch loss: 0.688645  [307200/859189]\n",
      "current batch loss: 0.704694  [313600/859189]\n",
      "current batch loss: 0.599915  [320000/859189]\n",
      "current batch loss: 0.619957  [326400/859189]\n",
      "current batch loss: 0.700598  [332800/859189]\n",
      "current batch loss: 0.642897  [339200/859189]\n",
      "current batch loss: 0.612999  [345600/859189]\n",
      "current batch loss: 0.663514  [352000/859189]\n",
      "current batch loss: 0.667946  [358400/859189]\n",
      "current batch loss: 0.589279  [364800/859189]\n",
      "current batch loss: 0.633185  [371200/859189]\n",
      "current batch loss: 0.684565  [377600/859189]\n",
      "current batch loss: 0.713227  [384000/859189]\n",
      "current batch loss: 0.659977  [390400/859189]\n",
      "current batch loss: 0.730007  [396800/859189]\n",
      "current batch loss: 0.639875  [403200/859189]\n",
      "current batch loss: 0.608478  [409600/859189]\n",
      "current batch loss: 0.660955  [416000/859189]\n",
      "current batch loss: 0.584732  [422400/859189]\n",
      "current batch loss: 0.659572  [428800/859189]\n",
      "current batch loss: 0.605212  [435200/859189]\n",
      "current batch loss: 0.687595  [441600/859189]\n",
      "current batch loss: 0.625495  [448000/859189]\n",
      "current batch loss: 0.642206  [454400/859189]\n",
      "current batch loss: 0.678719  [460800/859189]\n",
      "current batch loss: 0.714636  [467200/859189]\n",
      "current batch loss: 0.640558  [473600/859189]\n",
      "current batch loss: 0.697992  [480000/859189]\n",
      "current batch loss: 0.569156  [486400/859189]\n",
      "current batch loss: 0.675501  [492800/859189]\n",
      "current batch loss: 0.642490  [499200/859189]\n",
      "current batch loss: 0.656257  [505600/859189]\n",
      "current batch loss: 0.710774  [512000/859189]\n",
      "current batch loss: 0.644872  [518400/859189]\n",
      "current batch loss: 0.616567  [524800/859189]\n",
      "current batch loss: 0.706419  [531200/859189]\n",
      "current batch loss: 0.619582  [537600/859189]\n",
      "current batch loss: 0.681066  [544000/859189]\n",
      "current batch loss: 0.544488  [550400/859189]\n",
      "current batch loss: 0.649240  [556800/859189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.638598  [563200/859189]\n",
      "current batch loss: 0.716224  [569600/859189]\n",
      "current batch loss: 0.677547  [576000/859189]\n",
      "current batch loss: 0.692664  [582400/859189]\n",
      "current batch loss: 0.657966  [588800/859189]\n",
      "current batch loss: 0.618032  [595200/859189]\n",
      "current batch loss: 0.627616  [601600/859189]\n",
      "current batch loss: 0.703122  [608000/859189]\n",
      "current batch loss: 0.662338  [614400/859189]\n",
      "current batch loss: 0.634431  [620800/859189]\n",
      "current batch loss: 0.644993  [627200/859189]\n",
      "current batch loss: 0.660051  [633600/859189]\n",
      "current batch loss: 0.592799  [640000/859189]\n",
      "current batch loss: 0.600320  [646400/859189]\n",
      "current batch loss: 0.673426  [652800/859189]\n",
      "current batch loss: 0.633207  [659200/859189]\n",
      "current batch loss: 0.645095  [665600/859189]\n",
      "current batch loss: 0.713609  [672000/859189]\n",
      "current batch loss: 0.695124  [678400/859189]\n",
      "current batch loss: 0.619064  [684800/859189]\n",
      "current batch loss: 0.664541  [691200/859189]\n",
      "current batch loss: 0.695905  [697600/859189]\n",
      "current batch loss: 0.662347  [704000/859189]\n",
      "current batch loss: 0.622576  [710400/859189]\n",
      "current batch loss: 0.653980  [716800/859189]\n",
      "current batch loss: 0.614492  [723200/859189]\n",
      "current batch loss: 0.655706  [729600/859189]\n",
      "current batch loss: 0.651978  [736000/859189]\n",
      "current batch loss: 0.622205  [742400/859189]\n",
      "current batch loss: 0.663122  [748800/859189]\n",
      "current batch loss: 0.647862  [755200/859189]\n",
      "current batch loss: 0.712675  [761600/859189]\n",
      "current batch loss: 0.713001  [768000/859189]\n",
      "current batch loss: 0.695446  [774400/859189]\n",
      "current batch loss: 0.689421  [780800/859189]\n",
      "current batch loss: 0.620696  [787200/859189]\n",
      "current batch loss: 0.663749  [793600/859189]\n",
      "current batch loss: 0.641533  [800000/859189]\n",
      "current batch loss: 0.653893  [806400/859189]\n",
      "current batch loss: 0.650914  [812800/859189]\n",
      "current batch loss: 0.609358  [819200/859189]\n",
      "current batch loss: 0.704835  [825600/859189]\n",
      "current batch loss: 0.617435  [832000/859189]\n",
      "current batch loss: 0.630930  [838400/859189]\n",
      "current batch loss: 0.570400  [844800/859189]\n",
      "current batch loss: 0.697061  [851200/859189]\n",
      "current batch loss: 0.643595  [857600/859189]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.649170\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.648812\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 14\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.646259  [    0/859189]\n",
      "current batch loss: 0.675978  [ 6400/859189]\n",
      "current batch loss: 0.648674  [12800/859189]\n",
      "current batch loss: 0.628757  [19200/859189]\n",
      "current batch loss: 0.613562  [25600/859189]\n",
      "current batch loss: 0.678256  [32000/859189]\n",
      "current batch loss: 0.644889  [38400/859189]\n",
      "current batch loss: 0.602547  [44800/859189]\n",
      "current batch loss: 0.648727  [51200/859189]\n",
      "current batch loss: 0.633027  [57600/859189]\n",
      "current batch loss: 0.579737  [64000/859189]\n",
      "current batch loss: 0.665955  [70400/859189]\n",
      "current batch loss: 0.676454  [76800/859189]\n",
      "current batch loss: 0.688504  [83200/859189]\n",
      "current batch loss: 0.674940  [89600/859189]\n",
      "current batch loss: 0.729026  [96000/859189]\n",
      "current batch loss: 0.715348  [102400/859189]\n",
      "current batch loss: 0.595940  [108800/859189]\n",
      "current batch loss: 0.645765  [115200/859189]\n",
      "current batch loss: 0.637085  [121600/859189]\n",
      "current batch loss: 0.666273  [128000/859189]\n",
      "current batch loss: 0.634054  [134400/859189]\n",
      "current batch loss: 0.641059  [140800/859189]\n",
      "current batch loss: 0.698005  [147200/859189]\n",
      "current batch loss: 0.699204  [153600/859189]\n",
      "current batch loss: 0.649964  [160000/859189]\n",
      "current batch loss: 0.631967  [166400/859189]\n",
      "current batch loss: 0.699511  [172800/859189]\n",
      "current batch loss: 0.663231  [179200/859189]\n",
      "current batch loss: 0.627971  [185600/859189]\n",
      "current batch loss: 0.642977  [192000/859189]\n",
      "current batch loss: 0.644780  [198400/859189]\n",
      "current batch loss: 0.672562  [204800/859189]\n",
      "current batch loss: 0.634200  [211200/859189]\n",
      "current batch loss: 0.642291  [217600/859189]\n",
      "current batch loss: 0.656494  [224000/859189]\n",
      "current batch loss: 0.618587  [230400/859189]\n",
      "current batch loss: 0.678575  [236800/859189]\n",
      "current batch loss: 0.660967  [243200/859189]\n",
      "current batch loss: 0.620708  [249600/859189]\n",
      "current batch loss: 0.624414  [256000/859189]\n",
      "current batch loss: 0.651212  [262400/859189]\n",
      "current batch loss: 0.616054  [268800/859189]\n",
      "current batch loss: 0.699151  [275200/859189]\n",
      "current batch loss: 0.642663  [281600/859189]\n",
      "current batch loss: 0.679367  [288000/859189]\n",
      "current batch loss: 0.575002  [294400/859189]\n",
      "current batch loss: 0.655254  [300800/859189]\n",
      "current batch loss: 0.637085  [307200/859189]\n",
      "current batch loss: 0.699753  [313600/859189]\n",
      "current batch loss: 0.722601  [320000/859189]\n",
      "current batch loss: 0.612023  [326400/859189]\n",
      "current batch loss: 0.653600  [332800/859189]\n",
      "current batch loss: 0.647251  [339200/859189]\n",
      "current batch loss: 0.639013  [345600/859189]\n",
      "current batch loss: 0.665943  [352000/859189]\n",
      "current batch loss: 0.662761  [358400/859189]\n",
      "current batch loss: 0.704883  [364800/859189]\n",
      "current batch loss: 0.597899  [371200/859189]\n",
      "current batch loss: 0.582671  [377600/859189]\n",
      "current batch loss: 0.594243  [384000/859189]\n",
      "current batch loss: 0.643741  [390400/859189]\n",
      "current batch loss: 0.625674  [396800/859189]\n",
      "current batch loss: 0.626358  [403200/859189]\n",
      "current batch loss: 0.710996  [409600/859189]\n",
      "current batch loss: 0.701239  [416000/859189]\n",
      "current batch loss: 0.661001  [422400/859189]\n",
      "current batch loss: 0.716042  [428800/859189]\n",
      "current batch loss: 0.635570  [435200/859189]\n",
      "current batch loss: 0.651667  [441600/859189]\n",
      "current batch loss: 0.638668  [448000/859189]\n",
      "current batch loss: 0.582406  [454400/859189]\n",
      "current batch loss: 0.675454  [460800/859189]\n",
      "current batch loss: 0.612790  [467200/859189]\n",
      "current batch loss: 0.630728  [473600/859189]\n",
      "current batch loss: 0.643672  [480000/859189]\n",
      "current batch loss: 0.610338  [486400/859189]\n",
      "current batch loss: 0.643788  [492800/859189]\n",
      "current batch loss: 0.637001  [499200/859189]\n",
      "current batch loss: 0.624164  [505600/859189]\n",
      "current batch loss: 0.631854  [512000/859189]\n",
      "current batch loss: 0.648364  [518400/859189]\n",
      "current batch loss: 0.609653  [524800/859189]\n",
      "current batch loss: 0.647874  [531200/859189]\n",
      "current batch loss: 0.639315  [537600/859189]\n",
      "current batch loss: 0.625295  [544000/859189]\n",
      "current batch loss: 0.677053  [550400/859189]\n",
      "current batch loss: 0.701004  [556800/859189]\n",
      "current batch loss: 0.591980  [563200/859189]\n",
      "current batch loss: 0.627785  [569600/859189]\n",
      "current batch loss: 0.661732  [576000/859189]\n",
      "current batch loss: 0.644270  [582400/859189]\n",
      "current batch loss: 0.693960  [588800/859189]\n",
      "current batch loss: 0.684337  [595200/859189]\n",
      "current batch loss: 0.618669  [601600/859189]\n",
      "current batch loss: 0.674630  [608000/859189]\n",
      "current batch loss: 0.699922  [614400/859189]\n",
      "current batch loss: 0.702951  [620800/859189]\n",
      "current batch loss: 0.667884  [627200/859189]\n",
      "current batch loss: 0.607380  [633600/859189]\n",
      "current batch loss: 0.636970  [640000/859189]\n",
      "current batch loss: 0.643271  [646400/859189]\n",
      "current batch loss: 0.636451  [652800/859189]\n",
      "current batch loss: 0.657230  [659200/859189]\n",
      "current batch loss: 0.654081  [665600/859189]\n",
      "current batch loss: 0.588292  [672000/859189]\n",
      "current batch loss: 0.646406  [678400/859189]\n",
      "current batch loss: 0.626179  [684800/859189]\n",
      "current batch loss: 0.644951  [691200/859189]\n",
      "current batch loss: 0.672366  [697600/859189]\n",
      "current batch loss: 0.624672  [704000/859189]\n",
      "current batch loss: 0.739779  [710400/859189]\n",
      "current batch loss: 0.660463  [716800/859189]\n",
      "current batch loss: 0.671470  [723200/859189]\n",
      "current batch loss: 0.657594  [729600/859189]\n",
      "current batch loss: 0.651445  [736000/859189]\n",
      "current batch loss: 0.628187  [742400/859189]\n",
      "current batch loss: 0.674082  [748800/859189]\n",
      "current batch loss: 0.648423  [755200/859189]\n",
      "current batch loss: 0.643075  [761600/859189]\n",
      "current batch loss: 0.679395  [768000/859189]\n",
      "current batch loss: 0.603144  [774400/859189]\n",
      "current batch loss: 0.593204  [780800/859189]\n",
      "current batch loss: 0.631331  [787200/859189]\n",
      "current batch loss: 0.657539  [793600/859189]\n",
      "current batch loss: 0.606740  [800000/859189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.586634  [806400/859189]\n",
      "current batch loss: 0.676128  [812800/859189]\n",
      "current batch loss: 0.647444  [819200/859189]\n",
      "current batch loss: 0.687266  [825600/859189]\n",
      "current batch loss: 0.652879  [832000/859189]\n",
      "current batch loss: 0.713267  [838400/859189]\n",
      "current batch loss: 0.669239  [844800/859189]\n",
      "current batch loss: 0.613604  [851200/859189]\n",
      "current batch loss: 0.653029  [857600/859189]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.648054\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.647576\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 15\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.579045  [    0/859189]\n",
      "current batch loss: 0.601053  [ 6400/859189]\n",
      "current batch loss: 0.695562  [12800/859189]\n",
      "current batch loss: 0.691413  [19200/859189]\n",
      "current batch loss: 0.660087  [25600/859189]\n",
      "current batch loss: 0.668647  [32000/859189]\n",
      "current batch loss: 0.705100  [38400/859189]\n",
      "current batch loss: 0.646621  [44800/859189]\n",
      "current batch loss: 0.584173  [51200/859189]\n",
      "current batch loss: 0.635707  [57600/859189]\n",
      "current batch loss: 0.626781  [64000/859189]\n",
      "current batch loss: 0.617329  [70400/859189]\n",
      "current batch loss: 0.630961  [76800/859189]\n",
      "current batch loss: 0.669449  [83200/859189]\n",
      "current batch loss: 0.710382  [89600/859189]\n",
      "current batch loss: 0.682380  [96000/859189]\n",
      "current batch loss: 0.654408  [102400/859189]\n",
      "current batch loss: 0.556325  [108800/859189]\n",
      "current batch loss: 0.636292  [115200/859189]\n",
      "current batch loss: 0.635830  [121600/859189]\n",
      "current batch loss: 0.635029  [128000/859189]\n",
      "current batch loss: 0.616451  [134400/859189]\n",
      "current batch loss: 0.619442  [140800/859189]\n",
      "current batch loss: 0.706890  [147200/859189]\n",
      "current batch loss: 0.706325  [153600/859189]\n",
      "current batch loss: 0.714284  [160000/859189]\n",
      "current batch loss: 0.640061  [166400/859189]\n",
      "current batch loss: 0.631105  [172800/859189]\n",
      "current batch loss: 0.636410  [179200/859189]\n",
      "current batch loss: 0.619876  [185600/859189]\n",
      "current batch loss: 0.666191  [192000/859189]\n",
      "current batch loss: 0.572323  [198400/859189]\n",
      "current batch loss: 0.633793  [204800/859189]\n",
      "current batch loss: 0.682643  [211200/859189]\n",
      "current batch loss: 0.685357  [217600/859189]\n",
      "current batch loss: 0.671087  [224000/859189]\n",
      "current batch loss: 0.602250  [230400/859189]\n",
      "current batch loss: 0.630854  [236800/859189]\n",
      "current batch loss: 0.646791  [243200/859189]\n",
      "current batch loss: 0.664459  [249600/859189]\n",
      "current batch loss: 0.679683  [256000/859189]\n",
      "current batch loss: 0.623195  [262400/859189]\n",
      "current batch loss: 0.661596  [268800/859189]\n",
      "current batch loss: 0.629439  [275200/859189]\n",
      "current batch loss: 0.715231  [281600/859189]\n",
      "current batch loss: 0.612391  [288000/859189]\n",
      "current batch loss: 0.688360  [294400/859189]\n",
      "current batch loss: 0.620568  [300800/859189]\n",
      "current batch loss: 0.628503  [307200/859189]\n",
      "current batch loss: 0.632472  [313600/859189]\n",
      "current batch loss: 0.635343  [320000/859189]\n",
      "current batch loss: 0.647284  [326400/859189]\n",
      "current batch loss: 0.676187  [332800/859189]\n",
      "current batch loss: 0.666021  [339200/859189]\n",
      "current batch loss: 0.633174  [345600/859189]\n",
      "current batch loss: 0.632038  [352000/859189]\n",
      "current batch loss: 0.654787  [358400/859189]\n",
      "current batch loss: 0.702586  [364800/859189]\n",
      "current batch loss: 0.655573  [371200/859189]\n",
      "current batch loss: 0.648321  [377600/859189]\n",
      "current batch loss: 0.655493  [384000/859189]\n",
      "current batch loss: 0.633629  [390400/859189]\n",
      "current batch loss: 0.655305  [396800/859189]\n",
      "current batch loss: 0.681997  [403200/859189]\n",
      "current batch loss: 0.578779  [409600/859189]\n",
      "current batch loss: 0.672761  [416000/859189]\n",
      "current batch loss: 0.618731  [422400/859189]\n",
      "current batch loss: 0.677423  [428800/859189]\n",
      "current batch loss: 0.640422  [435200/859189]\n",
      "current batch loss: 0.636740  [441600/859189]\n",
      "current batch loss: 0.659551  [448000/859189]\n",
      "current batch loss: 0.612589  [454400/859189]\n",
      "current batch loss: 0.661759  [460800/859189]\n",
      "current batch loss: 0.661619  [467200/859189]\n",
      "current batch loss: 0.604468  [473600/859189]\n",
      "current batch loss: 0.643477  [480000/859189]\n",
      "current batch loss: 0.682165  [486400/859189]\n",
      "current batch loss: 0.616257  [492800/859189]\n",
      "current batch loss: 0.703375  [499200/859189]\n",
      "current batch loss: 0.666921  [505600/859189]\n",
      "current batch loss: 0.656891  [512000/859189]\n",
      "current batch loss: 0.714596  [518400/859189]\n",
      "current batch loss: 0.647363  [524800/859189]\n",
      "current batch loss: 0.667764  [531200/859189]\n",
      "current batch loss: 0.694936  [537600/859189]\n",
      "current batch loss: 0.607543  [544000/859189]\n",
      "current batch loss: 0.658259  [550400/859189]\n",
      "current batch loss: 0.598091  [556800/859189]\n",
      "current batch loss: 0.669911  [563200/859189]\n",
      "current batch loss: 0.583544  [569600/859189]\n",
      "current batch loss: 0.647277  [576000/859189]\n",
      "current batch loss: 0.665901  [582400/859189]\n",
      "current batch loss: 0.605819  [588800/859189]\n",
      "current batch loss: 0.632271  [595200/859189]\n",
      "current batch loss: 0.649011  [601600/859189]\n",
      "current batch loss: 0.594198  [608000/859189]\n",
      "current batch loss: 0.730382  [614400/859189]\n",
      "current batch loss: 0.608452  [620800/859189]\n",
      "current batch loss: 0.722877  [627200/859189]\n",
      "current batch loss: 0.689026  [633600/859189]\n",
      "current batch loss: 0.642732  [640000/859189]\n",
      "current batch loss: 0.572158  [646400/859189]\n",
      "current batch loss: 0.710041  [652800/859189]\n",
      "current batch loss: 0.662201  [659200/859189]\n",
      "current batch loss: 0.650450  [665600/859189]\n",
      "current batch loss: 0.635319  [672000/859189]\n",
      "current batch loss: 0.621699  [678400/859189]\n",
      "current batch loss: 0.694506  [684800/859189]\n",
      "current batch loss: 0.722279  [691200/859189]\n",
      "current batch loss: 0.676141  [697600/859189]\n",
      "current batch loss: 0.658790  [704000/859189]\n",
      "current batch loss: 0.623403  [710400/859189]\n",
      "current batch loss: 0.659678  [716800/859189]\n",
      "current batch loss: 0.625294  [723200/859189]\n",
      "current batch loss: 0.690987  [729600/859189]\n",
      "current batch loss: 0.682499  [736000/859189]\n",
      "current batch loss: 0.628680  [742400/859189]\n",
      "current batch loss: 0.586751  [748800/859189]\n",
      "current batch loss: 0.673287  [755200/859189]\n",
      "current batch loss: 0.655580  [761600/859189]\n",
      "current batch loss: 0.625465  [768000/859189]\n",
      "current batch loss: 0.650186  [774400/859189]\n",
      "current batch loss: 0.561469  [780800/859189]\n",
      "current batch loss: 0.712779  [787200/859189]\n",
      "current batch loss: 0.665242  [793600/859189]\n",
      "current batch loss: 0.633039  [800000/859189]\n",
      "current batch loss: 0.695164  [806400/859189]\n",
      "current batch loss: 0.689433  [812800/859189]\n",
      "current batch loss: 0.633080  [819200/859189]\n",
      "current batch loss: 0.635000  [825600/859189]\n",
      "current batch loss: 0.588521  [832000/859189]\n",
      "current batch loss: 0.628140  [838400/859189]\n",
      "current batch loss: 0.621476  [844800/859189]\n",
      "current batch loss: 0.598718  [851200/859189]\n",
      "current batch loss: 0.572855  [857600/859189]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.647531\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.647252\n",
      "-----------------------------------------------\n",
      "|\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "trn_dataset_sb = cwola_data_sb( X_train_p, y_train_p, b_train_p, 0.05 )\n",
    "val_dataset_sb = cwola_data_sb( X_val_p, y_val_p, b_val_p, 0.05 )\n",
    "\n",
    "trn_dataloader_sb = DataLoader( trn_dataset_sb, batch_size=64, shuffle=True )\n",
    "val_dataloader_sb = DataLoader( val_dataset_sb, batch_size=64, shuffle=True )\n",
    "\n",
    "print( \"train S/B: \" + str( np.round( ( trn_dataset_sb.labels_cut.sum() / (1.0-trn_dataset_sb.labels_cut).sum() )*100, 2 ).item() ) + \"%\" )\n",
    "print( \"val S/B: \" + str( np.round( ( val_dataset_sb.labels_cut.sum() / (1.0-val_dataset_sb.labels_cut).sum() )*100, 2 ).item() ) + \"%\" )\n",
    "\n",
    "# a useful function to present things clearer\n",
    "def separator():\n",
    "    print( \"-----------------------------------------------\" )\n",
    "\n",
    "epochs = 15\n",
    "\n",
    "# re-initialise the model and the optimizer\n",
    "model_sb5pc = cwolaNet( 4, 64 ).to(device)\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam( model_sb5pc.parameters(), lr=learning_rate )\n",
    "separator()\n",
    "print( \"model architecture \")\n",
    "separator()\n",
    "print( model )\n",
    "\n",
    "# track train and val losses\n",
    "trn_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for t in range(epochs):\n",
    "    separator()\n",
    "    print( f\"Epoch {t+1}\" )\n",
    "    separator()\n",
    "    train_epoch( trn_dataloader_sb, model_sb5pc, loss_fn, optimizer )\n",
    "    separator()\n",
    "    trn_loss = trn_pass( trn_dataloader_sb, model_sb5pc, loss_fn )\n",
    "    trn_losses.append( trn_loss )\n",
    "    separator()\n",
    "    val_loss = val_pass( val_dataloader_sb, model_sb5pc, loss_fn )\n",
    "    val_losses.append( val_loss )\n",
    "    separator()\n",
    "    print( \"|\" )\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_sb5pc = model_sb5pc( X_test_p ).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(14,5))\n",
    "\n",
    "fpr, tpr, th = roc_curve( y_test, test_pred_sb5pc )\n",
    "auc_score = roc_auc_score( y_test, test_pred_sb5pc )\n",
    "rnd_class = np.linspace(0, 1, 100)\n",
    "\n",
    "ax[0].plot(fpr, tpr, label='AUC = {:.2f}'.format(auc_score) )\n",
    "ax[0].plot(rnd_class, rnd_class, '--', label='Rnd classifier')\n",
    "ax[1].plot(tpr, 1/fpr, label='AUC = {:.2f}\\n $1/\\epsilon_{{bkg}}$(0.3) = {:.0f}'.format(auc_score, 1/fpr[closest_point(tpr, tpr_p=0.3)]))\n",
    "ax[1].plot(rnd_class, 1/rnd_class, '--', label='Rnd classifier')\n",
    "ax[1].set_yscale('log')\n",
    "\n",
    "ax[0].set_xlabel('$\\epsilon_{bkg}$ - FPR', fontproperties=axislabelfont)\n",
    "ax[0].set_ylabel('$\\epsilon_{s}$ - TPR', fontproperties=axislabelfont)\n",
    "\n",
    "ax[1].set_xlabel('$\\epsilon_{s}$ - TPR', fontproperties=axislabelfont)\n",
    "ax[1].set_ylabel('1/$\\epsilon_{bkg}$ - Inverse FPR', fontproperties=axislabelfont)\n",
    "for i in range(len(ax)):\n",
    "    ax[i].legend(prop=axislabelfont)\n",
    "    ax[i].tick_params(labelsize=axisfontsize)\n",
    "    ax[i].grid('on')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S/B = 1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train S/B: 1.0%\n",
      "val S/B: 1.0%\n",
      "-----------------------------------------------\n",
      "model architecture \n",
      "-----------------------------------------------\n",
      "cwolaNet(\n",
      "  (layer1): Linear(in_features=4, out_features=64, bias=True)\n",
      "  (relu_1): ReLU()\n",
      "  (layer2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (relu_2): ReLU()\n",
      "  (layer3): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "-----------------------------------------------\n",
      "Epoch 1\n",
      "-----------------------------------------------\n",
      "current batch loss: 6.054812  [    0/826458]\n",
      "current batch loss: 0.707391  [ 6400/826458]\n",
      "current batch loss: 0.682145  [12800/826458]\n",
      "current batch loss: 0.723682  [19200/826458]\n",
      "current batch loss: 0.715006  [25600/826458]\n",
      "current batch loss: 0.827510  [32000/826458]\n",
      "current batch loss: 0.671934  [38400/826458]\n",
      "current batch loss: 0.632651  [44800/826458]\n",
      "current batch loss: 0.712850  [51200/826458]\n",
      "current batch loss: 0.740941  [57600/826458]\n",
      "current batch loss: 0.650049  [64000/826458]\n",
      "current batch loss: 0.671821  [70400/826458]\n",
      "current batch loss: 0.615328  [76800/826458]\n",
      "current batch loss: 0.673722  [83200/826458]\n",
      "current batch loss: 0.727711  [89600/826458]\n",
      "current batch loss: 0.690061  [96000/826458]\n",
      "current batch loss: 0.624111  [102400/826458]\n",
      "current batch loss: 0.661071  [108800/826458]\n",
      "current batch loss: 0.643674  [115200/826458]\n",
      "current batch loss: 0.662400  [121600/826458]\n",
      "current batch loss: 0.730381  [128000/826458]\n",
      "current batch loss: 0.748749  [134400/826458]\n",
      "current batch loss: 0.651865  [140800/826458]\n",
      "current batch loss: 0.663735  [147200/826458]\n",
      "current batch loss: 0.638560  [153600/826458]\n",
      "current batch loss: 0.670979  [160000/826458]\n",
      "current batch loss: 0.650467  [166400/826458]\n",
      "current batch loss: 0.627621  [172800/826458]\n",
      "current batch loss: 0.642113  [179200/826458]\n",
      "current batch loss: 0.610352  [185600/826458]\n",
      "current batch loss: 0.699206  [192000/826458]\n",
      "current batch loss: 0.678973  [198400/826458]\n",
      "current batch loss: 0.650769  [204800/826458]\n",
      "current batch loss: 0.672256  [211200/826458]\n",
      "current batch loss: 0.671008  [217600/826458]\n",
      "current batch loss: 0.669303  [224000/826458]\n",
      "current batch loss: 0.604907  [230400/826458]\n",
      "current batch loss: 0.738481  [236800/826458]\n",
      "current batch loss: 0.636143  [243200/826458]\n",
      "current batch loss: 0.637435  [249600/826458]\n",
      "current batch loss: 0.668553  [256000/826458]\n",
      "current batch loss: 0.681537  [262400/826458]\n",
      "current batch loss: 0.728489  [268800/826458]\n",
      "current batch loss: 0.614618  [275200/826458]\n",
      "current batch loss: 0.715097  [281600/826458]\n",
      "current batch loss: 0.641337  [288000/826458]\n",
      "current batch loss: 0.763195  [294400/826458]\n",
      "current batch loss: 0.673905  [300800/826458]\n",
      "current batch loss: 0.657350  [307200/826458]\n",
      "current batch loss: 0.614143  [313600/826458]\n",
      "current batch loss: 0.685837  [320000/826458]\n",
      "current batch loss: 0.673535  [326400/826458]\n",
      "current batch loss: 0.617044  [332800/826458]\n",
      "current batch loss: 0.714717  [339200/826458]\n",
      "current batch loss: 0.685350  [345600/826458]\n",
      "current batch loss: 0.665252  [352000/826458]\n",
      "current batch loss: 0.665311  [358400/826458]\n",
      "current batch loss: 0.681576  [364800/826458]\n",
      "current batch loss: 0.620910  [371200/826458]\n",
      "current batch loss: 0.697514  [377600/826458]\n",
      "current batch loss: 0.628855  [384000/826458]\n",
      "current batch loss: 0.662020  [390400/826458]\n",
      "current batch loss: 0.707444  [396800/826458]\n",
      "current batch loss: 0.708927  [403200/826458]\n",
      "current batch loss: 0.663123  [409600/826458]\n",
      "current batch loss: 0.603936  [416000/826458]\n",
      "current batch loss: 0.631024  [422400/826458]\n",
      "current batch loss: 0.680119  [428800/826458]\n",
      "current batch loss: 0.605752  [435200/826458]\n",
      "current batch loss: 0.594391  [441600/826458]\n",
      "current batch loss: 0.632321  [448000/826458]\n",
      "current batch loss: 0.615406  [454400/826458]\n",
      "current batch loss: 0.679944  [460800/826458]\n",
      "current batch loss: 0.653225  [467200/826458]\n",
      "current batch loss: 0.644686  [473600/826458]\n",
      "current batch loss: 0.719783  [480000/826458]\n",
      "current batch loss: 0.684520  [486400/826458]\n",
      "current batch loss: 0.646700  [492800/826458]\n",
      "current batch loss: 0.609530  [499200/826458]\n",
      "current batch loss: 0.637755  [505600/826458]\n",
      "current batch loss: 0.665147  [512000/826458]\n",
      "current batch loss: 0.593719  [518400/826458]\n",
      "current batch loss: 0.700404  [524800/826458]\n",
      "current batch loss: 0.644752  [531200/826458]\n",
      "current batch loss: 0.668544  [537600/826458]\n",
      "current batch loss: 0.639182  [544000/826458]\n",
      "current batch loss: 0.625597  [550400/826458]\n",
      "current batch loss: 0.644681  [556800/826458]\n",
      "current batch loss: 0.692037  [563200/826458]\n",
      "current batch loss: 0.606400  [569600/826458]\n",
      "current batch loss: 0.681759  [576000/826458]\n",
      "current batch loss: 0.691382  [582400/826458]\n",
      "current batch loss: 0.631346  [588800/826458]\n",
      "current batch loss: 0.663081  [595200/826458]\n",
      "current batch loss: 0.681643  [601600/826458]\n",
      "current batch loss: 0.648166  [608000/826458]\n",
      "current batch loss: 0.648684  [614400/826458]\n",
      "current batch loss: 0.644308  [620800/826458]\n",
      "current batch loss: 0.618669  [627200/826458]\n",
      "current batch loss: 0.667638  [633600/826458]\n",
      "current batch loss: 0.601648  [640000/826458]\n",
      "current batch loss: 0.656436  [646400/826458]\n",
      "current batch loss: 0.639682  [652800/826458]\n",
      "current batch loss: 0.648235  [659200/826458]\n",
      "current batch loss: 0.668463  [665600/826458]\n",
      "current batch loss: 0.657477  [672000/826458]\n",
      "current batch loss: 0.646359  [678400/826458]\n",
      "current batch loss: 0.639189  [684800/826458]\n",
      "current batch loss: 0.678041  [691200/826458]\n",
      "current batch loss: 0.656696  [697600/826458]\n",
      "current batch loss: 0.635424  [704000/826458]\n",
      "current batch loss: 0.711304  [710400/826458]\n",
      "current batch loss: 0.647901  [716800/826458]\n",
      "current batch loss: 0.718649  [723200/826458]\n",
      "current batch loss: 0.631395  [729600/826458]\n",
      "current batch loss: 0.697736  [736000/826458]\n",
      "current batch loss: 0.617049  [742400/826458]\n",
      "current batch loss: 0.669660  [748800/826458]\n",
      "current batch loss: 0.656962  [755200/826458]\n",
      "current batch loss: 0.630948  [761600/826458]\n",
      "current batch loss: 0.596545  [768000/826458]\n",
      "current batch loss: 0.651320  [774400/826458]\n",
      "current batch loss: 0.669196  [780800/826458]\n",
      "current batch loss: 0.659419  [787200/826458]\n",
      "current batch loss: 0.650725  [793600/826458]\n",
      "current batch loss: 0.702683  [800000/826458]\n",
      "current batch loss: 0.677805  [806400/826458]\n",
      "current batch loss: 0.676828  [812800/826458]\n",
      "current batch loss: 0.653055  [819200/826458]\n",
      "current batch loss: 0.633764  [825600/826458]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.657638\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.657180\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 2\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.675190  [    0/826458]\n",
      "current batch loss: 0.632080  [ 6400/826458]\n",
      "current batch loss: 0.601635  [12800/826458]\n",
      "current batch loss: 0.669995  [19200/826458]\n",
      "current batch loss: 0.625740  [25600/826458]\n",
      "current batch loss: 0.657460  [32000/826458]\n",
      "current batch loss: 0.655369  [38400/826458]\n",
      "current batch loss: 0.683703  [44800/826458]\n",
      "current batch loss: 0.650075  [51200/826458]\n",
      "current batch loss: 0.612753  [57600/826458]\n",
      "current batch loss: 0.651011  [64000/826458]\n",
      "current batch loss: 0.640683  [70400/826458]\n",
      "current batch loss: 0.696927  [76800/826458]\n",
      "current batch loss: 0.698553  [83200/826458]\n",
      "current batch loss: 0.646417  [89600/826458]\n",
      "current batch loss: 0.645744  [96000/826458]\n",
      "current batch loss: 0.698738  [102400/826458]\n",
      "current batch loss: 0.676495  [108800/826458]\n",
      "current batch loss: 0.642304  [115200/826458]\n",
      "current batch loss: 0.679288  [121600/826458]\n",
      "current batch loss: 0.681961  [128000/826458]\n",
      "current batch loss: 0.632306  [134400/826458]\n",
      "current batch loss: 0.644070  [140800/826458]\n",
      "current batch loss: 0.696240  [147200/826458]\n",
      "current batch loss: 0.661712  [153600/826458]\n",
      "current batch loss: 0.709702  [160000/826458]\n",
      "current batch loss: 0.656094  [166400/826458]\n",
      "current batch loss: 0.672368  [172800/826458]\n",
      "current batch loss: 0.574667  [179200/826458]\n",
      "current batch loss: 0.663063  [185600/826458]\n",
      "current batch loss: 0.587163  [192000/826458]\n",
      "current batch loss: 0.638140  [198400/826458]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.630399  [204800/826458]\n",
      "current batch loss: 0.683260  [211200/826458]\n",
      "current batch loss: 0.670509  [217600/826458]\n",
      "current batch loss: 0.642496  [224000/826458]\n",
      "current batch loss: 0.627049  [230400/826458]\n",
      "current batch loss: 0.652974  [236800/826458]\n",
      "current batch loss: 0.660341  [243200/826458]\n",
      "current batch loss: 0.668526  [249600/826458]\n",
      "current batch loss: 0.677479  [256000/826458]\n",
      "current batch loss: 0.599560  [262400/826458]\n",
      "current batch loss: 0.679347  [268800/826458]\n",
      "current batch loss: 0.598778  [275200/826458]\n",
      "current batch loss: 0.644323  [281600/826458]\n",
      "current batch loss: 0.675706  [288000/826458]\n",
      "current batch loss: 0.575044  [294400/826458]\n",
      "current batch loss: 0.611424  [300800/826458]\n",
      "current batch loss: 0.660107  [307200/826458]\n",
      "current batch loss: 0.695020  [313600/826458]\n",
      "current batch loss: 0.650960  [320000/826458]\n",
      "current batch loss: 0.670954  [326400/826458]\n",
      "current batch loss: 0.675951  [332800/826458]\n",
      "current batch loss: 0.707262  [339200/826458]\n",
      "current batch loss: 0.618680  [345600/826458]\n",
      "current batch loss: 0.718886  [352000/826458]\n",
      "current batch loss: 0.677835  [358400/826458]\n",
      "current batch loss: 0.633545  [364800/826458]\n",
      "current batch loss: 0.620435  [371200/826458]\n",
      "current batch loss: 0.666193  [377600/826458]\n",
      "current batch loss: 0.630449  [384000/826458]\n",
      "current batch loss: 0.717277  [390400/826458]\n",
      "current batch loss: 0.633161  [396800/826458]\n",
      "current batch loss: 0.688042  [403200/826458]\n",
      "current batch loss: 0.658908  [409600/826458]\n",
      "current batch loss: 0.697027  [416000/826458]\n",
      "current batch loss: 0.676860  [422400/826458]\n",
      "current batch loss: 0.606090  [428800/826458]\n",
      "current batch loss: 0.655299  [435200/826458]\n",
      "current batch loss: 0.680527  [441600/826458]\n",
      "current batch loss: 0.646358  [448000/826458]\n",
      "current batch loss: 0.658247  [454400/826458]\n",
      "current batch loss: 0.628298  [460800/826458]\n",
      "current batch loss: 0.725973  [467200/826458]\n",
      "current batch loss: 0.678205  [473600/826458]\n",
      "current batch loss: 0.602486  [480000/826458]\n",
      "current batch loss: 0.641831  [486400/826458]\n",
      "current batch loss: 0.631255  [492800/826458]\n",
      "current batch loss: 0.690551  [499200/826458]\n",
      "current batch loss: 0.665300  [505600/826458]\n",
      "current batch loss: 0.641158  [512000/826458]\n",
      "current batch loss: 0.687648  [518400/826458]\n",
      "current batch loss: 0.646887  [524800/826458]\n",
      "current batch loss: 0.601917  [531200/826458]\n",
      "current batch loss: 0.658769  [537600/826458]\n",
      "current batch loss: 0.628787  [544000/826458]\n",
      "current batch loss: 0.671086  [550400/826458]\n",
      "current batch loss: 0.665898  [556800/826458]\n",
      "current batch loss: 0.630951  [563200/826458]\n",
      "current batch loss: 0.672803  [569600/826458]\n",
      "current batch loss: 0.661989  [576000/826458]\n",
      "current batch loss: 0.708885  [582400/826458]\n",
      "current batch loss: 0.563193  [588800/826458]\n",
      "current batch loss: 0.653963  [595200/826458]\n",
      "current batch loss: 0.610910  [601600/826458]\n",
      "current batch loss: 0.673204  [608000/826458]\n",
      "current batch loss: 0.658625  [614400/826458]\n",
      "current batch loss: 0.636625  [620800/826458]\n",
      "current batch loss: 0.674255  [627200/826458]\n",
      "current batch loss: 0.685470  [633600/826458]\n",
      "current batch loss: 0.627687  [640000/826458]\n",
      "current batch loss: 0.660308  [646400/826458]\n",
      "current batch loss: 0.624440  [652800/826458]\n",
      "current batch loss: 0.687541  [659200/826458]\n",
      "current batch loss: 0.647350  [665600/826458]\n",
      "current batch loss: 0.596272  [672000/826458]\n",
      "current batch loss: 0.678719  [678400/826458]\n",
      "current batch loss: 0.642951  [684800/826458]\n",
      "current batch loss: 0.683354  [691200/826458]\n",
      "current batch loss: 0.690376  [697600/826458]\n",
      "current batch loss: 0.595620  [704000/826458]\n",
      "current batch loss: 0.628338  [710400/826458]\n",
      "current batch loss: 0.682438  [716800/826458]\n",
      "current batch loss: 0.696012  [723200/826458]\n",
      "current batch loss: 0.660711  [729600/826458]\n",
      "current batch loss: 0.589344  [736000/826458]\n",
      "current batch loss: 0.703909  [742400/826458]\n",
      "current batch loss: 0.593127  [748800/826458]\n",
      "current batch loss: 0.669573  [755200/826458]\n",
      "current batch loss: 0.678581  [761600/826458]\n",
      "current batch loss: 0.671120  [768000/826458]\n",
      "current batch loss: 0.636986  [774400/826458]\n",
      "current batch loss: 0.693889  [780800/826458]\n",
      "current batch loss: 0.676934  [787200/826458]\n",
      "current batch loss: 0.703725  [793600/826458]\n",
      "current batch loss: 0.636844  [800000/826458]\n",
      "current batch loss: 0.636112  [806400/826458]\n",
      "current batch loss: 0.782228  [812800/826458]\n",
      "current batch loss: 0.685815  [819200/826458]\n",
      "current batch loss: 0.645982  [825600/826458]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.652561\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.652298\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 3\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.654817  [    0/826458]\n",
      "current batch loss: 0.650669  [ 6400/826458]\n",
      "current batch loss: 0.633217  [12800/826458]\n",
      "current batch loss: 0.662164  [19200/826458]\n",
      "current batch loss: 0.631760  [25600/826458]\n",
      "current batch loss: 0.680071  [32000/826458]\n",
      "current batch loss: 0.687025  [38400/826458]\n",
      "current batch loss: 0.587141  [44800/826458]\n",
      "current batch loss: 0.684242  [51200/826458]\n",
      "current batch loss: 0.665475  [57600/826458]\n",
      "current batch loss: 0.644653  [64000/826458]\n",
      "current batch loss: 0.692317  [70400/826458]\n",
      "current batch loss: 0.656368  [76800/826458]\n",
      "current batch loss: 0.642008  [83200/826458]\n",
      "current batch loss: 0.659686  [89600/826458]\n",
      "current batch loss: 0.668965  [96000/826458]\n",
      "current batch loss: 0.620251  [102400/826458]\n",
      "current batch loss: 0.681575  [108800/826458]\n",
      "current batch loss: 0.693011  [115200/826458]\n",
      "current batch loss: 0.616434  [121600/826458]\n",
      "current batch loss: 0.586936  [128000/826458]\n",
      "current batch loss: 0.609379  [134400/826458]\n",
      "current batch loss: 0.622388  [140800/826458]\n",
      "current batch loss: 0.663963  [147200/826458]\n",
      "current batch loss: 0.665351  [153600/826458]\n",
      "current batch loss: 0.609693  [160000/826458]\n",
      "current batch loss: 0.708039  [166400/826458]\n",
      "current batch loss: 0.643421  [172800/826458]\n",
      "current batch loss: 0.720907  [179200/826458]\n",
      "current batch loss: 0.699096  [185600/826458]\n",
      "current batch loss: 0.668548  [192000/826458]\n",
      "current batch loss: 0.612053  [198400/826458]\n",
      "current batch loss: 0.646991  [204800/826458]\n",
      "current batch loss: 0.663491  [211200/826458]\n",
      "current batch loss: 0.672961  [217600/826458]\n",
      "current batch loss: 0.649847  [224000/826458]\n",
      "current batch loss: 0.680298  [230400/826458]\n",
      "current batch loss: 0.652477  [236800/826458]\n",
      "current batch loss: 0.654774  [243200/826458]\n",
      "current batch loss: 0.665659  [249600/826458]\n",
      "current batch loss: 0.659827  [256000/826458]\n",
      "current batch loss: 0.663476  [262400/826458]\n",
      "current batch loss: 0.643699  [268800/826458]\n",
      "current batch loss: 0.677937  [275200/826458]\n",
      "current batch loss: 0.612330  [281600/826458]\n",
      "current batch loss: 0.680585  [288000/826458]\n",
      "current batch loss: 0.639738  [294400/826458]\n",
      "current batch loss: 0.606176  [300800/826458]\n",
      "current batch loss: 0.629483  [307200/826458]\n",
      "current batch loss: 0.658537  [313600/826458]\n",
      "current batch loss: 0.665196  [320000/826458]\n",
      "current batch loss: 0.669897  [326400/826458]\n",
      "current batch loss: 0.682862  [332800/826458]\n",
      "current batch loss: 0.618722  [339200/826458]\n",
      "current batch loss: 0.655631  [345600/826458]\n",
      "current batch loss: 0.609184  [352000/826458]\n",
      "current batch loss: 0.578573  [358400/826458]\n",
      "current batch loss: 0.678892  [364800/826458]\n",
      "current batch loss: 0.687596  [371200/826458]\n",
      "current batch loss: 0.666327  [377600/826458]\n",
      "current batch loss: 0.691029  [384000/826458]\n",
      "current batch loss: 0.670633  [390400/826458]\n",
      "current batch loss: 0.584705  [396800/826458]\n",
      "current batch loss: 0.633475  [403200/826458]\n",
      "current batch loss: 0.638607  [409600/826458]\n",
      "current batch loss: 0.646623  [416000/826458]\n",
      "current batch loss: 0.684734  [422400/826458]\n",
      "current batch loss: 0.637053  [428800/826458]\n",
      "current batch loss: 0.624654  [435200/826458]\n",
      "current batch loss: 0.667464  [441600/826458]\n",
      "current batch loss: 0.627687  [448000/826458]\n",
      "current batch loss: 0.631932  [454400/826458]\n",
      "current batch loss: 0.645067  [460800/826458]\n",
      "current batch loss: 0.662141  [467200/826458]\n",
      "current batch loss: 0.728881  [473600/826458]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.667762  [480000/826458]\n",
      "current batch loss: 0.740214  [486400/826458]\n",
      "current batch loss: 0.674839  [492800/826458]\n",
      "current batch loss: 0.560611  [499200/826458]\n",
      "current batch loss: 0.616275  [505600/826458]\n",
      "current batch loss: 0.689086  [512000/826458]\n",
      "current batch loss: 0.661538  [518400/826458]\n",
      "current batch loss: 0.608999  [524800/826458]\n",
      "current batch loss: 0.675478  [531200/826458]\n",
      "current batch loss: 0.664823  [537600/826458]\n",
      "current batch loss: 0.582446  [544000/826458]\n",
      "current batch loss: 0.623642  [550400/826458]\n",
      "current batch loss: 0.629904  [556800/826458]\n",
      "current batch loss: 0.628068  [563200/826458]\n",
      "current batch loss: 0.700459  [569600/826458]\n",
      "current batch loss: 0.613220  [576000/826458]\n",
      "current batch loss: 0.691125  [582400/826458]\n",
      "current batch loss: 0.663884  [588800/826458]\n",
      "current batch loss: 0.602766  [595200/826458]\n",
      "current batch loss: 0.693460  [601600/826458]\n",
      "current batch loss: 0.692691  [608000/826458]\n",
      "current batch loss: 0.675978  [614400/826458]\n",
      "current batch loss: 0.678175  [620800/826458]\n",
      "current batch loss: 0.656811  [627200/826458]\n",
      "current batch loss: 0.694280  [633600/826458]\n",
      "current batch loss: 0.595557  [640000/826458]\n",
      "current batch loss: 0.637054  [646400/826458]\n",
      "current batch loss: 0.640801  [652800/826458]\n",
      "current batch loss: 0.634875  [659200/826458]\n",
      "current batch loss: 0.695446  [665600/826458]\n",
      "current batch loss: 0.624353  [672000/826458]\n",
      "current batch loss: 0.654592  [678400/826458]\n",
      "current batch loss: 0.679669  [684800/826458]\n",
      "current batch loss: 0.653059  [691200/826458]\n",
      "current batch loss: 0.656106  [697600/826458]\n",
      "current batch loss: 0.659713  [704000/826458]\n",
      "current batch loss: 0.683905  [710400/826458]\n",
      "current batch loss: 0.631135  [716800/826458]\n",
      "current batch loss: 0.679956  [723200/826458]\n",
      "current batch loss: 0.608203  [729600/826458]\n",
      "current batch loss: 0.652488  [736000/826458]\n",
      "current batch loss: 0.747410  [742400/826458]\n",
      "current batch loss: 0.679384  [748800/826458]\n",
      "current batch loss: 0.630901  [755200/826458]\n",
      "current batch loss: 0.629688  [761600/826458]\n",
      "current batch loss: 0.679312  [768000/826458]\n",
      "current batch loss: 0.691892  [774400/826458]\n",
      "current batch loss: 0.682508  [780800/826458]\n",
      "current batch loss: 0.702566  [787200/826458]\n",
      "current batch loss: 0.695872  [793600/826458]\n",
      "current batch loss: 0.670999  [800000/826458]\n",
      "current batch loss: 0.633362  [806400/826458]\n",
      "current batch loss: 0.638699  [812800/826458]\n",
      "current batch loss: 0.673163  [819200/826458]\n",
      "current batch loss: 0.690437  [825600/826458]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.652932\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.652694\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 4\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.631671  [    0/826458]\n",
      "current batch loss: 0.621375  [ 6400/826458]\n",
      "current batch loss: 0.734186  [12800/826458]\n",
      "current batch loss: 0.655962  [19200/826458]\n",
      "current batch loss: 0.655739  [25600/826458]\n",
      "current batch loss: 0.628347  [32000/826458]\n",
      "current batch loss: 0.659530  [38400/826458]\n",
      "current batch loss: 0.603063  [44800/826458]\n",
      "current batch loss: 0.632188  [51200/826458]\n",
      "current batch loss: 0.541978  [57600/826458]\n",
      "current batch loss: 0.602367  [64000/826458]\n",
      "current batch loss: 0.665658  [70400/826458]\n",
      "current batch loss: 0.595869  [76800/826458]\n",
      "current batch loss: 0.614343  [83200/826458]\n",
      "current batch loss: 0.731185  [89600/826458]\n",
      "current batch loss: 0.664649  [96000/826458]\n",
      "current batch loss: 0.680572  [102400/826458]\n",
      "current batch loss: 0.669416  [108800/826458]\n",
      "current batch loss: 0.614268  [115200/826458]\n",
      "current batch loss: 0.692610  [121600/826458]\n",
      "current batch loss: 0.648578  [128000/826458]\n",
      "current batch loss: 0.679538  [134400/826458]\n",
      "current batch loss: 0.653265  [140800/826458]\n",
      "current batch loss: 0.681433  [147200/826458]\n",
      "current batch loss: 0.650458  [153600/826458]\n",
      "current batch loss: 0.664046  [160000/826458]\n",
      "current batch loss: 0.686251  [166400/826458]\n",
      "current batch loss: 0.634386  [172800/826458]\n",
      "current batch loss: 0.722732  [179200/826458]\n",
      "current batch loss: 0.676294  [185600/826458]\n",
      "current batch loss: 0.634286  [192000/826458]\n",
      "current batch loss: 0.610585  [198400/826458]\n",
      "current batch loss: 0.613072  [204800/826458]\n",
      "current batch loss: 0.607109  [211200/826458]\n",
      "current batch loss: 0.668278  [217600/826458]\n",
      "current batch loss: 0.648435  [224000/826458]\n",
      "current batch loss: 0.590046  [230400/826458]\n",
      "current batch loss: 0.621799  [236800/826458]\n",
      "current batch loss: 0.708761  [243200/826458]\n",
      "current batch loss: 0.715862  [249600/826458]\n",
      "current batch loss: 0.636146  [256000/826458]\n",
      "current batch loss: 0.652923  [262400/826458]\n",
      "current batch loss: 0.655797  [268800/826458]\n",
      "current batch loss: 0.639227  [275200/826458]\n",
      "current batch loss: 0.616260  [281600/826458]\n",
      "current batch loss: 0.633238  [288000/826458]\n",
      "current batch loss: 0.754297  [294400/826458]\n",
      "current batch loss: 0.639674  [300800/826458]\n",
      "current batch loss: 0.655961  [307200/826458]\n",
      "current batch loss: 0.616818  [313600/826458]\n",
      "current batch loss: 0.584390  [320000/826458]\n",
      "current batch loss: 0.657733  [326400/826458]\n",
      "current batch loss: 0.613337  [332800/826458]\n",
      "current batch loss: 0.616724  [339200/826458]\n",
      "current batch loss: 0.589761  [345600/826458]\n",
      "current batch loss: 0.685638  [352000/826458]\n",
      "current batch loss: 0.707213  [358400/826458]\n",
      "current batch loss: 0.716990  [364800/826458]\n",
      "current batch loss: 0.684726  [371200/826458]\n",
      "current batch loss: 0.661441  [377600/826458]\n",
      "current batch loss: 0.602994  [384000/826458]\n",
      "current batch loss: 0.674972  [390400/826458]\n",
      "current batch loss: 0.697156  [396800/826458]\n",
      "current batch loss: 0.609938  [403200/826458]\n",
      "current batch loss: 0.651214  [409600/826458]\n",
      "current batch loss: 0.617983  [416000/826458]\n",
      "current batch loss: 0.674581  [422400/826458]\n",
      "current batch loss: 0.662300  [428800/826458]\n",
      "current batch loss: 0.680497  [435200/826458]\n",
      "current batch loss: 0.602086  [441600/826458]\n",
      "current batch loss: 0.592771  [448000/826458]\n",
      "current batch loss: 0.663410  [454400/826458]\n",
      "current batch loss: 0.627527  [460800/826458]\n",
      "current batch loss: 0.673209  [467200/826458]\n",
      "current batch loss: 0.667270  [473600/826458]\n",
      "current batch loss: 0.634504  [480000/826458]\n",
      "current batch loss: 0.633245  [486400/826458]\n",
      "current batch loss: 0.658709  [492800/826458]\n",
      "current batch loss: 0.615165  [499200/826458]\n",
      "current batch loss: 0.594431  [505600/826458]\n",
      "current batch loss: 0.682184  [512000/826458]\n",
      "current batch loss: 0.602302  [518400/826458]\n",
      "current batch loss: 0.681269  [524800/826458]\n",
      "current batch loss: 0.584019  [531200/826458]\n",
      "current batch loss: 0.627061  [537600/826458]\n",
      "current batch loss: 0.688377  [544000/826458]\n",
      "current batch loss: 0.651957  [550400/826458]\n",
      "current batch loss: 0.697125  [556800/826458]\n",
      "current batch loss: 0.620736  [563200/826458]\n",
      "current batch loss: 0.691757  [569600/826458]\n",
      "current batch loss: 0.752371  [576000/826458]\n",
      "current batch loss: 0.727470  [582400/826458]\n",
      "current batch loss: 0.677381  [588800/826458]\n",
      "current batch loss: 0.688772  [595200/826458]\n",
      "current batch loss: 0.648396  [601600/826458]\n",
      "current batch loss: 0.714430  [608000/826458]\n",
      "current batch loss: 0.674879  [614400/826458]\n",
      "current batch loss: 0.677793  [620800/826458]\n",
      "current batch loss: 0.662247  [627200/826458]\n",
      "current batch loss: 0.630615  [633600/826458]\n",
      "current batch loss: 0.614029  [640000/826458]\n",
      "current batch loss: 0.682857  [646400/826458]\n",
      "current batch loss: 0.636502  [652800/826458]\n",
      "current batch loss: 0.690825  [659200/826458]\n",
      "current batch loss: 0.623335  [665600/826458]\n",
      "current batch loss: 0.644600  [672000/826458]\n",
      "current batch loss: 0.671841  [678400/826458]\n",
      "current batch loss: 0.653924  [684800/826458]\n",
      "current batch loss: 0.719345  [691200/826458]\n",
      "current batch loss: 0.667115  [697600/826458]\n",
      "current batch loss: 0.688507  [704000/826458]\n",
      "current batch loss: 0.627139  [710400/826458]\n",
      "current batch loss: 0.647471  [716800/826458]\n",
      "current batch loss: 0.670050  [723200/826458]\n",
      "current batch loss: 0.645599  [729600/826458]\n",
      "current batch loss: 0.582533  [736000/826458]\n",
      "current batch loss: 0.729261  [742400/826458]\n",
      "current batch loss: 0.659448  [748800/826458]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.624988  [755200/826458]\n",
      "current batch loss: 0.671056  [761600/826458]\n",
      "current batch loss: 0.592909  [768000/826458]\n",
      "current batch loss: 0.652118  [774400/826458]\n",
      "current batch loss: 0.620771  [780800/826458]\n",
      "current batch loss: 0.635960  [787200/826458]\n",
      "current batch loss: 0.718262  [793600/826458]\n",
      "current batch loss: 0.659640  [800000/826458]\n",
      "current batch loss: 0.683943  [806400/826458]\n",
      "current batch loss: 0.621394  [812800/826458]\n",
      "current batch loss: 0.666680  [819200/826458]\n",
      "current batch loss: 0.668537  [825600/826458]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.653126\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.652723\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 5\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.603484  [    0/826458]\n",
      "current batch loss: 0.688957  [ 6400/826458]\n",
      "current batch loss: 0.629629  [12800/826458]\n",
      "current batch loss: 0.633345  [19200/826458]\n",
      "current batch loss: 0.689743  [25600/826458]\n",
      "current batch loss: 0.616153  [32000/826458]\n",
      "current batch loss: 0.690175  [38400/826458]\n",
      "current batch loss: 0.649022  [44800/826458]\n",
      "current batch loss: 0.628041  [51200/826458]\n",
      "current batch loss: 0.631055  [57600/826458]\n",
      "current batch loss: 0.674707  [64000/826458]\n",
      "current batch loss: 0.641541  [70400/826458]\n",
      "current batch loss: 0.615069  [76800/826458]\n",
      "current batch loss: 0.607053  [83200/826458]\n",
      "current batch loss: 0.656972  [89600/826458]\n",
      "current batch loss: 0.680798  [96000/826458]\n",
      "current batch loss: 0.652782  [102400/826458]\n",
      "current batch loss: 0.634234  [108800/826458]\n",
      "current batch loss: 0.663042  [115200/826458]\n",
      "current batch loss: 0.674134  [121600/826458]\n",
      "current batch loss: 0.681849  [128000/826458]\n",
      "current batch loss: 0.612370  [134400/826458]\n",
      "current batch loss: 0.687790  [140800/826458]\n",
      "current batch loss: 0.635608  [147200/826458]\n",
      "current batch loss: 0.678361  [153600/826458]\n",
      "current batch loss: 0.631781  [160000/826458]\n",
      "current batch loss: 0.671842  [166400/826458]\n",
      "current batch loss: 0.576099  [172800/826458]\n",
      "current batch loss: 0.656489  [179200/826458]\n",
      "current batch loss: 0.702868  [185600/826458]\n",
      "current batch loss: 0.660896  [192000/826458]\n",
      "current batch loss: 0.614853  [198400/826458]\n",
      "current batch loss: 0.714134  [204800/826458]\n",
      "current batch loss: 0.585890  [211200/826458]\n",
      "current batch loss: 0.669704  [217600/826458]\n",
      "current batch loss: 0.719684  [224000/826458]\n",
      "current batch loss: 0.657931  [230400/826458]\n",
      "current batch loss: 0.610932  [236800/826458]\n",
      "current batch loss: 0.655425  [243200/826458]\n",
      "current batch loss: 0.606290  [249600/826458]\n",
      "current batch loss: 0.653960  [256000/826458]\n",
      "current batch loss: 0.664133  [262400/826458]\n",
      "current batch loss: 0.669581  [268800/826458]\n",
      "current batch loss: 0.653114  [275200/826458]\n",
      "current batch loss: 0.661773  [281600/826458]\n",
      "current batch loss: 0.681994  [288000/826458]\n",
      "current batch loss: 0.676222  [294400/826458]\n",
      "current batch loss: 0.607052  [300800/826458]\n",
      "current batch loss: 0.580241  [307200/826458]\n",
      "current batch loss: 0.663240  [313600/826458]\n",
      "current batch loss: 0.650588  [320000/826458]\n",
      "current batch loss: 0.612203  [326400/826458]\n",
      "current batch loss: 0.665701  [332800/826458]\n",
      "current batch loss: 0.648893  [339200/826458]\n",
      "current batch loss: 0.649008  [345600/826458]\n",
      "current batch loss: 0.638907  [352000/826458]\n",
      "current batch loss: 0.629698  [358400/826458]\n",
      "current batch loss: 0.639596  [364800/826458]\n",
      "current batch loss: 0.644708  [371200/826458]\n",
      "current batch loss: 0.596541  [377600/826458]\n",
      "current batch loss: 0.644852  [384000/826458]\n",
      "current batch loss: 0.705744  [390400/826458]\n",
      "current batch loss: 0.647230  [396800/826458]\n",
      "current batch loss: 0.674613  [403200/826458]\n",
      "current batch loss: 0.716570  [409600/826458]\n",
      "current batch loss: 0.641807  [416000/826458]\n",
      "current batch loss: 0.747523  [422400/826458]\n",
      "current batch loss: 0.639708  [428800/826458]\n",
      "current batch loss: 0.661324  [435200/826458]\n",
      "current batch loss: 0.659625  [441600/826458]\n",
      "current batch loss: 0.661343  [448000/826458]\n",
      "current batch loss: 0.682142  [454400/826458]\n",
      "current batch loss: 0.609260  [460800/826458]\n",
      "current batch loss: 0.593318  [467200/826458]\n",
      "current batch loss: 0.665765  [473600/826458]\n",
      "current batch loss: 0.665080  [480000/826458]\n",
      "current batch loss: 0.662062  [486400/826458]\n",
      "current batch loss: 0.649199  [492800/826458]\n",
      "current batch loss: 0.673427  [499200/826458]\n",
      "current batch loss: 0.637934  [505600/826458]\n",
      "current batch loss: 0.612004  [512000/826458]\n",
      "current batch loss: 0.587964  [518400/826458]\n",
      "current batch loss: 0.585248  [524800/826458]\n",
      "current batch loss: 0.641579  [531200/826458]\n",
      "current batch loss: 0.633130  [537600/826458]\n",
      "current batch loss: 0.709190  [544000/826458]\n",
      "current batch loss: 0.614097  [550400/826458]\n",
      "current batch loss: 0.616680  [556800/826458]\n",
      "current batch loss: 0.561794  [563200/826458]\n",
      "current batch loss: 0.699466  [569600/826458]\n",
      "current batch loss: 0.645431  [576000/826458]\n",
      "current batch loss: 0.605774  [582400/826458]\n",
      "current batch loss: 0.624710  [588800/826458]\n",
      "current batch loss: 0.603439  [595200/826458]\n",
      "current batch loss: 0.673356  [601600/826458]\n",
      "current batch loss: 0.685963  [608000/826458]\n",
      "current batch loss: 0.543065  [614400/826458]\n",
      "current batch loss: 0.686911  [620800/826458]\n",
      "current batch loss: 0.661133  [627200/826458]\n",
      "current batch loss: 0.672711  [633600/826458]\n",
      "current batch loss: 0.615411  [640000/826458]\n",
      "current batch loss: 0.607125  [646400/826458]\n",
      "current batch loss: 0.623865  [652800/826458]\n",
      "current batch loss: 0.706816  [659200/826458]\n",
      "current batch loss: 0.718549  [665600/826458]\n",
      "current batch loss: 0.665104  [672000/826458]\n",
      "current batch loss: 0.741385  [678400/826458]\n",
      "current batch loss: 0.616069  [684800/826458]\n",
      "current batch loss: 0.702026  [691200/826458]\n",
      "current batch loss: 0.649102  [697600/826458]\n",
      "current batch loss: 0.617674  [704000/826458]\n",
      "current batch loss: 0.691929  [710400/826458]\n",
      "current batch loss: 0.663445  [716800/826458]\n",
      "current batch loss: 0.628789  [723200/826458]\n",
      "current batch loss: 0.648690  [729600/826458]\n",
      "current batch loss: 0.644161  [736000/826458]\n",
      "current batch loss: 0.651516  [742400/826458]\n",
      "current batch loss: 0.671327  [748800/826458]\n",
      "current batch loss: 0.635011  [755200/826458]\n",
      "current batch loss: 0.657297  [761600/826458]\n",
      "current batch loss: 0.671883  [768000/826458]\n",
      "current batch loss: 0.612978  [774400/826458]\n",
      "current batch loss: 0.659722  [780800/826458]\n",
      "current batch loss: 0.683336  [787200/826458]\n",
      "current batch loss: 0.620817  [793600/826458]\n",
      "current batch loss: 0.660121  [800000/826458]\n",
      "current batch loss: 0.605601  [806400/826458]\n",
      "current batch loss: 0.609325  [812800/826458]\n",
      "current batch loss: 0.642305  [819200/826458]\n",
      "current batch loss: 0.627629  [825600/826458]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.652713\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.652709\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 6\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.668192  [    0/826458]\n",
      "current batch loss: 0.670237  [ 6400/826458]\n",
      "current batch loss: 0.645520  [12800/826458]\n",
      "current batch loss: 0.640941  [19200/826458]\n",
      "current batch loss: 0.682797  [25600/826458]\n",
      "current batch loss: 0.639890  [32000/826458]\n",
      "current batch loss: 0.641537  [38400/826458]\n",
      "current batch loss: 0.702755  [44800/826458]\n",
      "current batch loss: 0.676945  [51200/826458]\n",
      "current batch loss: 0.644356  [57600/826458]\n",
      "current batch loss: 0.642725  [64000/826458]\n",
      "current batch loss: 0.654855  [70400/826458]\n",
      "current batch loss: 0.661464  [76800/826458]\n",
      "current batch loss: 0.650751  [83200/826458]\n",
      "current batch loss: 0.667687  [89600/826458]\n",
      "current batch loss: 0.698352  [96000/826458]\n",
      "current batch loss: 0.643158  [102400/826458]\n",
      "current batch loss: 0.731648  [108800/826458]\n",
      "current batch loss: 0.654420  [115200/826458]\n",
      "current batch loss: 0.595149  [121600/826458]\n",
      "current batch loss: 0.679870  [128000/826458]\n",
      "current batch loss: 0.708901  [134400/826458]\n",
      "current batch loss: 0.633400  [140800/826458]\n",
      "current batch loss: 0.679683  [147200/826458]\n",
      "current batch loss: 0.647618  [153600/826458]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.708537  [160000/826458]\n",
      "current batch loss: 0.629578  [166400/826458]\n",
      "current batch loss: 0.587397  [172800/826458]\n",
      "current batch loss: 0.606974  [179200/826458]\n",
      "current batch loss: 0.622032  [185600/826458]\n",
      "current batch loss: 0.687147  [192000/826458]\n",
      "current batch loss: 0.634994  [198400/826458]\n",
      "current batch loss: 0.671359  [204800/826458]\n",
      "current batch loss: 0.720260  [211200/826458]\n",
      "current batch loss: 0.715289  [217600/826458]\n",
      "current batch loss: 0.641536  [224000/826458]\n",
      "current batch loss: 0.646813  [230400/826458]\n",
      "current batch loss: 0.645145  [236800/826458]\n",
      "current batch loss: 0.595571  [243200/826458]\n",
      "current batch loss: 0.664197  [249600/826458]\n",
      "current batch loss: 0.631216  [256000/826458]\n",
      "current batch loss: 0.651088  [262400/826458]\n",
      "current batch loss: 0.698652  [268800/826458]\n",
      "current batch loss: 0.688233  [275200/826458]\n",
      "current batch loss: 0.621939  [281600/826458]\n",
      "current batch loss: 0.582646  [288000/826458]\n",
      "current batch loss: 0.680941  [294400/826458]\n",
      "current batch loss: 0.591348  [300800/826458]\n",
      "current batch loss: 0.659373  [307200/826458]\n",
      "current batch loss: 0.653902  [313600/826458]\n",
      "current batch loss: 0.593688  [320000/826458]\n",
      "current batch loss: 0.689560  [326400/826458]\n",
      "current batch loss: 0.709155  [332800/826458]\n",
      "current batch loss: 0.628174  [339200/826458]\n",
      "current batch loss: 0.632232  [345600/826458]\n",
      "current batch loss: 0.677789  [352000/826458]\n",
      "current batch loss: 0.658038  [358400/826458]\n",
      "current batch loss: 0.627928  [364800/826458]\n",
      "current batch loss: 0.644020  [371200/826458]\n",
      "current batch loss: 0.611110  [377600/826458]\n",
      "current batch loss: 0.699438  [384000/826458]\n",
      "current batch loss: 0.695459  [390400/826458]\n",
      "current batch loss: 0.644311  [396800/826458]\n",
      "current batch loss: 0.671734  [403200/826458]\n",
      "current batch loss: 0.623864  [409600/826458]\n",
      "current batch loss: 0.636005  [416000/826458]\n",
      "current batch loss: 0.692482  [422400/826458]\n",
      "current batch loss: 0.644765  [428800/826458]\n",
      "current batch loss: 0.680884  [435200/826458]\n",
      "current batch loss: 0.644923  [441600/826458]\n",
      "current batch loss: 0.615426  [448000/826458]\n",
      "current batch loss: 0.630816  [454400/826458]\n",
      "current batch loss: 0.598987  [460800/826458]\n",
      "current batch loss: 0.664134  [467200/826458]\n",
      "current batch loss: 0.649532  [473600/826458]\n",
      "current batch loss: 0.695607  [480000/826458]\n",
      "current batch loss: 0.658216  [486400/826458]\n",
      "current batch loss: 0.634747  [492800/826458]\n",
      "current batch loss: 0.612116  [499200/826458]\n",
      "current batch loss: 0.662048  [505600/826458]\n",
      "current batch loss: 0.645697  [512000/826458]\n",
      "current batch loss: 0.647357  [518400/826458]\n",
      "current batch loss: 0.603952  [524800/826458]\n",
      "current batch loss: 0.695163  [531200/826458]\n",
      "current batch loss: 0.678784  [537600/826458]\n",
      "current batch loss: 0.682196  [544000/826458]\n",
      "current batch loss: 0.586445  [550400/826458]\n",
      "current batch loss: 0.656966  [556800/826458]\n",
      "current batch loss: 0.656414  [563200/826458]\n",
      "current batch loss: 0.651139  [569600/826458]\n",
      "current batch loss: 0.630994  [576000/826458]\n",
      "current batch loss: 0.707731  [582400/826458]\n",
      "current batch loss: 0.682993  [588800/826458]\n",
      "current batch loss: 0.622085  [595200/826458]\n",
      "current batch loss: 0.656205  [601600/826458]\n",
      "current batch loss: 0.661016  [608000/826458]\n",
      "current batch loss: 0.666970  [614400/826458]\n",
      "current batch loss: 0.610005  [620800/826458]\n",
      "current batch loss: 0.651186  [627200/826458]\n",
      "current batch loss: 0.694070  [633600/826458]\n",
      "current batch loss: 0.648829  [640000/826458]\n",
      "current batch loss: 0.655624  [646400/826458]\n",
      "current batch loss: 0.645159  [652800/826458]\n",
      "current batch loss: 0.641391  [659200/826458]\n",
      "current batch loss: 0.634002  [665600/826458]\n",
      "current batch loss: 0.661327  [672000/826458]\n",
      "current batch loss: 0.658102  [678400/826458]\n",
      "current batch loss: 0.725695  [684800/826458]\n",
      "current batch loss: 0.695912  [691200/826458]\n",
      "current batch loss: 0.673030  [697600/826458]\n",
      "current batch loss: 0.634238  [704000/826458]\n",
      "current batch loss: 0.633503  [710400/826458]\n",
      "current batch loss: 0.641924  [716800/826458]\n",
      "current batch loss: 0.572356  [723200/826458]\n",
      "current batch loss: 0.638088  [729600/826458]\n",
      "current batch loss: 0.640295  [736000/826458]\n",
      "current batch loss: 0.611863  [742400/826458]\n",
      "current batch loss: 0.614183  [748800/826458]\n",
      "current batch loss: 0.671233  [755200/826458]\n",
      "current batch loss: 0.635390  [761600/826458]\n",
      "current batch loss: 0.659226  [768000/826458]\n",
      "current batch loss: 0.687201  [774400/826458]\n",
      "current batch loss: 0.652317  [780800/826458]\n",
      "current batch loss: 0.640860  [787200/826458]\n",
      "current batch loss: 0.621113  [793600/826458]\n",
      "current batch loss: 0.598265  [800000/826458]\n",
      "current batch loss: 0.703706  [806400/826458]\n",
      "current batch loss: 0.616870  [812800/826458]\n",
      "current batch loss: 0.579948  [819200/826458]\n",
      "current batch loss: 0.602926  [825600/826458]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.652977\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.652605\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 7\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.608301  [    0/826458]\n",
      "current batch loss: 0.710997  [ 6400/826458]\n",
      "current batch loss: 0.697763  [12800/826458]\n",
      "current batch loss: 0.642659  [19200/826458]\n",
      "current batch loss: 0.629772  [25600/826458]\n",
      "current batch loss: 0.605300  [32000/826458]\n",
      "current batch loss: 0.698477  [38400/826458]\n",
      "current batch loss: 0.680374  [44800/826458]\n",
      "current batch loss: 0.660077  [51200/826458]\n",
      "current batch loss: 0.658256  [57600/826458]\n",
      "current batch loss: 0.681612  [64000/826458]\n",
      "current batch loss: 0.665775  [70400/826458]\n",
      "current batch loss: 0.730090  [76800/826458]\n",
      "current batch loss: 0.576382  [83200/826458]\n",
      "current batch loss: 0.636717  [89600/826458]\n",
      "current batch loss: 0.583982  [96000/826458]\n",
      "current batch loss: 0.692858  [102400/826458]\n",
      "current batch loss: 0.616398  [108800/826458]\n",
      "current batch loss: 0.636152  [115200/826458]\n",
      "current batch loss: 0.640408  [121600/826458]\n",
      "current batch loss: 0.646793  [128000/826458]\n",
      "current batch loss: 0.711439  [134400/826458]\n",
      "current batch loss: 0.605547  [140800/826458]\n",
      "current batch loss: 0.652182  [147200/826458]\n",
      "current batch loss: 0.644826  [153600/826458]\n",
      "current batch loss: 0.642536  [160000/826458]\n",
      "current batch loss: 0.653858  [166400/826458]\n",
      "current batch loss: 0.610156  [172800/826458]\n",
      "current batch loss: 0.617853  [179200/826458]\n",
      "current batch loss: 0.654191  [185600/826458]\n",
      "current batch loss: 0.610297  [192000/826458]\n",
      "current batch loss: 0.664676  [198400/826458]\n",
      "current batch loss: 0.673228  [204800/826458]\n",
      "current batch loss: 0.649729  [211200/826458]\n",
      "current batch loss: 0.637330  [217600/826458]\n",
      "current batch loss: 0.655886  [224000/826458]\n",
      "current batch loss: 0.602362  [230400/826458]\n",
      "current batch loss: 0.694556  [236800/826458]\n",
      "current batch loss: 0.645723  [243200/826458]\n",
      "current batch loss: 0.606112  [249600/826458]\n",
      "current batch loss: 0.627171  [256000/826458]\n",
      "current batch loss: 0.626438  [262400/826458]\n",
      "current batch loss: 0.659001  [268800/826458]\n",
      "current batch loss: 0.659877  [275200/826458]\n",
      "current batch loss: 0.649709  [281600/826458]\n",
      "current batch loss: 0.632597  [288000/826458]\n",
      "current batch loss: 0.674216  [294400/826458]\n",
      "current batch loss: 0.660877  [300800/826458]\n",
      "current batch loss: 0.595643  [307200/826458]\n",
      "current batch loss: 0.664957  [313600/826458]\n",
      "current batch loss: 0.624880  [320000/826458]\n",
      "current batch loss: 0.648885  [326400/826458]\n",
      "current batch loss: 0.566205  [332800/826458]\n",
      "current batch loss: 0.709649  [339200/826458]\n",
      "current batch loss: 0.701113  [345600/826458]\n",
      "current batch loss: 0.704042  [352000/826458]\n",
      "current batch loss: 0.608415  [358400/826458]\n",
      "current batch loss: 0.589047  [364800/826458]\n",
      "current batch loss: 0.650946  [371200/826458]\n",
      "current batch loss: 0.633347  [377600/826458]\n",
      "current batch loss: 0.654155  [384000/826458]\n",
      "current batch loss: 0.651601  [390400/826458]\n",
      "current batch loss: 0.636173  [396800/826458]\n",
      "current batch loss: 0.657729  [403200/826458]\n",
      "current batch loss: 0.642573  [409600/826458]\n",
      "current batch loss: 0.620719  [416000/826458]\n",
      "current batch loss: 0.676835  [422400/826458]\n",
      "current batch loss: 0.678332  [428800/826458]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.681282  [435200/826458]\n",
      "current batch loss: 0.644907  [441600/826458]\n",
      "current batch loss: 0.640919  [448000/826458]\n",
      "current batch loss: 0.644056  [454400/826458]\n",
      "current batch loss: 0.641005  [460800/826458]\n",
      "current batch loss: 0.626490  [467200/826458]\n",
      "current batch loss: 0.650528  [473600/826458]\n",
      "current batch loss: 0.635959  [480000/826458]\n",
      "current batch loss: 0.652627  [486400/826458]\n",
      "current batch loss: 0.736398  [492800/826458]\n",
      "current batch loss: 0.723159  [499200/826458]\n",
      "current batch loss: 0.659646  [505600/826458]\n",
      "current batch loss: 0.633560  [512000/826458]\n",
      "current batch loss: 0.636884  [518400/826458]\n",
      "current batch loss: 0.680562  [524800/826458]\n",
      "current batch loss: 0.686008  [531200/826458]\n",
      "current batch loss: 0.628560  [537600/826458]\n",
      "current batch loss: 0.643172  [544000/826458]\n",
      "current batch loss: 0.622736  [550400/826458]\n",
      "current batch loss: 0.624150  [556800/826458]\n",
      "current batch loss: 0.597231  [563200/826458]\n",
      "current batch loss: 0.628307  [569600/826458]\n",
      "current batch loss: 0.682139  [576000/826458]\n",
      "current batch loss: 0.697682  [582400/826458]\n",
      "current batch loss: 0.589081  [588800/826458]\n",
      "current batch loss: 0.614445  [595200/826458]\n",
      "current batch loss: 0.648659  [601600/826458]\n",
      "current batch loss: 0.664572  [608000/826458]\n",
      "current batch loss: 0.716588  [614400/826458]\n",
      "current batch loss: 0.566260  [620800/826458]\n",
      "current batch loss: 0.653796  [627200/826458]\n",
      "current batch loss: 0.620679  [633600/826458]\n",
      "current batch loss: 0.686836  [640000/826458]\n",
      "current batch loss: 0.666481  [646400/826458]\n",
      "current batch loss: 0.653685  [652800/826458]\n",
      "current batch loss: 0.629638  [659200/826458]\n",
      "current batch loss: 0.652398  [665600/826458]\n",
      "current batch loss: 0.644050  [672000/826458]\n",
      "current batch loss: 0.589629  [678400/826458]\n",
      "current batch loss: 0.686811  [684800/826458]\n",
      "current batch loss: 0.604819  [691200/826458]\n",
      "current batch loss: 0.613811  [697600/826458]\n",
      "current batch loss: 0.602346  [704000/826458]\n",
      "current batch loss: 0.706059  [710400/826458]\n",
      "current batch loss: 0.630581  [716800/826458]\n",
      "current batch loss: 0.718652  [723200/826458]\n",
      "current batch loss: 0.700494  [729600/826458]\n",
      "current batch loss: 0.578138  [736000/826458]\n",
      "current batch loss: 0.637445  [742400/826458]\n",
      "current batch loss: 0.618553  [748800/826458]\n",
      "current batch loss: 0.654115  [755200/826458]\n",
      "current batch loss: 0.649667  [761600/826458]\n",
      "current batch loss: 0.654893  [768000/826458]\n",
      "current batch loss: 0.634323  [774400/826458]\n",
      "current batch loss: 0.611669  [780800/826458]\n",
      "current batch loss: 0.651328  [787200/826458]\n",
      "current batch loss: 0.694227  [793600/826458]\n",
      "current batch loss: 0.606292  [800000/826458]\n",
      "current batch loss: 0.663161  [806400/826458]\n",
      "current batch loss: 0.666714  [812800/826458]\n",
      "current batch loss: 0.758280  [819200/826458]\n",
      "current batch loss: 0.616462  [825600/826458]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.652524\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.652335\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 8\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.662328  [    0/826458]\n",
      "current batch loss: 0.629480  [ 6400/826458]\n",
      "current batch loss: 0.627466  [12800/826458]\n",
      "current batch loss: 0.619272  [19200/826458]\n",
      "current batch loss: 0.686084  [25600/826458]\n",
      "current batch loss: 0.704635  [32000/826458]\n",
      "current batch loss: 0.652090  [38400/826458]\n",
      "current batch loss: 0.639713  [44800/826458]\n",
      "current batch loss: 0.639235  [51200/826458]\n",
      "current batch loss: 0.700123  [57600/826458]\n",
      "current batch loss: 0.678845  [64000/826458]\n",
      "current batch loss: 0.631852  [70400/826458]\n",
      "current batch loss: 0.659732  [76800/826458]\n",
      "current batch loss: 0.635843  [83200/826458]\n",
      "current batch loss: 0.618343  [89600/826458]\n",
      "current batch loss: 0.644425  [96000/826458]\n",
      "current batch loss: 0.615526  [102400/826458]\n",
      "current batch loss: 0.646277  [108800/826458]\n",
      "current batch loss: 0.704274  [115200/826458]\n",
      "current batch loss: 0.681168  [121600/826458]\n",
      "current batch loss: 0.641771  [128000/826458]\n",
      "current batch loss: 0.675308  [134400/826458]\n",
      "current batch loss: 0.607867  [140800/826458]\n",
      "current batch loss: 0.661195  [147200/826458]\n",
      "current batch loss: 0.645114  [153600/826458]\n",
      "current batch loss: 0.624183  [160000/826458]\n",
      "current batch loss: 0.631273  [166400/826458]\n",
      "current batch loss: 0.646145  [172800/826458]\n",
      "current batch loss: 0.593048  [179200/826458]\n",
      "current batch loss: 0.614832  [185600/826458]\n",
      "current batch loss: 0.646801  [192000/826458]\n",
      "current batch loss: 0.728910  [198400/826458]\n",
      "current batch loss: 0.617582  [204800/826458]\n",
      "current batch loss: 0.630184  [211200/826458]\n",
      "current batch loss: 0.726310  [217600/826458]\n",
      "current batch loss: 0.676596  [224000/826458]\n",
      "current batch loss: 0.642051  [230400/826458]\n",
      "current batch loss: 0.674193  [236800/826458]\n",
      "current batch loss: 0.658210  [243200/826458]\n",
      "current batch loss: 0.649776  [249600/826458]\n",
      "current batch loss: 0.646228  [256000/826458]\n",
      "current batch loss: 0.647524  [262400/826458]\n",
      "current batch loss: 0.632653  [268800/826458]\n",
      "current batch loss: 0.673300  [275200/826458]\n",
      "current batch loss: 0.696584  [281600/826458]\n",
      "current batch loss: 0.696292  [288000/826458]\n",
      "current batch loss: 0.589251  [294400/826458]\n",
      "current batch loss: 0.642592  [300800/826458]\n",
      "current batch loss: 0.628134  [307200/826458]\n",
      "current batch loss: 0.726800  [313600/826458]\n",
      "current batch loss: 0.641965  [320000/826458]\n",
      "current batch loss: 0.664442  [326400/826458]\n",
      "current batch loss: 0.652606  [332800/826458]\n",
      "current batch loss: 0.668767  [339200/826458]\n",
      "current batch loss: 0.682088  [345600/826458]\n",
      "current batch loss: 0.693183  [352000/826458]\n",
      "current batch loss: 0.638865  [358400/826458]\n",
      "current batch loss: 0.605314  [364800/826458]\n",
      "current batch loss: 0.663354  [371200/826458]\n",
      "current batch loss: 0.638510  [377600/826458]\n",
      "current batch loss: 0.640540  [384000/826458]\n",
      "current batch loss: 0.726166  [390400/826458]\n",
      "current batch loss: 0.697876  [396800/826458]\n",
      "current batch loss: 0.651457  [403200/826458]\n",
      "current batch loss: 0.617103  [409600/826458]\n",
      "current batch loss: 0.665809  [416000/826458]\n",
      "current batch loss: 0.644456  [422400/826458]\n",
      "current batch loss: 0.642574  [428800/826458]\n",
      "current batch loss: 0.641625  [435200/826458]\n",
      "current batch loss: 0.644286  [441600/826458]\n",
      "current batch loss: 0.670425  [448000/826458]\n",
      "current batch loss: 0.673024  [454400/826458]\n",
      "current batch loss: 0.678050  [460800/826458]\n",
      "current batch loss: 0.687974  [467200/826458]\n",
      "current batch loss: 0.669642  [473600/826458]\n",
      "current batch loss: 0.658007  [480000/826458]\n",
      "current batch loss: 0.667681  [486400/826458]\n",
      "current batch loss: 0.713474  [492800/826458]\n",
      "current batch loss: 0.681951  [499200/826458]\n",
      "current batch loss: 0.719634  [505600/826458]\n",
      "current batch loss: 0.633721  [512000/826458]\n",
      "current batch loss: 0.703014  [518400/826458]\n",
      "current batch loss: 0.678657  [524800/826458]\n",
      "current batch loss: 0.645338  [531200/826458]\n",
      "current batch loss: 0.628216  [537600/826458]\n",
      "current batch loss: 0.646815  [544000/826458]\n",
      "current batch loss: 0.642923  [550400/826458]\n",
      "current batch loss: 0.647791  [556800/826458]\n",
      "current batch loss: 0.606630  [563200/826458]\n",
      "current batch loss: 0.639422  [569600/826458]\n",
      "current batch loss: 0.599284  [576000/826458]\n",
      "current batch loss: 0.614362  [582400/826458]\n",
      "current batch loss: 0.638467  [588800/826458]\n",
      "current batch loss: 0.649601  [595200/826458]\n",
      "current batch loss: 0.682882  [601600/826458]\n",
      "current batch loss: 0.643730  [608000/826458]\n",
      "current batch loss: 0.685069  [614400/826458]\n",
      "current batch loss: 0.655262  [620800/826458]\n",
      "current batch loss: 0.601090  [627200/826458]\n",
      "current batch loss: 0.660380  [633600/826458]\n",
      "current batch loss: 0.748415  [640000/826458]\n",
      "current batch loss: 0.679954  [646400/826458]\n",
      "current batch loss: 0.616914  [652800/826458]\n",
      "current batch loss: 0.584054  [659200/826458]\n",
      "current batch loss: 0.648669  [665600/826458]\n",
      "current batch loss: 0.641006  [672000/826458]\n",
      "current batch loss: 0.655473  [678400/826458]\n",
      "current batch loss: 0.674399  [684800/826458]\n",
      "current batch loss: 0.633045  [691200/826458]\n",
      "current batch loss: 0.697484  [697600/826458]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.625623  [704000/826458]\n",
      "current batch loss: 0.651934  [710400/826458]\n",
      "current batch loss: 0.581252  [716800/826458]\n",
      "current batch loss: 0.632465  [723200/826458]\n",
      "current batch loss: 0.591198  [729600/826458]\n",
      "current batch loss: 0.678253  [736000/826458]\n",
      "current batch loss: 0.573295  [742400/826458]\n",
      "current batch loss: 0.712986  [748800/826458]\n",
      "current batch loss: 0.632812  [755200/826458]\n",
      "current batch loss: 0.687277  [761600/826458]\n",
      "current batch loss: 0.669723  [768000/826458]\n",
      "current batch loss: 0.602520  [774400/826458]\n",
      "current batch loss: 0.667909  [780800/826458]\n",
      "current batch loss: 0.682464  [787200/826458]\n",
      "current batch loss: 0.654933  [793600/826458]\n",
      "current batch loss: 0.692535  [800000/826458]\n",
      "current batch loss: 0.674984  [806400/826458]\n",
      "current batch loss: 0.638385  [812800/826458]\n",
      "current batch loss: 0.705876  [819200/826458]\n",
      "current batch loss: 0.679715  [825600/826458]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.652891\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.652932\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 9\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.638936  [    0/826458]\n",
      "current batch loss: 0.605612  [ 6400/826458]\n",
      "current batch loss: 0.639707  [12800/826458]\n",
      "current batch loss: 0.710992  [19200/826458]\n",
      "current batch loss: 0.616680  [25600/826458]\n",
      "current batch loss: 0.701348  [32000/826458]\n",
      "current batch loss: 0.632119  [38400/826458]\n",
      "current batch loss: 0.641927  [44800/826458]\n",
      "current batch loss: 0.660496  [51200/826458]\n",
      "current batch loss: 0.640543  [57600/826458]\n",
      "current batch loss: 0.649757  [64000/826458]\n",
      "current batch loss: 0.631221  [70400/826458]\n",
      "current batch loss: 0.684761  [76800/826458]\n",
      "current batch loss: 0.697395  [83200/826458]\n",
      "current batch loss: 0.665068  [89600/826458]\n",
      "current batch loss: 0.632136  [96000/826458]\n",
      "current batch loss: 0.708996  [102400/826458]\n",
      "current batch loss: 0.613498  [108800/826458]\n",
      "current batch loss: 0.644175  [115200/826458]\n",
      "current batch loss: 0.682600  [121600/826458]\n",
      "current batch loss: 0.670279  [128000/826458]\n",
      "current batch loss: 0.672071  [134400/826458]\n",
      "current batch loss: 0.615353  [140800/826458]\n",
      "current batch loss: 0.676685  [147200/826458]\n",
      "current batch loss: 0.640992  [153600/826458]\n",
      "current batch loss: 0.617560  [160000/826458]\n",
      "current batch loss: 0.680971  [166400/826458]\n",
      "current batch loss: 0.618539  [172800/826458]\n",
      "current batch loss: 0.672975  [179200/826458]\n",
      "current batch loss: 0.635532  [185600/826458]\n",
      "current batch loss: 0.635690  [192000/826458]\n",
      "current batch loss: 0.658646  [198400/826458]\n",
      "current batch loss: 0.605774  [204800/826458]\n",
      "current batch loss: 0.701576  [211200/826458]\n",
      "current batch loss: 0.666740  [217600/826458]\n",
      "current batch loss: 0.587153  [224000/826458]\n",
      "current batch loss: 0.692162  [230400/826458]\n",
      "current batch loss: 0.609991  [236800/826458]\n",
      "current batch loss: 0.659293  [243200/826458]\n",
      "current batch loss: 0.624774  [249600/826458]\n",
      "current batch loss: 0.660482  [256000/826458]\n",
      "current batch loss: 0.596611  [262400/826458]\n",
      "current batch loss: 0.649697  [268800/826458]\n",
      "current batch loss: 0.697093  [275200/826458]\n",
      "current batch loss: 0.639070  [281600/826458]\n",
      "current batch loss: 0.610402  [288000/826458]\n",
      "current batch loss: 0.664710  [294400/826458]\n",
      "current batch loss: 0.690270  [300800/826458]\n",
      "current batch loss: 0.626928  [307200/826458]\n",
      "current batch loss: 0.701226  [313600/826458]\n",
      "current batch loss: 0.624464  [320000/826458]\n",
      "current batch loss: 0.678124  [326400/826458]\n",
      "current batch loss: 0.635213  [332800/826458]\n",
      "current batch loss: 0.662595  [339200/826458]\n",
      "current batch loss: 0.648268  [345600/826458]\n",
      "current batch loss: 0.658272  [352000/826458]\n",
      "current batch loss: 0.616728  [358400/826458]\n",
      "current batch loss: 0.597527  [364800/826458]\n",
      "current batch loss: 0.635854  [371200/826458]\n",
      "current batch loss: 0.574289  [377600/826458]\n",
      "current batch loss: 0.648481  [384000/826458]\n",
      "current batch loss: 0.647735  [390400/826458]\n",
      "current batch loss: 0.628379  [396800/826458]\n",
      "current batch loss: 0.718589  [403200/826458]\n",
      "current batch loss: 0.629253  [409600/826458]\n",
      "current batch loss: 0.643912  [416000/826458]\n",
      "current batch loss: 0.667521  [422400/826458]\n",
      "current batch loss: 0.610638  [428800/826458]\n",
      "current batch loss: 0.652491  [435200/826458]\n",
      "current batch loss: 0.647013  [441600/826458]\n",
      "current batch loss: 0.653233  [448000/826458]\n",
      "current batch loss: 0.702729  [454400/826458]\n",
      "current batch loss: 0.676092  [460800/826458]\n",
      "current batch loss: 0.601874  [467200/826458]\n",
      "current batch loss: 0.651099  [473600/826458]\n",
      "current batch loss: 0.645915  [480000/826458]\n",
      "current batch loss: 0.658092  [486400/826458]\n",
      "current batch loss: 0.565724  [492800/826458]\n",
      "current batch loss: 0.631418  [499200/826458]\n",
      "current batch loss: 0.694901  [505600/826458]\n",
      "current batch loss: 0.662121  [512000/826458]\n",
      "current batch loss: 0.651973  [518400/826458]\n",
      "current batch loss: 0.662705  [524800/826458]\n",
      "current batch loss: 0.614936  [531200/826458]\n",
      "current batch loss: 0.616713  [537600/826458]\n",
      "current batch loss: 0.683638  [544000/826458]\n",
      "current batch loss: 0.657516  [550400/826458]\n",
      "current batch loss: 0.647673  [556800/826458]\n",
      "current batch loss: 0.666510  [563200/826458]\n",
      "current batch loss: 0.692491  [569600/826458]\n",
      "current batch loss: 0.648068  [576000/826458]\n",
      "current batch loss: 0.644064  [582400/826458]\n",
      "current batch loss: 0.638447  [588800/826458]\n",
      "current batch loss: 0.697700  [595200/826458]\n",
      "current batch loss: 0.649191  [601600/826458]\n",
      "current batch loss: 0.642673  [608000/826458]\n",
      "current batch loss: 0.690279  [614400/826458]\n",
      "current batch loss: 0.586683  [620800/826458]\n",
      "current batch loss: 0.605971  [627200/826458]\n",
      "current batch loss: 0.591222  [633600/826458]\n",
      "current batch loss: 0.573940  [640000/826458]\n",
      "current batch loss: 0.710578  [646400/826458]\n",
      "current batch loss: 0.654111  [652800/826458]\n",
      "current batch loss: 0.656068  [659200/826458]\n",
      "current batch loss: 0.700286  [665600/826458]\n",
      "current batch loss: 0.694617  [672000/826458]\n",
      "current batch loss: 0.652152  [678400/826458]\n",
      "current batch loss: 0.673851  [684800/826458]\n",
      "current batch loss: 0.705604  [691200/826458]\n",
      "current batch loss: 0.656263  [697600/826458]\n",
      "current batch loss: 0.662013  [704000/826458]\n",
      "current batch loss: 0.645857  [710400/826458]\n",
      "current batch loss: 0.664543  [716800/826458]\n",
      "current batch loss: 0.669523  [723200/826458]\n",
      "current batch loss: 0.684432  [729600/826458]\n",
      "current batch loss: 0.653893  [736000/826458]\n",
      "current batch loss: 0.681974  [742400/826458]\n",
      "current batch loss: 0.602724  [748800/826458]\n",
      "current batch loss: 0.662283  [755200/826458]\n",
      "current batch loss: 0.703085  [761600/826458]\n",
      "current batch loss: 0.663353  [768000/826458]\n",
      "current batch loss: 0.604340  [774400/826458]\n",
      "current batch loss: 0.679327  [780800/826458]\n",
      "current batch loss: 0.645591  [787200/826458]\n",
      "current batch loss: 0.648332  [793600/826458]\n",
      "current batch loss: 0.675515  [800000/826458]\n",
      "current batch loss: 0.598393  [806400/826458]\n",
      "current batch loss: 0.648164  [812800/826458]\n",
      "current batch loss: 0.615099  [819200/826458]\n",
      "current batch loss: 0.624158  [825600/826458]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.652585\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.652619\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 10\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.650704  [    0/826458]\n",
      "current batch loss: 0.643419  [ 6400/826458]\n",
      "current batch loss: 0.584725  [12800/826458]\n",
      "current batch loss: 0.633832  [19200/826458]\n",
      "current batch loss: 0.661904  [25600/826458]\n",
      "current batch loss: 0.668208  [32000/826458]\n",
      "current batch loss: 0.685997  [38400/826458]\n",
      "current batch loss: 0.653588  [44800/826458]\n",
      "current batch loss: 0.697069  [51200/826458]\n",
      "current batch loss: 0.653308  [57600/826458]\n",
      "current batch loss: 0.676041  [64000/826458]\n",
      "current batch loss: 0.603096  [70400/826458]\n",
      "current batch loss: 0.646562  [76800/826458]\n",
      "current batch loss: 0.704326  [83200/826458]\n",
      "current batch loss: 0.649463  [89600/826458]\n",
      "current batch loss: 0.610457  [96000/826458]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.615671  [102400/826458]\n",
      "current batch loss: 0.686460  [108800/826458]\n",
      "current batch loss: 0.648471  [115200/826458]\n",
      "current batch loss: 0.655054  [121600/826458]\n",
      "current batch loss: 0.667401  [128000/826458]\n",
      "current batch loss: 0.661458  [134400/826458]\n",
      "current batch loss: 0.670761  [140800/826458]\n",
      "current batch loss: 0.663711  [147200/826458]\n",
      "current batch loss: 0.683231  [153600/826458]\n",
      "current batch loss: 0.690483  [160000/826458]\n",
      "current batch loss: 0.672880  [166400/826458]\n",
      "current batch loss: 0.659813  [172800/826458]\n",
      "current batch loss: 0.707330  [179200/826458]\n",
      "current batch loss: 0.673304  [185600/826458]\n",
      "current batch loss: 0.652382  [192000/826458]\n",
      "current batch loss: 0.624956  [198400/826458]\n",
      "current batch loss: 0.629452  [204800/826458]\n",
      "current batch loss: 0.683522  [211200/826458]\n",
      "current batch loss: 0.653739  [217600/826458]\n",
      "current batch loss: 0.642421  [224000/826458]\n",
      "current batch loss: 0.647797  [230400/826458]\n",
      "current batch loss: 0.687400  [236800/826458]\n",
      "current batch loss: 0.633020  [243200/826458]\n",
      "current batch loss: 0.617571  [249600/826458]\n",
      "current batch loss: 0.682727  [256000/826458]\n",
      "current batch loss: 0.676834  [262400/826458]\n",
      "current batch loss: 0.582448  [268800/826458]\n",
      "current batch loss: 0.635021  [275200/826458]\n",
      "current batch loss: 0.637273  [281600/826458]\n",
      "current batch loss: 0.571072  [288000/826458]\n",
      "current batch loss: 0.699235  [294400/826458]\n",
      "current batch loss: 0.788250  [300800/826458]\n",
      "current batch loss: 0.671367  [307200/826458]\n",
      "current batch loss: 0.707097  [313600/826458]\n",
      "current batch loss: 0.685546  [320000/826458]\n",
      "current batch loss: 0.634359  [326400/826458]\n",
      "current batch loss: 0.639261  [332800/826458]\n",
      "current batch loss: 0.645094  [339200/826458]\n",
      "current batch loss: 0.694973  [345600/826458]\n",
      "current batch loss: 0.687057  [352000/826458]\n",
      "current batch loss: 0.685370  [358400/826458]\n",
      "current batch loss: 0.677276  [364800/826458]\n",
      "current batch loss: 0.642335  [371200/826458]\n",
      "current batch loss: 0.721124  [377600/826458]\n",
      "current batch loss: 0.649295  [384000/826458]\n",
      "current batch loss: 0.639277  [390400/826458]\n",
      "current batch loss: 0.619185  [396800/826458]\n",
      "current batch loss: 0.622809  [403200/826458]\n",
      "current batch loss: 0.669748  [409600/826458]\n",
      "current batch loss: 0.588206  [416000/826458]\n",
      "current batch loss: 0.602943  [422400/826458]\n",
      "current batch loss: 0.645165  [428800/826458]\n",
      "current batch loss: 0.647310  [435200/826458]\n",
      "current batch loss: 0.702044  [441600/826458]\n",
      "current batch loss: 0.660627  [448000/826458]\n",
      "current batch loss: 0.681775  [454400/826458]\n",
      "current batch loss: 0.691353  [460800/826458]\n",
      "current batch loss: 0.624544  [467200/826458]\n",
      "current batch loss: 0.654591  [473600/826458]\n",
      "current batch loss: 0.662117  [480000/826458]\n",
      "current batch loss: 0.604811  [486400/826458]\n",
      "current batch loss: 0.644075  [492800/826458]\n",
      "current batch loss: 0.652243  [499200/826458]\n",
      "current batch loss: 0.644253  [505600/826458]\n",
      "current batch loss: 0.699005  [512000/826458]\n",
      "current batch loss: 0.610314  [518400/826458]\n",
      "current batch loss: 0.668830  [524800/826458]\n",
      "current batch loss: 0.664293  [531200/826458]\n",
      "current batch loss: 0.621358  [537600/826458]\n",
      "current batch loss: 0.663620  [544000/826458]\n",
      "current batch loss: 0.681476  [550400/826458]\n",
      "current batch loss: 0.684581  [556800/826458]\n",
      "current batch loss: 0.703874  [563200/826458]\n",
      "current batch loss: 0.616988  [569600/826458]\n",
      "current batch loss: 0.580441  [576000/826458]\n",
      "current batch loss: 0.649529  [582400/826458]\n",
      "current batch loss: 0.640669  [588800/826458]\n",
      "current batch loss: 0.670016  [595200/826458]\n",
      "current batch loss: 0.617465  [601600/826458]\n",
      "current batch loss: 0.664217  [608000/826458]\n",
      "current batch loss: 0.632257  [614400/826458]\n",
      "current batch loss: 0.702407  [620800/826458]\n",
      "current batch loss: 0.643817  [627200/826458]\n",
      "current batch loss: 0.618290  [633600/826458]\n",
      "current batch loss: 0.617526  [640000/826458]\n",
      "current batch loss: 0.656710  [646400/826458]\n",
      "current batch loss: 0.640952  [652800/826458]\n",
      "current batch loss: 0.627271  [659200/826458]\n",
      "current batch loss: 0.655823  [665600/826458]\n",
      "current batch loss: 0.659354  [672000/826458]\n",
      "current batch loss: 0.621674  [678400/826458]\n",
      "current batch loss: 0.670518  [684800/826458]\n",
      "current batch loss: 0.665647  [691200/826458]\n",
      "current batch loss: 0.653911  [697600/826458]\n",
      "current batch loss: 0.718497  [704000/826458]\n",
      "current batch loss: 0.619807  [710400/826458]\n",
      "current batch loss: 0.639793  [716800/826458]\n",
      "current batch loss: 0.626706  [723200/826458]\n",
      "current batch loss: 0.636185  [729600/826458]\n",
      "current batch loss: 0.591009  [736000/826458]\n",
      "current batch loss: 0.581000  [742400/826458]\n",
      "current batch loss: 0.676725  [748800/826458]\n",
      "current batch loss: 0.641782  [755200/826458]\n",
      "current batch loss: 0.626875  [761600/826458]\n",
      "current batch loss: 0.658193  [768000/826458]\n",
      "current batch loss: 0.604929  [774400/826458]\n",
      "current batch loss: 0.677543  [780800/826458]\n",
      "current batch loss: 0.607703  [787200/826458]\n",
      "current batch loss: 0.707583  [793600/826458]\n",
      "current batch loss: 0.669089  [800000/826458]\n",
      "current batch loss: 0.668936  [806400/826458]\n",
      "current batch loss: 0.647970  [812800/826458]\n",
      "current batch loss: 0.687158  [819200/826458]\n",
      "current batch loss: 0.629919  [825600/826458]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.652544\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.652376\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 11\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.614779  [    0/826458]\n",
      "current batch loss: 0.639637  [ 6400/826458]\n",
      "current batch loss: 0.700401  [12800/826458]\n",
      "current batch loss: 0.630062  [19200/826458]\n",
      "current batch loss: 0.669311  [25600/826458]\n",
      "current batch loss: 0.585383  [32000/826458]\n",
      "current batch loss: 0.670264  [38400/826458]\n",
      "current batch loss: 0.697307  [44800/826458]\n",
      "current batch loss: 0.685225  [51200/826458]\n",
      "current batch loss: 0.656133  [57600/826458]\n",
      "current batch loss: 0.594409  [64000/826458]\n",
      "current batch loss: 0.645714  [70400/826458]\n",
      "current batch loss: 0.595912  [76800/826458]\n",
      "current batch loss: 0.641122  [83200/826458]\n",
      "current batch loss: 0.664137  [89600/826458]\n",
      "current batch loss: 0.659729  [96000/826458]\n",
      "current batch loss: 0.594394  [102400/826458]\n",
      "current batch loss: 0.686894  [108800/826458]\n",
      "current batch loss: 0.646745  [115200/826458]\n",
      "current batch loss: 0.631196  [121600/826458]\n",
      "current batch loss: 0.647964  [128000/826458]\n",
      "current batch loss: 0.637642  [134400/826458]\n",
      "current batch loss: 0.629668  [140800/826458]\n",
      "current batch loss: 0.627105  [147200/826458]\n",
      "current batch loss: 0.630629  [153600/826458]\n",
      "current batch loss: 0.598573  [160000/826458]\n",
      "current batch loss: 0.631867  [166400/826458]\n",
      "current batch loss: 0.683830  [172800/826458]\n",
      "current batch loss: 0.580835  [179200/826458]\n",
      "current batch loss: 0.653779  [185600/826458]\n",
      "current batch loss: 0.683384  [192000/826458]\n",
      "current batch loss: 0.678022  [198400/826458]\n",
      "current batch loss: 0.761123  [204800/826458]\n",
      "current batch loss: 0.643324  [211200/826458]\n",
      "current batch loss: 0.661283  [217600/826458]\n",
      "current batch loss: 0.655563  [224000/826458]\n",
      "current batch loss: 0.625747  [230400/826458]\n",
      "current batch loss: 0.669328  [236800/826458]\n",
      "current batch loss: 0.594614  [243200/826458]\n",
      "current batch loss: 0.708598  [249600/826458]\n",
      "current batch loss: 0.690550  [256000/826458]\n",
      "current batch loss: 0.641891  [262400/826458]\n",
      "current batch loss: 0.652208  [268800/826458]\n",
      "current batch loss: 0.621511  [275200/826458]\n",
      "current batch loss: 0.713549  [281600/826458]\n",
      "current batch loss: 0.742394  [288000/826458]\n",
      "current batch loss: 0.670289  [294400/826458]\n",
      "current batch loss: 0.622141  [300800/826458]\n",
      "current batch loss: 0.664979  [307200/826458]\n",
      "current batch loss: 0.674018  [313600/826458]\n",
      "current batch loss: 0.653831  [320000/826458]\n",
      "current batch loss: 0.627345  [326400/826458]\n",
      "current batch loss: 0.632424  [332800/826458]\n",
      "current batch loss: 0.660664  [339200/826458]\n",
      "current batch loss: 0.714258  [345600/826458]\n",
      "current batch loss: 0.686035  [352000/826458]\n",
      "current batch loss: 0.621530  [358400/826458]\n",
      "current batch loss: 0.595900  [364800/826458]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.650436  [371200/826458]\n",
      "current batch loss: 0.653019  [377600/826458]\n",
      "current batch loss: 0.643972  [384000/826458]\n",
      "current batch loss: 0.684560  [390400/826458]\n",
      "current batch loss: 0.577478  [396800/826458]\n",
      "current batch loss: 0.586054  [403200/826458]\n",
      "current batch loss: 0.638045  [409600/826458]\n",
      "current batch loss: 0.667089  [416000/826458]\n",
      "current batch loss: 0.649708  [422400/826458]\n",
      "current batch loss: 0.648542  [428800/826458]\n",
      "current batch loss: 0.671539  [435200/826458]\n",
      "current batch loss: 0.572338  [441600/826458]\n",
      "current batch loss: 0.669608  [448000/826458]\n",
      "current batch loss: 0.665999  [454400/826458]\n",
      "current batch loss: 0.636233  [460800/826458]\n",
      "current batch loss: 0.633263  [467200/826458]\n",
      "current batch loss: 0.703304  [473600/826458]\n",
      "current batch loss: 0.687370  [480000/826458]\n",
      "current batch loss: 0.652935  [486400/826458]\n",
      "current batch loss: 0.636478  [492800/826458]\n",
      "current batch loss: 0.593052  [499200/826458]\n",
      "current batch loss: 0.711970  [505600/826458]\n",
      "current batch loss: 0.680157  [512000/826458]\n",
      "current batch loss: 0.695874  [518400/826458]\n",
      "current batch loss: 0.622293  [524800/826458]\n",
      "current batch loss: 0.665506  [531200/826458]\n",
      "current batch loss: 0.623899  [537600/826458]\n",
      "current batch loss: 0.607581  [544000/826458]\n",
      "current batch loss: 0.633829  [550400/826458]\n",
      "current batch loss: 0.617513  [556800/826458]\n",
      "current batch loss: 0.609594  [563200/826458]\n",
      "current batch loss: 0.643030  [569600/826458]\n",
      "current batch loss: 0.699058  [576000/826458]\n",
      "current batch loss: 0.644462  [582400/826458]\n",
      "current batch loss: 0.686335  [588800/826458]\n",
      "current batch loss: 0.682611  [595200/826458]\n",
      "current batch loss: 0.649480  [601600/826458]\n",
      "current batch loss: 0.689314  [608000/826458]\n",
      "current batch loss: 0.696027  [614400/826458]\n",
      "current batch loss: 0.597241  [620800/826458]\n",
      "current batch loss: 0.599480  [627200/826458]\n",
      "current batch loss: 0.630435  [633600/826458]\n",
      "current batch loss: 0.637381  [640000/826458]\n",
      "current batch loss: 0.647011  [646400/826458]\n",
      "current batch loss: 0.576458  [652800/826458]\n",
      "current batch loss: 0.641225  [659200/826458]\n",
      "current batch loss: 0.688569  [665600/826458]\n",
      "current batch loss: 0.621470  [672000/826458]\n",
      "current batch loss: 0.656380  [678400/826458]\n",
      "current batch loss: 0.686106  [684800/826458]\n",
      "current batch loss: 0.635117  [691200/826458]\n",
      "current batch loss: 0.614872  [697600/826458]\n",
      "current batch loss: 0.698367  [704000/826458]\n",
      "current batch loss: 0.658088  [710400/826458]\n",
      "current batch loss: 0.657828  [716800/826458]\n",
      "current batch loss: 0.652052  [723200/826458]\n",
      "current batch loss: 0.633437  [729600/826458]\n",
      "current batch loss: 0.664322  [736000/826458]\n",
      "current batch loss: 0.706191  [742400/826458]\n",
      "current batch loss: 0.580268  [748800/826458]\n",
      "current batch loss: 0.652083  [755200/826458]\n",
      "current batch loss: 0.630545  [761600/826458]\n",
      "current batch loss: 0.719401  [768000/826458]\n",
      "current batch loss: 0.716886  [774400/826458]\n",
      "current batch loss: 0.607872  [780800/826458]\n",
      "current batch loss: 0.615614  [787200/826458]\n",
      "current batch loss: 0.679649  [793600/826458]\n",
      "current batch loss: 0.639303  [800000/826458]\n",
      "current batch loss: 0.650632  [806400/826458]\n",
      "current batch loss: 0.631811  [812800/826458]\n",
      "current batch loss: 0.650080  [819200/826458]\n",
      "current batch loss: 0.661678  [825600/826458]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.653111\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.653183\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 12\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.638557  [    0/826458]\n",
      "current batch loss: 0.650015  [ 6400/826458]\n",
      "current batch loss: 0.649827  [12800/826458]\n",
      "current batch loss: 0.711215  [19200/826458]\n",
      "current batch loss: 0.659513  [25600/826458]\n",
      "current batch loss: 0.626177  [32000/826458]\n",
      "current batch loss: 0.635699  [38400/826458]\n",
      "current batch loss: 0.713602  [44800/826458]\n",
      "current batch loss: 0.642610  [51200/826458]\n",
      "current batch loss: 0.679129  [57600/826458]\n",
      "current batch loss: 0.703838  [64000/826458]\n",
      "current batch loss: 0.670271  [70400/826458]\n",
      "current batch loss: 0.650049  [76800/826458]\n",
      "current batch loss: 0.658364  [83200/826458]\n",
      "current batch loss: 0.635758  [89600/826458]\n",
      "current batch loss: 0.698789  [96000/826458]\n",
      "current batch loss: 0.612697  [102400/826458]\n",
      "current batch loss: 0.618742  [108800/826458]\n",
      "current batch loss: 0.673590  [115200/826458]\n",
      "current batch loss: 0.623312  [121600/826458]\n",
      "current batch loss: 0.605363  [128000/826458]\n",
      "current batch loss: 0.605347  [134400/826458]\n",
      "current batch loss: 0.615869  [140800/826458]\n",
      "current batch loss: 0.680327  [147200/826458]\n",
      "current batch loss: 0.642569  [153600/826458]\n",
      "current batch loss: 0.622686  [160000/826458]\n",
      "current batch loss: 0.653242  [166400/826458]\n",
      "current batch loss: 0.651482  [172800/826458]\n",
      "current batch loss: 0.651336  [179200/826458]\n",
      "current batch loss: 0.580804  [185600/826458]\n",
      "current batch loss: 0.658902  [192000/826458]\n",
      "current batch loss: 0.596140  [198400/826458]\n",
      "current batch loss: 0.735284  [204800/826458]\n",
      "current batch loss: 0.652007  [211200/826458]\n",
      "current batch loss: 0.706166  [217600/826458]\n",
      "current batch loss: 0.724550  [224000/826458]\n",
      "current batch loss: 0.590433  [230400/826458]\n",
      "current batch loss: 0.669812  [236800/826458]\n",
      "current batch loss: 0.672005  [243200/826458]\n",
      "current batch loss: 0.619099  [249600/826458]\n",
      "current batch loss: 0.634968  [256000/826458]\n",
      "current batch loss: 0.686119  [262400/826458]\n",
      "current batch loss: 0.643536  [268800/826458]\n",
      "current batch loss: 0.703445  [275200/826458]\n",
      "current batch loss: 0.706934  [281600/826458]\n",
      "current batch loss: 0.655388  [288000/826458]\n",
      "current batch loss: 0.715557  [294400/826458]\n",
      "current batch loss: 0.672398  [300800/826458]\n",
      "current batch loss: 0.624605  [307200/826458]\n",
      "current batch loss: 0.657486  [313600/826458]\n",
      "current batch loss: 0.709324  [320000/826458]\n",
      "current batch loss: 0.719041  [326400/826458]\n",
      "current batch loss: 0.572126  [332800/826458]\n",
      "current batch loss: 0.657537  [339200/826458]\n",
      "current batch loss: 0.662775  [345600/826458]\n",
      "current batch loss: 0.727490  [352000/826458]\n",
      "current batch loss: 0.582345  [358400/826458]\n",
      "current batch loss: 0.655798  [364800/826458]\n",
      "current batch loss: 0.613803  [371200/826458]\n",
      "current batch loss: 0.646422  [377600/826458]\n",
      "current batch loss: 0.674540  [384000/826458]\n",
      "current batch loss: 0.633464  [390400/826458]\n",
      "current batch loss: 0.631329  [396800/826458]\n",
      "current batch loss: 0.688134  [403200/826458]\n",
      "current batch loss: 0.629838  [409600/826458]\n",
      "current batch loss: 0.698074  [416000/826458]\n",
      "current batch loss: 0.640138  [422400/826458]\n",
      "current batch loss: 0.662416  [428800/826458]\n",
      "current batch loss: 0.689093  [435200/826458]\n",
      "current batch loss: 0.630591  [441600/826458]\n",
      "current batch loss: 0.659311  [448000/826458]\n",
      "current batch loss: 0.625629  [454400/826458]\n",
      "current batch loss: 0.627714  [460800/826458]\n",
      "current batch loss: 0.661998  [467200/826458]\n",
      "current batch loss: 0.682040  [473600/826458]\n",
      "current batch loss: 0.586196  [480000/826458]\n",
      "current batch loss: 0.631797  [486400/826458]\n",
      "current batch loss: 0.656429  [492800/826458]\n",
      "current batch loss: 0.621810  [499200/826458]\n",
      "current batch loss: 0.629557  [505600/826458]\n",
      "current batch loss: 0.658736  [512000/826458]\n",
      "current batch loss: 0.682057  [518400/826458]\n",
      "current batch loss: 0.684882  [524800/826458]\n",
      "current batch loss: 0.646220  [531200/826458]\n",
      "current batch loss: 0.697935  [537600/826458]\n",
      "current batch loss: 0.660636  [544000/826458]\n",
      "current batch loss: 0.656836  [550400/826458]\n",
      "current batch loss: 0.629666  [556800/826458]\n",
      "current batch loss: 0.615108  [563200/826458]\n",
      "current batch loss: 0.633997  [569600/826458]\n",
      "current batch loss: 0.634935  [576000/826458]\n",
      "current batch loss: 0.585885  [582400/826458]\n",
      "current batch loss: 0.622194  [588800/826458]\n",
      "current batch loss: 0.567045  [595200/826458]\n",
      "current batch loss: 0.644260  [601600/826458]\n",
      "current batch loss: 0.670733  [608000/826458]\n",
      "current batch loss: 0.649688  [614400/826458]\n",
      "current batch loss: 0.638806  [620800/826458]\n",
      "current batch loss: 0.606642  [627200/826458]\n",
      "current batch loss: 0.634803  [633600/826458]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.567650  [640000/826458]\n",
      "current batch loss: 0.679799  [646400/826458]\n",
      "current batch loss: 0.611875  [652800/826458]\n",
      "current batch loss: 0.662382  [659200/826458]\n",
      "current batch loss: 0.694146  [665600/826458]\n",
      "current batch loss: 0.684326  [672000/826458]\n",
      "current batch loss: 0.685454  [678400/826458]\n",
      "current batch loss: 0.710573  [684800/826458]\n",
      "current batch loss: 0.697015  [691200/826458]\n",
      "current batch loss: 0.619961  [697600/826458]\n",
      "current batch loss: 0.662541  [704000/826458]\n",
      "current batch loss: 0.644174  [710400/826458]\n",
      "current batch loss: 0.697409  [716800/826458]\n",
      "current batch loss: 0.645180  [723200/826458]\n",
      "current batch loss: 0.711047  [729600/826458]\n",
      "current batch loss: 0.658135  [736000/826458]\n",
      "current batch loss: 0.645783  [742400/826458]\n",
      "current batch loss: 0.667314  [748800/826458]\n",
      "current batch loss: 0.599987  [755200/826458]\n",
      "current batch loss: 0.694560  [761600/826458]\n",
      "current batch loss: 0.684667  [768000/826458]\n",
      "current batch loss: 0.658372  [774400/826458]\n",
      "current batch loss: 0.633238  [780800/826458]\n",
      "current batch loss: 0.708059  [787200/826458]\n",
      "current batch loss: 0.699429  [793600/826458]\n",
      "current batch loss: 0.658045  [800000/826458]\n",
      "current batch loss: 0.603488  [806400/826458]\n",
      "current batch loss: 0.686503  [812800/826458]\n",
      "current batch loss: 0.629209  [819200/826458]\n",
      "current batch loss: 0.663755  [825600/826458]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.652684\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.652447\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 13\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.647706  [    0/826458]\n",
      "current batch loss: 0.684054  [ 6400/826458]\n",
      "current batch loss: 0.731534  [12800/826458]\n",
      "current batch loss: 0.576085  [19200/826458]\n",
      "current batch loss: 0.701106  [25600/826458]\n",
      "current batch loss: 0.585149  [32000/826458]\n",
      "current batch loss: 0.688713  [38400/826458]\n",
      "current batch loss: 0.679513  [44800/826458]\n",
      "current batch loss: 0.691302  [51200/826458]\n",
      "current batch loss: 0.586525  [57600/826458]\n",
      "current batch loss: 0.642671  [64000/826458]\n",
      "current batch loss: 0.654223  [70400/826458]\n",
      "current batch loss: 0.663722  [76800/826458]\n",
      "current batch loss: 0.679995  [83200/826458]\n",
      "current batch loss: 0.576792  [89600/826458]\n",
      "current batch loss: 0.705875  [96000/826458]\n",
      "current batch loss: 0.655624  [102400/826458]\n",
      "current batch loss: 0.675768  [108800/826458]\n",
      "current batch loss: 0.610719  [115200/826458]\n",
      "current batch loss: 0.652903  [121600/826458]\n",
      "current batch loss: 0.638684  [128000/826458]\n",
      "current batch loss: 0.624401  [134400/826458]\n",
      "current batch loss: 0.583665  [140800/826458]\n",
      "current batch loss: 0.659483  [147200/826458]\n",
      "current batch loss: 0.658876  [153600/826458]\n",
      "current batch loss: 0.605329  [160000/826458]\n",
      "current batch loss: 0.648024  [166400/826458]\n",
      "current batch loss: 0.739991  [172800/826458]\n",
      "current batch loss: 0.578034  [179200/826458]\n",
      "current batch loss: 0.657281  [185600/826458]\n",
      "current batch loss: 0.677863  [192000/826458]\n",
      "current batch loss: 0.650289  [198400/826458]\n",
      "current batch loss: 0.630393  [204800/826458]\n",
      "current batch loss: 0.626294  [211200/826458]\n",
      "current batch loss: 0.691399  [217600/826458]\n",
      "current batch loss: 0.658639  [224000/826458]\n",
      "current batch loss: 0.654101  [230400/826458]\n",
      "current batch loss: 0.668879  [236800/826458]\n",
      "current batch loss: 0.678972  [243200/826458]\n",
      "current batch loss: 0.606741  [249600/826458]\n",
      "current batch loss: 0.660341  [256000/826458]\n",
      "current batch loss: 0.660467  [262400/826458]\n",
      "current batch loss: 0.641961  [268800/826458]\n",
      "current batch loss: 0.683164  [275200/826458]\n",
      "current batch loss: 0.622896  [281600/826458]\n",
      "current batch loss: 0.649922  [288000/826458]\n",
      "current batch loss: 0.598790  [294400/826458]\n",
      "current batch loss: 0.627605  [300800/826458]\n",
      "current batch loss: 0.641036  [307200/826458]\n",
      "current batch loss: 0.730770  [313600/826458]\n",
      "current batch loss: 0.714716  [320000/826458]\n",
      "current batch loss: 0.682059  [326400/826458]\n",
      "current batch loss: 0.656933  [332800/826458]\n",
      "current batch loss: 0.648406  [339200/826458]\n",
      "current batch loss: 0.678857  [345600/826458]\n",
      "current batch loss: 0.650279  [352000/826458]\n",
      "current batch loss: 0.657707  [358400/826458]\n",
      "current batch loss: 0.613018  [364800/826458]\n",
      "current batch loss: 0.677712  [371200/826458]\n",
      "current batch loss: 0.652062  [377600/826458]\n",
      "current batch loss: 0.623646  [384000/826458]\n",
      "current batch loss: 0.606003  [390400/826458]\n",
      "current batch loss: 0.676613  [396800/826458]\n",
      "current batch loss: 0.657782  [403200/826458]\n",
      "current batch loss: 0.681555  [409600/826458]\n",
      "current batch loss: 0.681927  [416000/826458]\n",
      "current batch loss: 0.641225  [422400/826458]\n",
      "current batch loss: 0.649473  [428800/826458]\n",
      "current batch loss: 0.635364  [435200/826458]\n",
      "current batch loss: 0.653992  [441600/826458]\n",
      "current batch loss: 0.658891  [448000/826458]\n",
      "current batch loss: 0.626744  [454400/826458]\n",
      "current batch loss: 0.681763  [460800/826458]\n",
      "current batch loss: 0.622042  [467200/826458]\n",
      "current batch loss: 0.609402  [473600/826458]\n",
      "current batch loss: 0.709431  [480000/826458]\n",
      "current batch loss: 0.655936  [486400/826458]\n",
      "current batch loss: 0.598745  [492800/826458]\n",
      "current batch loss: 0.684730  [499200/826458]\n",
      "current batch loss: 0.673074  [505600/826458]\n",
      "current batch loss: 0.649338  [512000/826458]\n",
      "current batch loss: 0.606888  [518400/826458]\n",
      "current batch loss: 0.598389  [524800/826458]\n",
      "current batch loss: 0.669476  [531200/826458]\n",
      "current batch loss: 0.626263  [537600/826458]\n",
      "current batch loss: 0.641925  [544000/826458]\n",
      "current batch loss: 0.672564  [550400/826458]\n",
      "current batch loss: 0.625305  [556800/826458]\n",
      "current batch loss: 0.663800  [563200/826458]\n",
      "current batch loss: 0.672731  [569600/826458]\n",
      "current batch loss: 0.668368  [576000/826458]\n",
      "current batch loss: 0.662284  [582400/826458]\n",
      "current batch loss: 0.662104  [588800/826458]\n",
      "current batch loss: 0.653269  [595200/826458]\n",
      "current batch loss: 0.648050  [601600/826458]\n",
      "current batch loss: 0.632835  [608000/826458]\n",
      "current batch loss: 0.610143  [614400/826458]\n",
      "current batch loss: 0.702783  [620800/826458]\n",
      "current batch loss: 0.654013  [627200/826458]\n",
      "current batch loss: 0.697521  [633600/826458]\n",
      "current batch loss: 0.633087  [640000/826458]\n",
      "current batch loss: 0.648340  [646400/826458]\n",
      "current batch loss: 0.715784  [652800/826458]\n",
      "current batch loss: 0.661226  [659200/826458]\n",
      "current batch loss: 0.662126  [665600/826458]\n",
      "current batch loss: 0.644257  [672000/826458]\n",
      "current batch loss: 0.633770  [678400/826458]\n",
      "current batch loss: 0.640635  [684800/826458]\n",
      "current batch loss: 0.607027  [691200/826458]\n",
      "current batch loss: 0.657294  [697600/826458]\n",
      "current batch loss: 0.679802  [704000/826458]\n",
      "current batch loss: 0.671457  [710400/826458]\n",
      "current batch loss: 0.685701  [716800/826458]\n",
      "current batch loss: 0.631532  [723200/826458]\n",
      "current batch loss: 0.642100  [729600/826458]\n",
      "current batch loss: 0.678971  [736000/826458]\n",
      "current batch loss: 0.677325  [742400/826458]\n",
      "current batch loss: 0.605239  [748800/826458]\n",
      "current batch loss: 0.633088  [755200/826458]\n",
      "current batch loss: 0.639794  [761600/826458]\n",
      "current batch loss: 0.622692  [768000/826458]\n",
      "current batch loss: 0.636270  [774400/826458]\n",
      "current batch loss: 0.649025  [780800/826458]\n",
      "current batch loss: 0.616103  [787200/826458]\n",
      "current batch loss: 0.625637  [793600/826458]\n",
      "current batch loss: 0.669060  [800000/826458]\n",
      "current batch loss: 0.673709  [806400/826458]\n",
      "current batch loss: 0.676889  [812800/826458]\n",
      "current batch loss: 0.664054  [819200/826458]\n",
      "current batch loss: 0.654804  [825600/826458]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.652866\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.652829\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 14\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.649423  [    0/826458]\n",
      "current batch loss: 0.689126  [ 6400/826458]\n",
      "current batch loss: 0.596862  [12800/826458]\n",
      "current batch loss: 0.646211  [19200/826458]\n",
      "current batch loss: 0.657759  [25600/826458]\n",
      "current batch loss: 0.649631  [32000/826458]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.686998  [38400/826458]\n",
      "current batch loss: 0.665465  [44800/826458]\n",
      "current batch loss: 0.634364  [51200/826458]\n",
      "current batch loss: 0.641380  [57600/826458]\n",
      "current batch loss: 0.661932  [64000/826458]\n",
      "current batch loss: 0.657348  [70400/826458]\n",
      "current batch loss: 0.674261  [76800/826458]\n",
      "current batch loss: 0.669159  [83200/826458]\n",
      "current batch loss: 0.657515  [89600/826458]\n",
      "current batch loss: 0.611535  [96000/826458]\n",
      "current batch loss: 0.659487  [102400/826458]\n",
      "current batch loss: 0.669787  [108800/826458]\n",
      "current batch loss: 0.598283  [115200/826458]\n",
      "current batch loss: 0.687601  [121600/826458]\n",
      "current batch loss: 0.626745  [128000/826458]\n",
      "current batch loss: 0.687299  [134400/826458]\n",
      "current batch loss: 0.648127  [140800/826458]\n",
      "current batch loss: 0.697371  [147200/826458]\n",
      "current batch loss: 0.632698  [153600/826458]\n",
      "current batch loss: 0.618476  [160000/826458]\n",
      "current batch loss: 0.630920  [166400/826458]\n",
      "current batch loss: 0.661645  [172800/826458]\n",
      "current batch loss: 0.563082  [179200/826458]\n",
      "current batch loss: 0.632067  [185600/826458]\n",
      "current batch loss: 0.635290  [192000/826458]\n",
      "current batch loss: 0.669130  [198400/826458]\n",
      "current batch loss: 0.686514  [204800/826458]\n",
      "current batch loss: 0.647771  [211200/826458]\n",
      "current batch loss: 0.686681  [217600/826458]\n",
      "current batch loss: 0.687479  [224000/826458]\n",
      "current batch loss: 0.669200  [230400/826458]\n",
      "current batch loss: 0.637652  [236800/826458]\n",
      "current batch loss: 0.717704  [243200/826458]\n",
      "current batch loss: 0.657179  [249600/826458]\n",
      "current batch loss: 0.634231  [256000/826458]\n",
      "current batch loss: 0.661369  [262400/826458]\n",
      "current batch loss: 0.602931  [268800/826458]\n",
      "current batch loss: 0.682857  [275200/826458]\n",
      "current batch loss: 0.669062  [281600/826458]\n",
      "current batch loss: 0.716394  [288000/826458]\n",
      "current batch loss: 0.730985  [294400/826458]\n",
      "current batch loss: 0.639278  [300800/826458]\n",
      "current batch loss: 0.674258  [307200/826458]\n",
      "current batch loss: 0.640412  [313600/826458]\n",
      "current batch loss: 0.600322  [320000/826458]\n",
      "current batch loss: 0.610563  [326400/826458]\n",
      "current batch loss: 0.753351  [332800/826458]\n",
      "current batch loss: 0.636369  [339200/826458]\n",
      "current batch loss: 0.689658  [345600/826458]\n",
      "current batch loss: 0.634007  [352000/826458]\n",
      "current batch loss: 0.678052  [358400/826458]\n",
      "current batch loss: 0.681305  [364800/826458]\n",
      "current batch loss: 0.667101  [371200/826458]\n",
      "current batch loss: 0.647515  [377600/826458]\n",
      "current batch loss: 0.644051  [384000/826458]\n",
      "current batch loss: 0.744784  [390400/826458]\n",
      "current batch loss: 0.722068  [396800/826458]\n",
      "current batch loss: 0.630916  [403200/826458]\n",
      "current batch loss: 0.648184  [409600/826458]\n",
      "current batch loss: 0.659197  [416000/826458]\n",
      "current batch loss: 0.694212  [422400/826458]\n",
      "current batch loss: 0.688234  [428800/826458]\n",
      "current batch loss: 0.659031  [435200/826458]\n",
      "current batch loss: 0.632100  [441600/826458]\n",
      "current batch loss: 0.631631  [448000/826458]\n",
      "current batch loss: 0.619314  [454400/826458]\n",
      "current batch loss: 0.680491  [460800/826458]\n",
      "current batch loss: 0.725049  [467200/826458]\n",
      "current batch loss: 0.632215  [473600/826458]\n",
      "current batch loss: 0.692621  [480000/826458]\n",
      "current batch loss: 0.630607  [486400/826458]\n",
      "current batch loss: 0.668866  [492800/826458]\n",
      "current batch loss: 0.607274  [499200/826458]\n",
      "current batch loss: 0.616386  [505600/826458]\n",
      "current batch loss: 0.626511  [512000/826458]\n",
      "current batch loss: 0.662674  [518400/826458]\n",
      "current batch loss: 0.593996  [524800/826458]\n",
      "current batch loss: 0.652224  [531200/826458]\n",
      "current batch loss: 0.629518  [537600/826458]\n",
      "current batch loss: 0.685562  [544000/826458]\n",
      "current batch loss: 0.555591  [550400/826458]\n",
      "current batch loss: 0.646848  [556800/826458]\n",
      "current batch loss: 0.678384  [563200/826458]\n",
      "current batch loss: 0.593536  [569600/826458]\n",
      "current batch loss: 0.690475  [576000/826458]\n",
      "current batch loss: 0.643625  [582400/826458]\n",
      "current batch loss: 0.614634  [588800/826458]\n",
      "current batch loss: 0.693422  [595200/826458]\n",
      "current batch loss: 0.635874  [601600/826458]\n",
      "current batch loss: 0.693091  [608000/826458]\n",
      "current batch loss: 0.685923  [614400/826458]\n",
      "current batch loss: 0.645732  [620800/826458]\n",
      "current batch loss: 0.673190  [627200/826458]\n",
      "current batch loss: 0.641383  [633600/826458]\n",
      "current batch loss: 0.663581  [640000/826458]\n",
      "current batch loss: 0.595192  [646400/826458]\n",
      "current batch loss: 0.590043  [652800/826458]\n",
      "current batch loss: 0.728062  [659200/826458]\n",
      "current batch loss: 0.616567  [665600/826458]\n",
      "current batch loss: 0.648678  [672000/826458]\n",
      "current batch loss: 0.712069  [678400/826458]\n",
      "current batch loss: 0.628686  [684800/826458]\n",
      "current batch loss: 0.636507  [691200/826458]\n",
      "current batch loss: 0.644125  [697600/826458]\n",
      "current batch loss: 0.680159  [704000/826458]\n",
      "current batch loss: 0.645770  [710400/826458]\n",
      "current batch loss: 0.702880  [716800/826458]\n",
      "current batch loss: 0.691249  [723200/826458]\n",
      "current batch loss: 0.645841  [729600/826458]\n",
      "current batch loss: 0.648884  [736000/826458]\n",
      "current batch loss: 0.642404  [742400/826458]\n",
      "current batch loss: 0.691829  [748800/826458]\n",
      "current batch loss: 0.714075  [755200/826458]\n",
      "current batch loss: 0.700288  [761600/826458]\n",
      "current batch loss: 0.707402  [768000/826458]\n",
      "current batch loss: 0.669102  [774400/826458]\n",
      "current batch loss: 0.615314  [780800/826458]\n",
      "current batch loss: 0.704201  [787200/826458]\n",
      "current batch loss: 0.628407  [793600/826458]\n",
      "current batch loss: 0.635036  [800000/826458]\n",
      "current batch loss: 0.671095  [806400/826458]\n",
      "current batch loss: 0.680105  [812800/826458]\n",
      "current batch loss: 0.643438  [819200/826458]\n",
      "current batch loss: 0.663833  [825600/826458]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.652548\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.652370\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 15\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.610447  [    0/826458]\n",
      "current batch loss: 0.673459  [ 6400/826458]\n",
      "current batch loss: 0.663447  [12800/826458]\n",
      "current batch loss: 0.616228  [19200/826458]\n",
      "current batch loss: 0.649882  [25600/826458]\n",
      "current batch loss: 0.649037  [32000/826458]\n",
      "current batch loss: 0.641243  [38400/826458]\n",
      "current batch loss: 0.695073  [44800/826458]\n",
      "current batch loss: 0.660800  [51200/826458]\n",
      "current batch loss: 0.638723  [57600/826458]\n",
      "current batch loss: 0.689397  [64000/826458]\n",
      "current batch loss: 0.590832  [70400/826458]\n",
      "current batch loss: 0.621405  [76800/826458]\n",
      "current batch loss: 0.639997  [83200/826458]\n",
      "current batch loss: 0.621022  [89600/826458]\n",
      "current batch loss: 0.600203  [96000/826458]\n",
      "current batch loss: 0.669331  [102400/826458]\n",
      "current batch loss: 0.608155  [108800/826458]\n",
      "current batch loss: 0.726845  [115200/826458]\n",
      "current batch loss: 0.603063  [121600/826458]\n",
      "current batch loss: 0.647156  [128000/826458]\n",
      "current batch loss: 0.643404  [134400/826458]\n",
      "current batch loss: 0.678975  [140800/826458]\n",
      "current batch loss: 0.617779  [147200/826458]\n",
      "current batch loss: 0.656661  [153600/826458]\n",
      "current batch loss: 0.704389  [160000/826458]\n",
      "current batch loss: 0.637768  [166400/826458]\n",
      "current batch loss: 0.668875  [172800/826458]\n",
      "current batch loss: 0.641244  [179200/826458]\n",
      "current batch loss: 0.583133  [185600/826458]\n",
      "current batch loss: 0.666451  [192000/826458]\n",
      "current batch loss: 0.636686  [198400/826458]\n",
      "current batch loss: 0.612710  [204800/826458]\n",
      "current batch loss: 0.617170  [211200/826458]\n",
      "current batch loss: 0.622230  [217600/826458]\n",
      "current batch loss: 0.667592  [224000/826458]\n",
      "current batch loss: 0.690313  [230400/826458]\n",
      "current batch loss: 0.625320  [236800/826458]\n",
      "current batch loss: 0.676855  [243200/826458]\n",
      "current batch loss: 0.612423  [249600/826458]\n",
      "current batch loss: 0.718826  [256000/826458]\n",
      "current batch loss: 0.602479  [262400/826458]\n",
      "current batch loss: 0.663856  [268800/826458]\n",
      "current batch loss: 0.659203  [275200/826458]\n",
      "current batch loss: 0.655855  [281600/826458]\n",
      "current batch loss: 0.637203  [288000/826458]\n",
      "current batch loss: 0.629857  [294400/826458]\n",
      "current batch loss: 0.592899  [300800/826458]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.649869  [307200/826458]\n",
      "current batch loss: 0.647312  [313600/826458]\n",
      "current batch loss: 0.720732  [320000/826458]\n",
      "current batch loss: 0.597820  [326400/826458]\n",
      "current batch loss: 0.687372  [332800/826458]\n",
      "current batch loss: 0.623710  [339200/826458]\n",
      "current batch loss: 0.674666  [345600/826458]\n",
      "current batch loss: 0.639814  [352000/826458]\n",
      "current batch loss: 0.608868  [358400/826458]\n",
      "current batch loss: 0.634586  [364800/826458]\n",
      "current batch loss: 0.643876  [371200/826458]\n",
      "current batch loss: 0.588931  [377600/826458]\n",
      "current batch loss: 0.614397  [384000/826458]\n",
      "current batch loss: 0.650924  [390400/826458]\n",
      "current batch loss: 0.627213  [396800/826458]\n",
      "current batch loss: 0.645783  [403200/826458]\n",
      "current batch loss: 0.626789  [409600/826458]\n",
      "current batch loss: 0.630119  [416000/826458]\n",
      "current batch loss: 0.647393  [422400/826458]\n",
      "current batch loss: 0.653392  [428800/826458]\n",
      "current batch loss: 0.716629  [435200/826458]\n",
      "current batch loss: 0.659550  [441600/826458]\n",
      "current batch loss: 0.641808  [448000/826458]\n",
      "current batch loss: 0.626974  [454400/826458]\n",
      "current batch loss: 0.699484  [460800/826458]\n",
      "current batch loss: 0.611337  [467200/826458]\n",
      "current batch loss: 0.622761  [473600/826458]\n",
      "current batch loss: 0.611098  [480000/826458]\n",
      "current batch loss: 0.662238  [486400/826458]\n",
      "current batch loss: 0.658379  [492800/826458]\n",
      "current batch loss: 0.719655  [499200/826458]\n",
      "current batch loss: 0.687699  [505600/826458]\n",
      "current batch loss: 0.567093  [512000/826458]\n",
      "current batch loss: 0.653798  [518400/826458]\n",
      "current batch loss: 0.622844  [524800/826458]\n",
      "current batch loss: 0.729243  [531200/826458]\n",
      "current batch loss: 0.699232  [537600/826458]\n",
      "current batch loss: 0.606037  [544000/826458]\n",
      "current batch loss: 0.638456  [550400/826458]\n",
      "current batch loss: 0.699647  [556800/826458]\n",
      "current batch loss: 0.725702  [563200/826458]\n",
      "current batch loss: 0.637167  [569600/826458]\n",
      "current batch loss: 0.646012  [576000/826458]\n",
      "current batch loss: 0.605719  [582400/826458]\n",
      "current batch loss: 0.678032  [588800/826458]\n",
      "current batch loss: 0.659907  [595200/826458]\n",
      "current batch loss: 0.671609  [601600/826458]\n",
      "current batch loss: 0.625916  [608000/826458]\n",
      "current batch loss: 0.566030  [614400/826458]\n",
      "current batch loss: 0.681460  [620800/826458]\n",
      "current batch loss: 0.638502  [627200/826458]\n",
      "current batch loss: 0.629929  [633600/826458]\n",
      "current batch loss: 0.693267  [640000/826458]\n",
      "current batch loss: 0.630649  [646400/826458]\n",
      "current batch loss: 0.659553  [652800/826458]\n",
      "current batch loss: 0.669310  [659200/826458]\n",
      "current batch loss: 0.627718  [665600/826458]\n",
      "current batch loss: 0.560998  [672000/826458]\n",
      "current batch loss: 0.734327  [678400/826458]\n",
      "current batch loss: 0.631895  [684800/826458]\n",
      "current batch loss: 0.623892  [691200/826458]\n",
      "current batch loss: 0.615161  [697600/826458]\n",
      "current batch loss: 0.609680  [704000/826458]\n",
      "current batch loss: 0.614079  [710400/826458]\n",
      "current batch loss: 0.618784  [716800/826458]\n",
      "current batch loss: 0.601270  [723200/826458]\n",
      "current batch loss: 0.676278  [729600/826458]\n",
      "current batch loss: 0.609695  [736000/826458]\n",
      "current batch loss: 0.557488  [742400/826458]\n",
      "current batch loss: 0.595915  [748800/826458]\n",
      "current batch loss: 0.712131  [755200/826458]\n",
      "current batch loss: 0.591225  [761600/826458]\n",
      "current batch loss: 0.632992  [768000/826458]\n",
      "current batch loss: 0.674831  [774400/826458]\n",
      "current batch loss: 0.643099  [780800/826458]\n",
      "current batch loss: 0.635908  [787200/826458]\n",
      "current batch loss: 0.607329  [793600/826458]\n",
      "current batch loss: 0.665593  [800000/826458]\n",
      "current batch loss: 0.665559  [806400/826458]\n",
      "current batch loss: 0.630647  [812800/826458]\n",
      "current batch loss: 0.589984  [819200/826458]\n",
      "current batch loss: 0.682075  [825600/826458]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.652534\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.652454\n",
      "-----------------------------------------------\n",
      "|\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "trn_dataset_sb = cwola_data_sb( X_train_p, y_train_p, b_train_p, 0.01 )\n",
    "val_dataset_sb = cwola_data_sb( X_val_p, y_val_p, b_val_p, 0.01 )\n",
    "\n",
    "trn_dataloader_sb = DataLoader( trn_dataset_sb, batch_size=64, shuffle=True )\n",
    "val_dataloader_sb = DataLoader( val_dataset_sb, batch_size=64, shuffle=True )\n",
    "\n",
    "print( \"train S/B: \" + str( np.round( ( trn_dataset_sb.labels_cut.sum() / (1.0-trn_dataset_sb.labels_cut).sum() )*100, 2 ).item() ) + \"%\" )\n",
    "print( \"val S/B: \" + str( np.round( ( val_dataset_sb.labels_cut.sum() / (1.0-val_dataset_sb.labels_cut).sum() )*100, 2 ).item() ) + \"%\" )\n",
    "\n",
    "# a useful function to present things clearer\n",
    "def separator():\n",
    "    print( \"-----------------------------------------------\" )\n",
    "\n",
    "epochs = 15\n",
    "\n",
    "# re-initialise the model and the optimizer\n",
    "model_sb1pc = cwolaNet( 4, 64 ).to(device)\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam( model_sb1pc.parameters(), lr=learning_rate )\n",
    "separator()\n",
    "print( \"model architecture \")\n",
    "separator()\n",
    "print( model )\n",
    "\n",
    "# track train and val losses\n",
    "trn_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for t in range(epochs):\n",
    "    separator()\n",
    "    print( f\"Epoch {t+1}\" )\n",
    "    separator()\n",
    "    train_epoch( trn_dataloader_sb, model_sb1pc, loss_fn, optimizer )\n",
    "    separator()\n",
    "    trn_loss = trn_pass( trn_dataloader_sb, model_sb1pc, loss_fn )\n",
    "    trn_losses.append( trn_loss )\n",
    "    separator()\n",
    "    val_loss = val_pass( val_dataloader_sb, model_sb1pc, loss_fn )\n",
    "    val_losses.append( val_loss )\n",
    "    separator()\n",
    "    print( \"|\" )\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_sb1pc = model_sb1pc( X_test_p ).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(14,5))\n",
    "\n",
    "fpr, tpr, th = roc_curve( y_test, test_pred_sb1pc )\n",
    "auc_score = roc_auc_score( y_test, test_pred_sb1pc )\n",
    "rnd_class = np.linspace(0, 1, 100)\n",
    "\n",
    "ax[0].plot(fpr, tpr, label='AUC = {:.2f}'.format(auc_score) )\n",
    "ax[0].plot(rnd_class, rnd_class, '--', label='Rnd classifier')\n",
    "ax[1].plot(tpr, 1/fpr, label='AUC = {:.2f}\\n $1/\\epsilon_{{bkg}}$(0.3) = {:.0f}'.format(auc_score, 1/fpr[closest_point(tpr, tpr_p=0.3)]))\n",
    "ax[1].plot(rnd_class, 1/rnd_class, '--', label='Rnd classifier')\n",
    "ax[1].set_yscale('log')\n",
    "\n",
    "ax[0].set_xlabel('$\\epsilon_{bkg}$ - FPR', fontproperties=axislabelfont)\n",
    "ax[0].set_ylabel('$\\epsilon_{s}$ - TPR', fontproperties=axislabelfont)\n",
    "\n",
    "ax[1].set_xlabel('$\\epsilon_{s}$ - TPR', fontproperties=axislabelfont)\n",
    "ax[1].set_ylabel('1/$\\epsilon_{bkg}$ - Inverse FPR', fontproperties=axislabelfont)\n",
    "for i in range(len(ax)):\n",
    "    ax[i].legend(prop=axislabelfont)\n",
    "    ax[i].tick_params(labelsize=axisfontsize)\n",
    "    ax[i].grid('on')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that the performance really does get a lot worse at lower S/B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots( figsize=(7,5))\n",
    "\n",
    "fpr_sb10pc, tpr_sb10pc, th_sb10pc = roc_curve( y_test, test_pred )\n",
    "fpr_sb5pc, tpr_sb5pc, th_sb5pc = roc_curve( y_test, test_pred_sb5pc )\n",
    "fpr_sb1pc, tpr_sb1pc, th_sb1pc = roc_curve( y_test, test_pred_sb1pc )\n",
    "\n",
    "auc_score_sb10pc = roc_auc_score( y_test, test_pred )\n",
    "auc_score_sb5pc = roc_auc_score( y_test, test_pred_sb5pc )\n",
    "auc_score_sb1pc = roc_auc_score( y_test, test_pred_sb1pc )\n",
    "\n",
    "rnd_class = np.linspace(0, 1, 100)\n",
    "\n",
    "ax.plot(tpr_sb10pc, 1/fpr_sb10pc, label='S/B=10\\%, AUC = {:.2f}'.format(auc_score_sb10pc))\n",
    "ax.plot(tpr_sb5pc, 1/fpr_sb5pc, label='S/B=5\\%, AUC = {:.2f}'.format(auc_score_sb5pc))\n",
    "ax.plot(tpr_sb1pc, 1/fpr_sb1pc, label='S/B=1\\%, AUC = {:.2f}'.format(auc_score_sb1pc))\n",
    "#ax.plot(rnd_class, 1/rnd_class, '--', label='Rnd classifier')\n",
    "ax.set_yscale('log')\n",
    "\n",
    "ax.set_xlabel('$\\epsilon_{s}$ - TPR', fontproperties=axislabelfont)\n",
    "ax.set_ylabel('1/$\\epsilon_{bkg}$ - Inverse FPR', fontproperties=axislabelfont)\n",
    "ax.legend(prop=axislabelfont)\n",
    "ax.tick_params(labelsize=axisfontsize)\n",
    "ax.grid('on')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations & 'optimal CWoLa'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's make the dataset.\n",
    "\n",
    "We will only use the signal window data here, and split it artificially into different 'bins'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many signal events in the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((380294, 4), 76608)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_dat_sw.shape, trn_lbs_sw.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38432"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_len = int( trn_lbs_sw.shape[0] )\n",
    "half_trn_len = int( trn_lbs_sw.shape[0]/2 )\n",
    "trn_lbs_sw[0:half_trn_len].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many signal events in the validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42375, 4), 8613)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dat_sw.shape, val_lbs_sw.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4313"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_len = int( val_lbs_sw.shape[0] )\n",
    "half_val_len = int( val_lbs_sw.shape[0]/2 )\n",
    "val_lbs_sw[0:half_val_len].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many signal events in the testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42020, 4), 8481)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_dat_sw.shape, tst_lbs_sw.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4274"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_len = int( tst_lbs_sw.shape[0] )\n",
    "half_tst_len = int( tst_lbs_sw.shape[0]/2 )\n",
    "tst_lbs_sw[0:half_tst_len].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the training, validation, and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_oc = trn_dat_sw.copy()\n",
    "y_train_oc = trn_lbs_sw.copy()\n",
    "b_train_oc = np.concatenate( ( np.ones( half_trn_len ), np.zeros( trn_len - half_trn_len ) ), axis=0 )\n",
    "b_train_oc[ np.where( y_train_oc==1.0 ) ] = 1.0\n",
    "\n",
    "X_val_oc = val_dat_sw.copy()\n",
    "y_val_oc = val_lbs_sw.copy()\n",
    "b_val_oc = np.concatenate( ( np.ones( half_val_len ), np.zeros( val_len - half_val_len ) ), axis=0 )\n",
    "b_val_oc[ np.where( y_val_oc==1.0 ) ] = 1.0\n",
    "\n",
    "X_test_oc = tst_dat_sw.copy()\n",
    "y_test_oc = tst_lbs_sw.copy()\n",
    "b_test_oc = np.concatenate( ( np.ones( half_tst_len ), np.zeros( tst_len - half_tst_len ) ), axis=0 )\n",
    "b_test_oc[ np.where( y_test_oc==1.0 ) ] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the shapes and sums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((380294, 4), (380294,), (380294,), 228323.0, 303686, 76608)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_oc.shape, y_train_oc.shape, b_train_oc.shape, b_train_oc.sum(), (1-y_train_oc).sum(), y_train_oc.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42375, 4), (42375,), (42375,), 25487.0, 33762, 8613)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_oc.shape, y_val_oc.shape, b_val_oc.shape, b_val_oc.sum(), (1-y_val_oc).sum(), y_val_oc.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42020, 4), (42020,), (42020,), 25217.0, 33539, 8481)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_oc.shape, y_test_oc.shape, b_test_oc.shape, b_test_oc.sum(), (1-y_test_oc).sum(), y_test_oc.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there are too many signal events.  We want the S/B to be the same as before, i.e. 10%.  Let's delete some of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_nbkg_oc = ( 1.0 - y_train_oc ).sum()\n",
    "trn_nsig_oc = y_train_oc.sum()\n",
    "trn_ndel_oc = int( trn_nsig_oc - 0.1*trn_nbkg_oc )\n",
    "trn_del_ind_oc = np.where( y_train_oc==1.0 )[0][0:trn_ndel_oc]\n",
    "X_train_oc = np.delete( X_train_oc, trn_del_ind_oc, axis=0 )\n",
    "y_train_oc = np.delete( y_train_oc, trn_del_ind_oc, axis=0 )\n",
    "b_train_oc = np.delete( b_train_oc, trn_del_ind_oc, axis=0 )\n",
    "\n",
    "val_nbkg_oc = ( 1.0 - y_val_oc ).sum()\n",
    "val_nsig_oc = y_val_oc.sum()\n",
    "val_ndel_oc = int( val_nsig_oc - 0.1*val_nbkg_oc )\n",
    "val_del_ind_oc = np.where( y_val_oc==1.0 )[0][0:val_ndel_oc]\n",
    "X_val_oc = np.delete( X_val_oc, val_del_ind_oc, axis=0 )\n",
    "y_val_oc = np.delete( y_val_oc, val_del_ind_oc, axis=0 )\n",
    "b_val_oc = np.delete( b_val_oc, val_del_ind_oc, axis=0 )\n",
    "\n",
    "tst_nbkg_oc = ( 1.0 - y_test_oc ).sum()\n",
    "tst_nsig_oc = y_test_oc.sum()\n",
    "tst_ndel_oc = int( tst_nsig_oc - 0.1*tst_nbkg_oc )\n",
    "tst_del_ind_oc = np.where( y_test_oc==1.0 )[0][0:tst_ndel_oc]\n",
    "X_test_oc = np.delete( X_test_oc, tst_del_ind_oc, axis=0 )\n",
    "y_test_oc = np.delete( y_test_oc, tst_del_ind_oc, axis=0 )\n",
    "b_test_oc = np.delete( b_test_oc, tst_del_ind_oc, axis=0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the shapes and sums again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((334055, 4), (334055,), (334055,), 182084.0, 303686, 30369)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_oc.shape, y_train_oc.shape, b_train_oc.shape, b_train_oc.sum(), (1-y_train_oc).sum(), y_train_oc.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37139, 4), (37139,), (37139,), 20251.0, 33762, 3377)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_oc.shape, y_val_oc.shape, b_val_oc.shape, b_val_oc.sum(), (1-y_val_oc).sum(), y_val_oc.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36893, 4), (36893,), (36893,), 20090.0, 33539, 3354)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_oc.shape, y_test_oc.shape, b_test_oc.shape, b_test_oc.sum(), (1-y_test_oc).sum(), y_test_oc.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's put them in datasets and dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_oc_p = torch.Tensor( X_train_oc )\n",
    "y_train_oc_p = torch.Tensor( y_train_oc ).unsqueeze(-1)\n",
    "b_train_oc_p = torch.Tensor( b_train_oc ).unsqueeze(-1)\n",
    "\n",
    "X_val_oc_p = torch.Tensor( X_val_oc )\n",
    "y_val_oc_p = torch.Tensor( y_val_oc ).unsqueeze(-1)\n",
    "b_val_oc_p = torch.Tensor( b_val_oc ).unsqueeze(-1)\n",
    "\n",
    "X_test_oc_p = torch.Tensor( X_test_oc )\n",
    "y_test_oc_p = torch.Tensor( y_test_oc ).unsqueeze(-1)\n",
    "b_test_oc_p = torch.Tensor( b_test_oc ).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dataset_oc = cwola_data( X_train_oc_p, y_train_oc_p, b_train_oc_p )\n",
    "val_dataset_oc = cwola_data( X_val_oc_p, y_val_oc_p, b_val_oc_p )\n",
    "tst_dataset_oc = cwola_data( X_test_oc_p, y_test_oc_p, b_test_oc_p )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dataloader_oc = DataLoader( trn_dataset_oc, batch_size=64, shuffle=True )\n",
    "val_dataloader_oc = DataLoader( val_dataset_oc, batch_size=64, shuffle=True )\n",
    "tst_dataloader_oc = DataLoader( tst_dataset_oc, batch_size=64, shuffle=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's optimise it.\n",
    "\n",
    "We only have ~40% of the data here, so I'm increasing the number of epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900000, 334055, 0.37117222222222224, 2.6941671281675172, 40)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_len = X_train.shape[0]\n",
    "trn_len_oc = X_train_oc.shape[0]\n",
    "trn_len, trn_len_oc, trn_len_oc/trn_len, trn_len / trn_len_oc, int( 15*trn_len / trn_len_oc )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------\n",
      "model architecture \n",
      "-----------------------------------------------\n",
      "cwolaNet(\n",
      "  (layer1): Linear(in_features=4, out_features=64, bias=True)\n",
      "  (relu_1): ReLU()\n",
      "  (layer2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (relu_2): ReLU()\n",
      "  (layer3): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "-----------------------------------------------\n",
      "Epoch 1\n",
      "-----------------------------------------------\n",
      "current batch loss: 2.362020  [    0/900000]\n",
      "current batch loss: 0.648834  [ 6400/900000]\n",
      "current batch loss: 0.672626  [12800/900000]\n",
      "current batch loss: 0.722955  [19200/900000]\n",
      "current batch loss: 0.678765  [25600/900000]\n",
      "current batch loss: 0.626515  [32000/900000]\n",
      "current batch loss: 0.623540  [38400/900000]\n",
      "current batch loss: 0.658629  [44800/900000]\n",
      "current batch loss: 0.730936  [51200/900000]\n",
      "current batch loss: 0.684623  [57600/900000]\n",
      "current batch loss: 0.684048  [64000/900000]\n",
      "current batch loss: 0.664977  [70400/900000]\n",
      "current batch loss: 0.675716  [76800/900000]\n",
      "current batch loss: 0.666403  [83200/900000]\n",
      "current batch loss: 0.668218  [89600/900000]\n",
      "current batch loss: 0.708445  [96000/900000]\n",
      "current batch loss: 0.639009  [102400/900000]\n",
      "current batch loss: 0.611030  [108800/900000]\n",
      "current batch loss: 0.627264  [115200/900000]\n",
      "current batch loss: 0.573340  [121600/900000]\n",
      "current batch loss: 0.628914  [128000/900000]\n",
      "current batch loss: 0.649368  [134400/900000]\n",
      "current batch loss: 0.731535  [140800/900000]\n",
      "current batch loss: 0.684159  [147200/900000]\n",
      "current batch loss: 0.603718  [153600/900000]\n",
      "current batch loss: 0.692205  [160000/900000]\n",
      "current batch loss: 0.574325  [166400/900000]\n",
      "current batch loss: 0.635818  [172800/900000]\n",
      "current batch loss: 0.792483  [179200/900000]\n",
      "current batch loss: 0.612174  [185600/900000]\n",
      "current batch loss: 0.601704  [192000/900000]\n",
      "current batch loss: 0.692308  [198400/900000]\n",
      "current batch loss: 0.671167  [204800/900000]\n",
      "current batch loss: 0.585033  [211200/900000]\n",
      "current batch loss: 0.628207  [217600/900000]\n",
      "current batch loss: 0.667770  [224000/900000]\n",
      "current batch loss: 0.616672  [230400/900000]\n",
      "current batch loss: 0.593982  [236800/900000]\n",
      "current batch loss: 0.659847  [243200/900000]\n",
      "current batch loss: 0.671511  [249600/900000]\n",
      "current batch loss: 0.693054  [256000/900000]\n",
      "current batch loss: 0.677452  [262400/900000]\n",
      "current batch loss: 0.668407  [268800/900000]\n",
      "current batch loss: 0.578220  [275200/900000]\n",
      "current batch loss: 0.617726  [281600/900000]\n",
      "current batch loss: 0.703899  [288000/900000]\n",
      "current batch loss: 0.652708  [294400/900000]\n",
      "current batch loss: 0.606467  [300800/900000]\n",
      "current batch loss: 0.630032  [307200/900000]\n",
      "current batch loss: 0.654940  [313600/900000]\n",
      "current batch loss: 0.708001  [320000/900000]\n",
      "current batch loss: 0.647406  [326400/900000]\n",
      "current batch loss: 0.704098  [332800/900000]\n",
      "current batch loss: 0.620494  [339200/900000]\n",
      "current batch loss: 0.672539  [345600/900000]\n",
      "current batch loss: 0.633390  [352000/900000]\n",
      "current batch loss: 0.678522  [358400/900000]\n",
      "current batch loss: 0.730998  [364800/900000]\n",
      "current batch loss: 0.665894  [371200/900000]\n",
      "current batch loss: 0.629601  [377600/900000]\n",
      "current batch loss: 0.706566  [384000/900000]\n",
      "current batch loss: 0.663139  [390400/900000]\n",
      "current batch loss: 0.672528  [396800/900000]\n",
      "current batch loss: 0.644196  [403200/900000]\n",
      "current batch loss: 0.649249  [409600/900000]\n",
      "current batch loss: 0.630312  [416000/900000]\n",
      "current batch loss: 0.627771  [422400/900000]\n",
      "current batch loss: 0.605475  [428800/900000]\n",
      "current batch loss: 0.710783  [435200/900000]\n",
      "current batch loss: 0.710747  [441600/900000]\n",
      "current batch loss: 0.639884  [448000/900000]\n",
      "current batch loss: 0.659381  [454400/900000]\n",
      "current batch loss: 0.632790  [460800/900000]\n",
      "current batch loss: 0.655233  [467200/900000]\n",
      "current batch loss: 0.652145  [473600/900000]\n",
      "current batch loss: 0.715735  [480000/900000]\n",
      "current batch loss: 0.633109  [486400/900000]\n",
      "current batch loss: 0.671230  [492800/900000]\n",
      "current batch loss: 0.639177  [499200/900000]\n",
      "current batch loss: 0.690414  [505600/900000]\n",
      "current batch loss: 0.665798  [512000/900000]\n",
      "current batch loss: 0.601329  [518400/900000]\n",
      "current batch loss: 0.688119  [524800/900000]\n",
      "current batch loss: 0.715977  [531200/900000]\n",
      "current batch loss: 0.674216  [537600/900000]\n",
      "current batch loss: 0.670163  [544000/900000]\n",
      "current batch loss: 0.627942  [550400/900000]\n",
      "current batch loss: 0.667485  [556800/900000]\n",
      "current batch loss: 0.627249  [563200/900000]\n",
      "current batch loss: 0.664069  [569600/900000]\n",
      "current batch loss: 0.674087  [576000/900000]\n",
      "current batch loss: 0.662631  [582400/900000]\n",
      "current batch loss: 0.617802  [588800/900000]\n",
      "current batch loss: 0.622428  [595200/900000]\n",
      "current batch loss: 0.662413  [601600/900000]\n",
      "current batch loss: 0.670740  [608000/900000]\n",
      "current batch loss: 0.634979  [614400/900000]\n",
      "current batch loss: 0.676752  [620800/900000]\n",
      "current batch loss: 0.680604  [627200/900000]\n",
      "current batch loss: 0.682533  [633600/900000]\n",
      "current batch loss: 0.651679  [640000/900000]\n",
      "current batch loss: 0.614319  [646400/900000]\n",
      "current batch loss: 0.660682  [652800/900000]\n",
      "current batch loss: 0.676014  [659200/900000]\n",
      "current batch loss: 0.618745  [665600/900000]\n",
      "current batch loss: 0.621808  [672000/900000]\n",
      "current batch loss: 0.682592  [678400/900000]\n",
      "current batch loss: 0.624884  [684800/900000]\n",
      "current batch loss: 0.659825  [691200/900000]\n",
      "current batch loss: 0.654270  [697600/900000]\n",
      "current batch loss: 0.615462  [704000/900000]\n",
      "current batch loss: 0.749111  [710400/900000]\n",
      "current batch loss: 0.580469  [716800/900000]\n",
      "current batch loss: 0.586505  [723200/900000]\n",
      "current batch loss: 0.662680  [729600/900000]\n",
      "current batch loss: 0.611349  [736000/900000]\n",
      "current batch loss: 0.653481  [742400/900000]\n",
      "current batch loss: 0.587888  [748800/900000]\n",
      "current batch loss: 0.637091  [755200/900000]\n",
      "current batch loss: 0.560680  [761600/900000]\n",
      "current batch loss: 0.653171  [768000/900000]\n",
      "current batch loss: 0.629272  [774400/900000]\n",
      "current batch loss: 0.599821  [780800/900000]\n",
      "current batch loss: 0.650932  [787200/900000]\n",
      "current batch loss: 0.654406  [793600/900000]\n",
      "current batch loss: 0.655493  [800000/900000]\n",
      "current batch loss: 0.581236  [806400/900000]\n",
      "current batch loss: 0.656408  [812800/900000]\n",
      "current batch loss: 0.605113  [819200/900000]\n",
      "current batch loss: 0.683848  [825600/900000]\n",
      "current batch loss: 0.646231  [832000/900000]\n",
      "current batch loss: 0.619685  [838400/900000]\n",
      "current batch loss: 0.676323  [844800/900000]\n",
      "current batch loss: 0.593739  [851200/900000]\n",
      "current batch loss: 0.624686  [857600/900000]\n",
      "current batch loss: 0.637271  [864000/900000]\n",
      "current batch loss: 0.650030  [870400/900000]\n",
      "current batch loss: 0.640700  [876800/900000]\n",
      "current batch loss: 0.699333  [883200/900000]\n",
      "current batch loss: 0.619375  [889600/900000]\n",
      "current batch loss: 0.557688  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.650484\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.648312\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 2\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.647537  [    0/900000]\n",
      "current batch loss: 0.642257  [ 6400/900000]\n",
      "current batch loss: 0.674106  [12800/900000]\n",
      "current batch loss: 0.617962  [19200/900000]\n",
      "current batch loss: 0.679191  [25600/900000]\n",
      "current batch loss: 0.632646  [32000/900000]\n",
      "current batch loss: 0.658189  [38400/900000]\n",
      "current batch loss: 0.578839  [44800/900000]\n",
      "current batch loss: 0.634478  [51200/900000]\n",
      "current batch loss: 0.664167  [57600/900000]\n",
      "current batch loss: 0.604977  [64000/900000]\n",
      "current batch loss: 0.681924  [70400/900000]\n",
      "current batch loss: 0.656882  [76800/900000]\n",
      "current batch loss: 0.637705  [83200/900000]\n",
      "current batch loss: 0.596797  [89600/900000]\n",
      "current batch loss: 0.638389  [96000/900000]\n",
      "current batch loss: 0.651344  [102400/900000]\n",
      "current batch loss: 0.618359  [108800/900000]\n",
      "current batch loss: 0.618169  [115200/900000]\n",
      "current batch loss: 0.617989  [121600/900000]\n",
      "current batch loss: 0.633817  [128000/900000]\n",
      "current batch loss: 0.619892  [134400/900000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.642415  [140800/900000]\n",
      "current batch loss: 0.679804  [147200/900000]\n",
      "current batch loss: 0.671468  [153600/900000]\n",
      "current batch loss: 0.657570  [160000/900000]\n",
      "current batch loss: 0.706905  [166400/900000]\n",
      "current batch loss: 0.658196  [172800/900000]\n",
      "current batch loss: 0.634802  [179200/900000]\n",
      "current batch loss: 0.597448  [185600/900000]\n",
      "current batch loss: 0.618192  [192000/900000]\n",
      "current batch loss: 0.653197  [198400/900000]\n",
      "current batch loss: 0.545804  [204800/900000]\n",
      "current batch loss: 0.615309  [211200/900000]\n",
      "current batch loss: 0.620554  [217600/900000]\n",
      "current batch loss: 0.624513  [224000/900000]\n",
      "current batch loss: 0.648602  [230400/900000]\n",
      "current batch loss: 0.577616  [236800/900000]\n",
      "current batch loss: 0.638873  [243200/900000]\n",
      "current batch loss: 0.737002  [249600/900000]\n",
      "current batch loss: 0.658552  [256000/900000]\n",
      "current batch loss: 0.612083  [262400/900000]\n",
      "current batch loss: 0.603953  [268800/900000]\n",
      "current batch loss: 0.651458  [275200/900000]\n",
      "current batch loss: 0.698597  [281600/900000]\n",
      "current batch loss: 0.695318  [288000/900000]\n",
      "current batch loss: 0.675414  [294400/900000]\n",
      "current batch loss: 0.638753  [300800/900000]\n",
      "current batch loss: 0.677278  [307200/900000]\n",
      "current batch loss: 0.604418  [313600/900000]\n",
      "current batch loss: 0.595770  [320000/900000]\n",
      "current batch loss: 0.633711  [326400/900000]\n",
      "current batch loss: 0.642263  [332800/900000]\n",
      "current batch loss: 0.703518  [339200/900000]\n",
      "current batch loss: 0.652684  [345600/900000]\n",
      "current batch loss: 0.617207  [352000/900000]\n",
      "current batch loss: 0.520590  [358400/900000]\n",
      "current batch loss: 0.643262  [364800/900000]\n",
      "current batch loss: 0.668998  [371200/900000]\n",
      "current batch loss: 0.647658  [377600/900000]\n",
      "current batch loss: 0.562637  [384000/900000]\n",
      "current batch loss: 0.580369  [390400/900000]\n",
      "current batch loss: 0.653263  [396800/900000]\n",
      "current batch loss: 0.699796  [403200/900000]\n",
      "current batch loss: 0.614070  [409600/900000]\n",
      "current batch loss: 0.644755  [416000/900000]\n",
      "current batch loss: 0.621593  [422400/900000]\n",
      "current batch loss: 0.664987  [428800/900000]\n",
      "current batch loss: 0.686453  [435200/900000]\n",
      "current batch loss: 0.612956  [441600/900000]\n",
      "current batch loss: 0.635393  [448000/900000]\n",
      "current batch loss: 0.680641  [454400/900000]\n",
      "current batch loss: 0.645517  [460800/900000]\n",
      "current batch loss: 0.633898  [467200/900000]\n",
      "current batch loss: 0.636873  [473600/900000]\n",
      "current batch loss: 0.639202  [480000/900000]\n",
      "current batch loss: 0.696058  [486400/900000]\n",
      "current batch loss: 0.623941  [492800/900000]\n",
      "current batch loss: 0.588804  [499200/900000]\n",
      "current batch loss: 0.596693  [505600/900000]\n",
      "current batch loss: 0.628869  [512000/900000]\n",
      "current batch loss: 0.570899  [518400/900000]\n",
      "current batch loss: 0.642125  [524800/900000]\n",
      "current batch loss: 0.658469  [531200/900000]\n",
      "current batch loss: 0.664262  [537600/900000]\n",
      "current batch loss: 0.651783  [544000/900000]\n",
      "current batch loss: 0.575292  [550400/900000]\n",
      "current batch loss: 0.676077  [556800/900000]\n",
      "current batch loss: 0.609752  [563200/900000]\n",
      "current batch loss: 0.618048  [569600/900000]\n",
      "current batch loss: 0.634558  [576000/900000]\n",
      "current batch loss: 0.649561  [582400/900000]\n",
      "current batch loss: 0.665457  [588800/900000]\n",
      "current batch loss: 0.634035  [595200/900000]\n",
      "current batch loss: 0.600230  [601600/900000]\n",
      "current batch loss: 0.586407  [608000/900000]\n",
      "current batch loss: 0.649355  [614400/900000]\n",
      "current batch loss: 0.608981  [620800/900000]\n",
      "current batch loss: 0.650723  [627200/900000]\n",
      "current batch loss: 0.621118  [633600/900000]\n",
      "current batch loss: 0.675619  [640000/900000]\n",
      "current batch loss: 0.629170  [646400/900000]\n",
      "current batch loss: 0.630536  [652800/900000]\n",
      "current batch loss: 0.702090  [659200/900000]\n",
      "current batch loss: 0.661488  [665600/900000]\n",
      "current batch loss: 0.606363  [672000/900000]\n",
      "current batch loss: 0.605592  [678400/900000]\n",
      "current batch loss: 0.627442  [684800/900000]\n",
      "current batch loss: 0.644563  [691200/900000]\n",
      "current batch loss: 0.634287  [697600/900000]\n",
      "current batch loss: 0.709614  [704000/900000]\n",
      "current batch loss: 0.738616  [710400/900000]\n",
      "current batch loss: 0.574479  [716800/900000]\n",
      "current batch loss: 0.616604  [723200/900000]\n",
      "current batch loss: 0.609483  [729600/900000]\n",
      "current batch loss: 0.611257  [736000/900000]\n",
      "current batch loss: 0.659090  [742400/900000]\n",
      "current batch loss: 0.754995  [748800/900000]\n",
      "current batch loss: 0.617416  [755200/900000]\n",
      "current batch loss: 0.696834  [761600/900000]\n",
      "current batch loss: 0.655999  [768000/900000]\n",
      "current batch loss: 0.721942  [774400/900000]\n",
      "current batch loss: 0.641189  [780800/900000]\n",
      "current batch loss: 0.602637  [787200/900000]\n",
      "current batch loss: 0.621630  [793600/900000]\n",
      "current batch loss: 0.582613  [800000/900000]\n",
      "current batch loss: 0.706384  [806400/900000]\n",
      "current batch loss: 0.638009  [812800/900000]\n",
      "current batch loss: 0.683184  [819200/900000]\n",
      "current batch loss: 0.622081  [825600/900000]\n",
      "current batch loss: 0.601269  [832000/900000]\n",
      "current batch loss: 0.760346  [838400/900000]\n",
      "current batch loss: 0.625804  [844800/900000]\n",
      "current batch loss: 0.651208  [851200/900000]\n",
      "current batch loss: 0.614078  [857600/900000]\n",
      "current batch loss: 0.574625  [864000/900000]\n",
      "current batch loss: 0.738735  [870400/900000]\n",
      "current batch loss: 0.637818  [876800/900000]\n",
      "current batch loss: 0.661432  [883200/900000]\n",
      "current batch loss: 0.640306  [889600/900000]\n",
      "current batch loss: 0.654531  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.640701\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.639295\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 3\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.608625  [    0/900000]\n",
      "current batch loss: 0.633897  [ 6400/900000]\n",
      "current batch loss: 0.628003  [12800/900000]\n",
      "current batch loss: 0.701692  [19200/900000]\n",
      "current batch loss: 0.638116  [25600/900000]\n",
      "current batch loss: 0.636217  [32000/900000]\n",
      "current batch loss: 0.642732  [38400/900000]\n",
      "current batch loss: 0.597082  [44800/900000]\n",
      "current batch loss: 0.663467  [51200/900000]\n",
      "current batch loss: 0.628857  [57600/900000]\n",
      "current batch loss: 0.653517  [64000/900000]\n",
      "current batch loss: 0.653034  [70400/900000]\n",
      "current batch loss: 0.573998  [76800/900000]\n",
      "current batch loss: 0.715611  [83200/900000]\n",
      "current batch loss: 0.583568  [89600/900000]\n",
      "current batch loss: 0.670876  [96000/900000]\n",
      "current batch loss: 0.710450  [102400/900000]\n",
      "current batch loss: 0.641862  [108800/900000]\n",
      "current batch loss: 0.638789  [115200/900000]\n",
      "current batch loss: 0.622144  [121600/900000]\n",
      "current batch loss: 0.670506  [128000/900000]\n",
      "current batch loss: 0.611203  [134400/900000]\n",
      "current batch loss: 0.614922  [140800/900000]\n",
      "current batch loss: 0.627734  [147200/900000]\n",
      "current batch loss: 0.656554  [153600/900000]\n",
      "current batch loss: 0.694376  [160000/900000]\n",
      "current batch loss: 0.626341  [166400/900000]\n",
      "current batch loss: 0.601803  [172800/900000]\n",
      "current batch loss: 0.642378  [179200/900000]\n",
      "current batch loss: 0.678923  [185600/900000]\n",
      "current batch loss: 0.733659  [192000/900000]\n",
      "current batch loss: 0.598917  [198400/900000]\n",
      "current batch loss: 0.649844  [204800/900000]\n",
      "current batch loss: 0.624111  [211200/900000]\n",
      "current batch loss: 0.642858  [217600/900000]\n",
      "current batch loss: 0.612829  [224000/900000]\n",
      "current batch loss: 0.621290  [230400/900000]\n",
      "current batch loss: 0.609946  [236800/900000]\n",
      "current batch loss: 0.666525  [243200/900000]\n",
      "current batch loss: 0.709384  [249600/900000]\n",
      "current batch loss: 0.676296  [256000/900000]\n",
      "current batch loss: 0.657471  [262400/900000]\n",
      "current batch loss: 0.664681  [268800/900000]\n",
      "current batch loss: 0.647139  [275200/900000]\n",
      "current batch loss: 0.710605  [281600/900000]\n",
      "current batch loss: 0.675379  [288000/900000]\n",
      "current batch loss: 0.617560  [294400/900000]\n",
      "current batch loss: 0.680355  [300800/900000]\n",
      "current batch loss: 0.603058  [307200/900000]\n",
      "current batch loss: 0.643520  [313600/900000]\n",
      "current batch loss: 0.684601  [320000/900000]\n",
      "current batch loss: 0.646896  [326400/900000]\n",
      "current batch loss: 0.644555  [332800/900000]\n",
      "current batch loss: 0.590626  [339200/900000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.641714  [345600/900000]\n",
      "current batch loss: 0.620782  [352000/900000]\n",
      "current batch loss: 0.646052  [358400/900000]\n",
      "current batch loss: 0.725117  [364800/900000]\n",
      "current batch loss: 0.645554  [371200/900000]\n",
      "current batch loss: 0.662990  [377600/900000]\n",
      "current batch loss: 0.622760  [384000/900000]\n",
      "current batch loss: 0.587172  [390400/900000]\n",
      "current batch loss: 0.637648  [396800/900000]\n",
      "current batch loss: 0.628529  [403200/900000]\n",
      "current batch loss: 0.713997  [409600/900000]\n",
      "current batch loss: 0.608490  [416000/900000]\n",
      "current batch loss: 0.615936  [422400/900000]\n",
      "current batch loss: 0.666304  [428800/900000]\n",
      "current batch loss: 0.708353  [435200/900000]\n",
      "current batch loss: 0.612310  [441600/900000]\n",
      "current batch loss: 0.591168  [448000/900000]\n",
      "current batch loss: 0.724555  [454400/900000]\n",
      "current batch loss: 0.579382  [460800/900000]\n",
      "current batch loss: 0.601808  [467200/900000]\n",
      "current batch loss: 0.656133  [473600/900000]\n",
      "current batch loss: 0.584865  [480000/900000]\n",
      "current batch loss: 0.610151  [486400/900000]\n",
      "current batch loss: 0.706795  [492800/900000]\n",
      "current batch loss: 0.727057  [499200/900000]\n",
      "current batch loss: 0.619534  [505600/900000]\n",
      "current batch loss: 0.553139  [512000/900000]\n",
      "current batch loss: 0.662669  [518400/900000]\n",
      "current batch loss: 0.674010  [524800/900000]\n",
      "current batch loss: 0.681443  [531200/900000]\n",
      "current batch loss: 0.630932  [537600/900000]\n",
      "current batch loss: 0.703928  [544000/900000]\n",
      "current batch loss: 0.625878  [550400/900000]\n",
      "current batch loss: 0.623583  [556800/900000]\n",
      "current batch loss: 0.696452  [563200/900000]\n",
      "current batch loss: 0.689334  [569600/900000]\n",
      "current batch loss: 0.681414  [576000/900000]\n",
      "current batch loss: 0.619513  [582400/900000]\n",
      "current batch loss: 0.696216  [588800/900000]\n",
      "current batch loss: 0.633575  [595200/900000]\n",
      "current batch loss: 0.728374  [601600/900000]\n",
      "current batch loss: 0.701421  [608000/900000]\n",
      "current batch loss: 0.649894  [614400/900000]\n",
      "current batch loss: 0.647206  [620800/900000]\n",
      "current batch loss: 0.615728  [627200/900000]\n",
      "current batch loss: 0.622938  [633600/900000]\n",
      "current batch loss: 0.662150  [640000/900000]\n",
      "current batch loss: 0.586821  [646400/900000]\n",
      "current batch loss: 0.648504  [652800/900000]\n",
      "current batch loss: 0.604893  [659200/900000]\n",
      "current batch loss: 0.702072  [665600/900000]\n",
      "current batch loss: 0.740308  [672000/900000]\n",
      "current batch loss: 0.599333  [678400/900000]\n",
      "current batch loss: 0.648676  [684800/900000]\n",
      "current batch loss: 0.629850  [691200/900000]\n",
      "current batch loss: 0.640250  [697600/900000]\n",
      "current batch loss: 0.660201  [704000/900000]\n",
      "current batch loss: 0.709287  [710400/900000]\n",
      "current batch loss: 0.648420  [716800/900000]\n",
      "current batch loss: 0.657477  [723200/900000]\n",
      "current batch loss: 0.640520  [729600/900000]\n",
      "current batch loss: 0.640562  [736000/900000]\n",
      "current batch loss: 0.758415  [742400/900000]\n",
      "current batch loss: 0.559647  [748800/900000]\n",
      "current batch loss: 0.661605  [755200/900000]\n",
      "current batch loss: 0.634357  [761600/900000]\n",
      "current batch loss: 0.669810  [768000/900000]\n",
      "current batch loss: 0.637540  [774400/900000]\n",
      "current batch loss: 0.647499  [780800/900000]\n",
      "current batch loss: 0.586842  [787200/900000]\n",
      "current batch loss: 0.689823  [793600/900000]\n",
      "current batch loss: 0.667682  [800000/900000]\n",
      "current batch loss: 0.728404  [806400/900000]\n",
      "current batch loss: 0.675855  [812800/900000]\n",
      "current batch loss: 0.668454  [819200/900000]\n",
      "current batch loss: 0.588588  [825600/900000]\n",
      "current batch loss: 0.600450  [832000/900000]\n",
      "current batch loss: 0.687656  [838400/900000]\n",
      "current batch loss: 0.604841  [844800/900000]\n",
      "current batch loss: 0.601666  [851200/900000]\n",
      "current batch loss: 0.641382  [857600/900000]\n",
      "current batch loss: 0.652565  [864000/900000]\n",
      "current batch loss: 0.667332  [870400/900000]\n",
      "current batch loss: 0.630390  [876800/900000]\n",
      "current batch loss: 0.622299  [883200/900000]\n",
      "current batch loss: 0.685285  [889600/900000]\n",
      "current batch loss: 0.636261  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.641544\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.639727\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 4\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.692937  [    0/900000]\n",
      "current batch loss: 0.586226  [ 6400/900000]\n",
      "current batch loss: 0.663599  [12800/900000]\n",
      "current batch loss: 0.625043  [19200/900000]\n",
      "current batch loss: 0.639310  [25600/900000]\n",
      "current batch loss: 0.613963  [32000/900000]\n",
      "current batch loss: 0.648397  [38400/900000]\n",
      "current batch loss: 0.685522  [44800/900000]\n",
      "current batch loss: 0.578607  [51200/900000]\n",
      "current batch loss: 0.600835  [57600/900000]\n",
      "current batch loss: 0.635235  [64000/900000]\n",
      "current batch loss: 0.638802  [70400/900000]\n",
      "current batch loss: 0.700105  [76800/900000]\n",
      "current batch loss: 0.690720  [83200/900000]\n",
      "current batch loss: 0.614316  [89600/900000]\n",
      "current batch loss: 0.649424  [96000/900000]\n",
      "current batch loss: 0.649470  [102400/900000]\n",
      "current batch loss: 0.662830  [108800/900000]\n",
      "current batch loss: 0.722746  [115200/900000]\n",
      "current batch loss: 0.615040  [121600/900000]\n",
      "current batch loss: 0.676775  [128000/900000]\n",
      "current batch loss: 0.683018  [134400/900000]\n",
      "current batch loss: 0.654401  [140800/900000]\n",
      "current batch loss: 0.644581  [147200/900000]\n",
      "current batch loss: 0.611995  [153600/900000]\n",
      "current batch loss: 0.622158  [160000/900000]\n",
      "current batch loss: 0.582566  [166400/900000]\n",
      "current batch loss: 0.603576  [172800/900000]\n",
      "current batch loss: 0.640017  [179200/900000]\n",
      "current batch loss: 0.593777  [185600/900000]\n",
      "current batch loss: 0.584208  [192000/900000]\n",
      "current batch loss: 0.731793  [198400/900000]\n",
      "current batch loss: 0.574091  [204800/900000]\n",
      "current batch loss: 0.582624  [211200/900000]\n",
      "current batch loss: 0.638221  [217600/900000]\n",
      "current batch loss: 0.645875  [224000/900000]\n",
      "current batch loss: 0.669383  [230400/900000]\n",
      "current batch loss: 0.655561  [236800/900000]\n",
      "current batch loss: 0.672776  [243200/900000]\n",
      "current batch loss: 0.717488  [249600/900000]\n",
      "current batch loss: 0.566516  [256000/900000]\n",
      "current batch loss: 0.580123  [262400/900000]\n",
      "current batch loss: 0.702144  [268800/900000]\n",
      "current batch loss: 0.594607  [275200/900000]\n",
      "current batch loss: 0.578623  [281600/900000]\n",
      "current batch loss: 0.614349  [288000/900000]\n",
      "current batch loss: 0.663749  [294400/900000]\n",
      "current batch loss: 0.645853  [300800/900000]\n",
      "current batch loss: 0.658036  [307200/900000]\n",
      "current batch loss: 0.637988  [313600/900000]\n",
      "current batch loss: 0.588417  [320000/900000]\n",
      "current batch loss: 0.609754  [326400/900000]\n",
      "current batch loss: 0.648972  [332800/900000]\n",
      "current batch loss: 0.658015  [339200/900000]\n",
      "current batch loss: 0.614999  [345600/900000]\n",
      "current batch loss: 0.620570  [352000/900000]\n",
      "current batch loss: 0.597719  [358400/900000]\n",
      "current batch loss: 0.653932  [364800/900000]\n",
      "current batch loss: 0.662727  [371200/900000]\n",
      "current batch loss: 0.669415  [377600/900000]\n",
      "current batch loss: 0.695524  [384000/900000]\n",
      "current batch loss: 0.687002  [390400/900000]\n",
      "current batch loss: 0.701133  [396800/900000]\n",
      "current batch loss: 0.568626  [403200/900000]\n",
      "current batch loss: 0.622192  [409600/900000]\n",
      "current batch loss: 0.688740  [416000/900000]\n",
      "current batch loss: 0.633676  [422400/900000]\n",
      "current batch loss: 0.573331  [428800/900000]\n",
      "current batch loss: 0.620529  [435200/900000]\n",
      "current batch loss: 0.624381  [441600/900000]\n",
      "current batch loss: 0.595726  [448000/900000]\n",
      "current batch loss: 0.684370  [454400/900000]\n",
      "current batch loss: 0.651269  [460800/900000]\n",
      "current batch loss: 0.695088  [467200/900000]\n",
      "current batch loss: 0.655301  [473600/900000]\n",
      "current batch loss: 0.556810  [480000/900000]\n",
      "current batch loss: 0.663699  [486400/900000]\n",
      "current batch loss: 0.666052  [492800/900000]\n",
      "current batch loss: 0.589527  [499200/900000]\n",
      "current batch loss: 0.646232  [505600/900000]\n",
      "current batch loss: 0.606674  [512000/900000]\n",
      "current batch loss: 0.656351  [518400/900000]\n",
      "current batch loss: 0.651225  [524800/900000]\n",
      "current batch loss: 0.685136  [531200/900000]\n",
      "current batch loss: 0.629716  [537600/900000]\n",
      "current batch loss: 0.599178  [544000/900000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.682218  [550400/900000]\n",
      "current batch loss: 0.641802  [556800/900000]\n",
      "current batch loss: 0.688235  [563200/900000]\n",
      "current batch loss: 0.611098  [569600/900000]\n",
      "current batch loss: 0.633244  [576000/900000]\n",
      "current batch loss: 0.687024  [582400/900000]\n",
      "current batch loss: 0.561944  [588800/900000]\n",
      "current batch loss: 0.625899  [595200/900000]\n",
      "current batch loss: 0.560178  [601600/900000]\n",
      "current batch loss: 0.629178  [608000/900000]\n",
      "current batch loss: 0.605407  [614400/900000]\n",
      "current batch loss: 0.605940  [620800/900000]\n",
      "current batch loss: 0.610528  [627200/900000]\n",
      "current batch loss: 0.692517  [633600/900000]\n",
      "current batch loss: 0.660598  [640000/900000]\n",
      "current batch loss: 0.716291  [646400/900000]\n",
      "current batch loss: 0.647621  [652800/900000]\n",
      "current batch loss: 0.651474  [659200/900000]\n",
      "current batch loss: 0.637915  [665600/900000]\n",
      "current batch loss: 0.642571  [672000/900000]\n",
      "current batch loss: 0.579958  [678400/900000]\n",
      "current batch loss: 0.638758  [684800/900000]\n",
      "current batch loss: 0.585009  [691200/900000]\n",
      "current batch loss: 0.578026  [697600/900000]\n",
      "current batch loss: 0.645825  [704000/900000]\n",
      "current batch loss: 0.605451  [710400/900000]\n",
      "current batch loss: 0.635755  [716800/900000]\n",
      "current batch loss: 0.628894  [723200/900000]\n",
      "current batch loss: 0.540308  [729600/900000]\n",
      "current batch loss: 0.667294  [736000/900000]\n",
      "current batch loss: 0.564932  [742400/900000]\n",
      "current batch loss: 0.656107  [748800/900000]\n",
      "current batch loss: 0.651901  [755200/900000]\n",
      "current batch loss: 0.624921  [761600/900000]\n",
      "current batch loss: 0.695546  [768000/900000]\n",
      "current batch loss: 0.589661  [774400/900000]\n",
      "current batch loss: 0.614049  [780800/900000]\n",
      "current batch loss: 0.642141  [787200/900000]\n",
      "current batch loss: 0.628545  [793600/900000]\n",
      "current batch loss: 0.594084  [800000/900000]\n",
      "current batch loss: 0.656291  [806400/900000]\n",
      "current batch loss: 0.564481  [812800/900000]\n",
      "current batch loss: 0.737232  [819200/900000]\n",
      "current batch loss: 0.652298  [825600/900000]\n",
      "current batch loss: 0.614063  [832000/900000]\n",
      "current batch loss: 0.561814  [838400/900000]\n",
      "current batch loss: 0.671210  [844800/900000]\n",
      "current batch loss: 0.611361  [851200/900000]\n",
      "current batch loss: 0.682689  [857600/900000]\n",
      "current batch loss: 0.578505  [864000/900000]\n",
      "current batch loss: 0.694740  [870400/900000]\n",
      "current batch loss: 0.653873  [876800/900000]\n",
      "current batch loss: 0.628363  [883200/900000]\n",
      "current batch loss: 0.655913  [889600/900000]\n",
      "current batch loss: 0.650050  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.640040\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.638861\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 5\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.600231  [    0/900000]\n",
      "current batch loss: 0.663093  [ 6400/900000]\n",
      "current batch loss: 0.747788  [12800/900000]\n",
      "current batch loss: 0.591739  [19200/900000]\n",
      "current batch loss: 0.638880  [25600/900000]\n",
      "current batch loss: 0.629749  [32000/900000]\n",
      "current batch loss: 0.620147  [38400/900000]\n",
      "current batch loss: 0.633641  [44800/900000]\n",
      "current batch loss: 0.612900  [51200/900000]\n",
      "current batch loss: 0.656666  [57600/900000]\n",
      "current batch loss: 0.683903  [64000/900000]\n",
      "current batch loss: 0.675784  [70400/900000]\n",
      "current batch loss: 0.638332  [76800/900000]\n",
      "current batch loss: 0.633567  [83200/900000]\n",
      "current batch loss: 0.709627  [89600/900000]\n",
      "current batch loss: 0.555861  [96000/900000]\n",
      "current batch loss: 0.612684  [102400/900000]\n",
      "current batch loss: 0.629641  [108800/900000]\n",
      "current batch loss: 0.654868  [115200/900000]\n",
      "current batch loss: 0.610604  [121600/900000]\n",
      "current batch loss: 0.686580  [128000/900000]\n",
      "current batch loss: 0.697170  [134400/900000]\n",
      "current batch loss: 0.658053  [140800/900000]\n",
      "current batch loss: 0.680395  [147200/900000]\n",
      "current batch loss: 0.596765  [153600/900000]\n",
      "current batch loss: 0.576360  [160000/900000]\n",
      "current batch loss: 0.624141  [166400/900000]\n",
      "current batch loss: 0.661051  [172800/900000]\n",
      "current batch loss: 0.639750  [179200/900000]\n",
      "current batch loss: 0.672959  [185600/900000]\n",
      "current batch loss: 0.617400  [192000/900000]\n",
      "current batch loss: 0.618097  [198400/900000]\n",
      "current batch loss: 0.640022  [204800/900000]\n",
      "current batch loss: 0.655328  [211200/900000]\n",
      "current batch loss: 0.609236  [217600/900000]\n",
      "current batch loss: 0.617515  [224000/900000]\n",
      "current batch loss: 0.646148  [230400/900000]\n",
      "current batch loss: 0.628860  [236800/900000]\n",
      "current batch loss: 0.611702  [243200/900000]\n",
      "current batch loss: 0.671384  [249600/900000]\n",
      "current batch loss: 0.628352  [256000/900000]\n",
      "current batch loss: 0.623089  [262400/900000]\n",
      "current batch loss: 0.677753  [268800/900000]\n",
      "current batch loss: 0.620762  [275200/900000]\n",
      "current batch loss: 0.672045  [281600/900000]\n",
      "current batch loss: 0.672599  [288000/900000]\n",
      "current batch loss: 0.531794  [294400/900000]\n",
      "current batch loss: 0.651379  [300800/900000]\n",
      "current batch loss: 0.670781  [307200/900000]\n",
      "current batch loss: 0.682061  [313600/900000]\n",
      "current batch loss: 0.673962  [320000/900000]\n",
      "current batch loss: 0.648401  [326400/900000]\n",
      "current batch loss: 0.646122  [332800/900000]\n",
      "current batch loss: 0.591699  [339200/900000]\n",
      "current batch loss: 0.685754  [345600/900000]\n",
      "current batch loss: 0.597965  [352000/900000]\n",
      "current batch loss: 0.703498  [358400/900000]\n",
      "current batch loss: 0.627353  [364800/900000]\n",
      "current batch loss: 0.621301  [371200/900000]\n",
      "current batch loss: 0.652168  [377600/900000]\n",
      "current batch loss: 0.662127  [384000/900000]\n",
      "current batch loss: 0.673955  [390400/900000]\n",
      "current batch loss: 0.595679  [396800/900000]\n",
      "current batch loss: 0.725525  [403200/900000]\n",
      "current batch loss: 0.686829  [409600/900000]\n",
      "current batch loss: 0.613076  [416000/900000]\n",
      "current batch loss: 0.658033  [422400/900000]\n",
      "current batch loss: 0.710083  [428800/900000]\n",
      "current batch loss: 0.675808  [435200/900000]\n",
      "current batch loss: 0.647846  [441600/900000]\n",
      "current batch loss: 0.589259  [448000/900000]\n",
      "current batch loss: 0.631991  [454400/900000]\n",
      "current batch loss: 0.682622  [460800/900000]\n",
      "current batch loss: 0.652007  [467200/900000]\n",
      "current batch loss: 0.650482  [473600/900000]\n",
      "current batch loss: 0.667318  [480000/900000]\n",
      "current batch loss: 0.676895  [486400/900000]\n",
      "current batch loss: 0.654901  [492800/900000]\n",
      "current batch loss: 0.633142  [499200/900000]\n",
      "current batch loss: 0.556275  [505600/900000]\n",
      "current batch loss: 0.642150  [512000/900000]\n",
      "current batch loss: 0.681240  [518400/900000]\n",
      "current batch loss: 0.663977  [524800/900000]\n",
      "current batch loss: 0.640124  [531200/900000]\n",
      "current batch loss: 0.633060  [537600/900000]\n",
      "current batch loss: 0.616551  [544000/900000]\n",
      "current batch loss: 0.719550  [550400/900000]\n",
      "current batch loss: 0.594834  [556800/900000]\n",
      "current batch loss: 0.573899  [563200/900000]\n",
      "current batch loss: 0.617858  [569600/900000]\n",
      "current batch loss: 0.637859  [576000/900000]\n",
      "current batch loss: 0.618943  [582400/900000]\n",
      "current batch loss: 0.667355  [588800/900000]\n",
      "current batch loss: 0.710050  [595200/900000]\n",
      "current batch loss: 0.670969  [601600/900000]\n",
      "current batch loss: 0.654343  [608000/900000]\n",
      "current batch loss: 0.591953  [614400/900000]\n",
      "current batch loss: 0.665116  [620800/900000]\n",
      "current batch loss: 0.639609  [627200/900000]\n",
      "current batch loss: 0.694027  [633600/900000]\n",
      "current batch loss: 0.680548  [640000/900000]\n",
      "current batch loss: 0.697852  [646400/900000]\n",
      "current batch loss: 0.608484  [652800/900000]\n",
      "current batch loss: 0.674126  [659200/900000]\n",
      "current batch loss: 0.661079  [665600/900000]\n",
      "current batch loss: 0.619075  [672000/900000]\n",
      "current batch loss: 0.648741  [678400/900000]\n",
      "current batch loss: 0.638189  [684800/900000]\n",
      "current batch loss: 0.681293  [691200/900000]\n",
      "current batch loss: 0.655325  [697600/900000]\n",
      "current batch loss: 0.686223  [704000/900000]\n",
      "current batch loss: 0.669341  [710400/900000]\n",
      "current batch loss: 0.561973  [716800/900000]\n",
      "current batch loss: 0.673989  [723200/900000]\n",
      "current batch loss: 0.549056  [729600/900000]\n",
      "current batch loss: 0.626911  [736000/900000]\n",
      "current batch loss: 0.621164  [742400/900000]\n",
      "current batch loss: 0.581952  [748800/900000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.699533  [755200/900000]\n",
      "current batch loss: 0.576494  [761600/900000]\n",
      "current batch loss: 0.651361  [768000/900000]\n",
      "current batch loss: 0.606318  [774400/900000]\n",
      "current batch loss: 0.628030  [780800/900000]\n",
      "current batch loss: 0.634998  [787200/900000]\n",
      "current batch loss: 0.639655  [793600/900000]\n",
      "current batch loss: 0.673952  [800000/900000]\n",
      "current batch loss: 0.661152  [806400/900000]\n",
      "current batch loss: 0.675429  [812800/900000]\n",
      "current batch loss: 0.654602  [819200/900000]\n",
      "current batch loss: 0.626298  [825600/900000]\n",
      "current batch loss: 0.670072  [832000/900000]\n",
      "current batch loss: 0.630080  [838400/900000]\n",
      "current batch loss: 0.693717  [844800/900000]\n",
      "current batch loss: 0.631245  [851200/900000]\n",
      "current batch loss: 0.667090  [857600/900000]\n",
      "current batch loss: 0.637884  [864000/900000]\n",
      "current batch loss: 0.639375  [870400/900000]\n",
      "current batch loss: 0.645481  [876800/900000]\n",
      "current batch loss: 0.660746  [883200/900000]\n",
      "current batch loss: 0.636274  [889600/900000]\n",
      "current batch loss: 0.698851  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.639230\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.637842\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 6\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.682576  [    0/900000]\n",
      "current batch loss: 0.588425  [ 6400/900000]\n",
      "current batch loss: 0.589868  [12800/900000]\n",
      "current batch loss: 0.698706  [19200/900000]\n",
      "current batch loss: 0.721411  [25600/900000]\n",
      "current batch loss: 0.569202  [32000/900000]\n",
      "current batch loss: 0.693150  [38400/900000]\n",
      "current batch loss: 0.638290  [44800/900000]\n",
      "current batch loss: 0.645176  [51200/900000]\n",
      "current batch loss: 0.597940  [57600/900000]\n",
      "current batch loss: 0.626474  [64000/900000]\n",
      "current batch loss: 0.619497  [70400/900000]\n",
      "current batch loss: 0.691806  [76800/900000]\n",
      "current batch loss: 0.601012  [83200/900000]\n",
      "current batch loss: 0.613770  [89600/900000]\n",
      "current batch loss: 0.791284  [96000/900000]\n",
      "current batch loss: 0.578951  [102400/900000]\n",
      "current batch loss: 0.641281  [108800/900000]\n",
      "current batch loss: 0.667340  [115200/900000]\n",
      "current batch loss: 0.693564  [121600/900000]\n",
      "current batch loss: 0.616660  [128000/900000]\n",
      "current batch loss: 0.646826  [134400/900000]\n",
      "current batch loss: 0.669151  [140800/900000]\n",
      "current batch loss: 0.629703  [147200/900000]\n",
      "current batch loss: 0.620930  [153600/900000]\n",
      "current batch loss: 0.653464  [160000/900000]\n",
      "current batch loss: 0.637956  [166400/900000]\n",
      "current batch loss: 0.663276  [172800/900000]\n",
      "current batch loss: 0.598068  [179200/900000]\n",
      "current batch loss: 0.631542  [185600/900000]\n",
      "current batch loss: 0.596643  [192000/900000]\n",
      "current batch loss: 0.604511  [198400/900000]\n",
      "current batch loss: 0.613697  [204800/900000]\n",
      "current batch loss: 0.675765  [211200/900000]\n",
      "current batch loss: 0.640645  [217600/900000]\n",
      "current batch loss: 0.628664  [224000/900000]\n",
      "current batch loss: 0.678188  [230400/900000]\n",
      "current batch loss: 0.663664  [236800/900000]\n",
      "current batch loss: 0.633094  [243200/900000]\n",
      "current batch loss: 0.706927  [249600/900000]\n",
      "current batch loss: 0.617039  [256000/900000]\n",
      "current batch loss: 0.566708  [262400/900000]\n",
      "current batch loss: 0.678969  [268800/900000]\n",
      "current batch loss: 0.717229  [275200/900000]\n",
      "current batch loss: 0.682634  [281600/900000]\n",
      "current batch loss: 0.591878  [288000/900000]\n",
      "current batch loss: 0.657592  [294400/900000]\n",
      "current batch loss: 0.629673  [300800/900000]\n",
      "current batch loss: 0.587784  [307200/900000]\n",
      "current batch loss: 0.609838  [313600/900000]\n",
      "current batch loss: 0.671863  [320000/900000]\n",
      "current batch loss: 0.659557  [326400/900000]\n",
      "current batch loss: 0.625810  [332800/900000]\n",
      "current batch loss: 0.553824  [339200/900000]\n",
      "current batch loss: 0.611854  [345600/900000]\n",
      "current batch loss: 0.637740  [352000/900000]\n",
      "current batch loss: 0.594717  [358400/900000]\n",
      "current batch loss: 0.650823  [364800/900000]\n",
      "current batch loss: 0.602714  [371200/900000]\n",
      "current batch loss: 0.625043  [377600/900000]\n",
      "current batch loss: 0.661222  [384000/900000]\n",
      "current batch loss: 0.580279  [390400/900000]\n",
      "current batch loss: 0.624458  [396800/900000]\n",
      "current batch loss: 0.687207  [403200/900000]\n",
      "current batch loss: 0.640458  [409600/900000]\n",
      "current batch loss: 0.601833  [416000/900000]\n",
      "current batch loss: 0.620542  [422400/900000]\n",
      "current batch loss: 0.619825  [428800/900000]\n",
      "current batch loss: 0.655506  [435200/900000]\n",
      "current batch loss: 0.664624  [441600/900000]\n",
      "current batch loss: 0.644619  [448000/900000]\n",
      "current batch loss: 0.620997  [454400/900000]\n",
      "current batch loss: 0.638066  [460800/900000]\n",
      "current batch loss: 0.613413  [467200/900000]\n",
      "current batch loss: 0.693033  [473600/900000]\n",
      "current batch loss: 0.655102  [480000/900000]\n",
      "current batch loss: 0.610097  [486400/900000]\n",
      "current batch loss: 0.635987  [492800/900000]\n",
      "current batch loss: 0.637714  [499200/900000]\n",
      "current batch loss: 0.646456  [505600/900000]\n",
      "current batch loss: 0.688928  [512000/900000]\n",
      "current batch loss: 0.583810  [518400/900000]\n",
      "current batch loss: 0.615073  [524800/900000]\n",
      "current batch loss: 0.658532  [531200/900000]\n",
      "current batch loss: 0.663265  [537600/900000]\n",
      "current batch loss: 0.553159  [544000/900000]\n",
      "current batch loss: 0.700872  [550400/900000]\n",
      "current batch loss: 0.595749  [556800/900000]\n",
      "current batch loss: 0.642367  [563200/900000]\n",
      "current batch loss: 0.641072  [569600/900000]\n",
      "current batch loss: 0.679739  [576000/900000]\n",
      "current batch loss: 0.632712  [582400/900000]\n",
      "current batch loss: 0.602838  [588800/900000]\n",
      "current batch loss: 0.633408  [595200/900000]\n",
      "current batch loss: 0.667491  [601600/900000]\n",
      "current batch loss: 0.678382  [608000/900000]\n",
      "current batch loss: 0.690073  [614400/900000]\n",
      "current batch loss: 0.626062  [620800/900000]\n",
      "current batch loss: 0.652469  [627200/900000]\n",
      "current batch loss: 0.666159  [633600/900000]\n",
      "current batch loss: 0.645134  [640000/900000]\n",
      "current batch loss: 0.681038  [646400/900000]\n",
      "current batch loss: 0.619227  [652800/900000]\n",
      "current batch loss: 0.623814  [659200/900000]\n",
      "current batch loss: 0.713464  [665600/900000]\n",
      "current batch loss: 0.626029  [672000/900000]\n",
      "current batch loss: 0.615741  [678400/900000]\n",
      "current batch loss: 0.613705  [684800/900000]\n",
      "current batch loss: 0.637533  [691200/900000]\n",
      "current batch loss: 0.608640  [697600/900000]\n",
      "current batch loss: 0.701466  [704000/900000]\n",
      "current batch loss: 0.626877  [710400/900000]\n",
      "current batch loss: 0.653922  [716800/900000]\n",
      "current batch loss: 0.661378  [723200/900000]\n",
      "current batch loss: 0.635925  [729600/900000]\n",
      "current batch loss: 0.587054  [736000/900000]\n",
      "current batch loss: 0.678445  [742400/900000]\n",
      "current batch loss: 0.568862  [748800/900000]\n",
      "current batch loss: 0.648674  [755200/900000]\n",
      "current batch loss: 0.625016  [761600/900000]\n",
      "current batch loss: 0.724145  [768000/900000]\n",
      "current batch loss: 0.608864  [774400/900000]\n",
      "current batch loss: 0.742318  [780800/900000]\n",
      "current batch loss: 0.597093  [787200/900000]\n",
      "current batch loss: 0.620650  [793600/900000]\n",
      "current batch loss: 0.636334  [800000/900000]\n",
      "current batch loss: 0.634045  [806400/900000]\n",
      "current batch loss: 0.657694  [812800/900000]\n",
      "current batch loss: 0.677368  [819200/900000]\n",
      "current batch loss: 0.607931  [825600/900000]\n",
      "current batch loss: 0.737569  [832000/900000]\n",
      "current batch loss: 0.643823  [838400/900000]\n",
      "current batch loss: 0.699856  [844800/900000]\n",
      "current batch loss: 0.573374  [851200/900000]\n",
      "current batch loss: 0.660514  [857600/900000]\n",
      "current batch loss: 0.678747  [864000/900000]\n",
      "current batch loss: 0.651114  [870400/900000]\n",
      "current batch loss: 0.658619  [876800/900000]\n",
      "current batch loss: 0.637741  [883200/900000]\n",
      "current batch loss: 0.673622  [889600/900000]\n",
      "current batch loss: 0.601981  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.639099\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.638008\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 7\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.658055  [    0/900000]\n",
      "current batch loss: 0.584099  [ 6400/900000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.650562  [12800/900000]\n",
      "current batch loss: 0.667250  [19200/900000]\n",
      "current batch loss: 0.601323  [25600/900000]\n",
      "current batch loss: 0.676872  [32000/900000]\n",
      "current batch loss: 0.615221  [38400/900000]\n",
      "current batch loss: 0.726057  [44800/900000]\n",
      "current batch loss: 0.654140  [51200/900000]\n",
      "current batch loss: 0.624810  [57600/900000]\n",
      "current batch loss: 0.584997  [64000/900000]\n",
      "current batch loss: 0.660068  [70400/900000]\n",
      "current batch loss: 0.652555  [76800/900000]\n",
      "current batch loss: 0.668292  [83200/900000]\n",
      "current batch loss: 0.641325  [89600/900000]\n",
      "current batch loss: 0.603614  [96000/900000]\n",
      "current batch loss: 0.631374  [102400/900000]\n",
      "current batch loss: 0.656349  [108800/900000]\n",
      "current batch loss: 0.602954  [115200/900000]\n",
      "current batch loss: 0.614588  [121600/900000]\n",
      "current batch loss: 0.669171  [128000/900000]\n",
      "current batch loss: 0.641800  [134400/900000]\n",
      "current batch loss: 0.665062  [140800/900000]\n",
      "current batch loss: 0.689221  [147200/900000]\n",
      "current batch loss: 0.593538  [153600/900000]\n",
      "current batch loss: 0.647798  [160000/900000]\n",
      "current batch loss: 0.580608  [166400/900000]\n",
      "current batch loss: 0.685537  [172800/900000]\n",
      "current batch loss: 0.656227  [179200/900000]\n",
      "current batch loss: 0.620021  [185600/900000]\n",
      "current batch loss: 0.623594  [192000/900000]\n",
      "current batch loss: 0.602901  [198400/900000]\n",
      "current batch loss: 0.609585  [204800/900000]\n",
      "current batch loss: 0.592350  [211200/900000]\n",
      "current batch loss: 0.574464  [217600/900000]\n",
      "current batch loss: 0.616067  [224000/900000]\n",
      "current batch loss: 0.677156  [230400/900000]\n",
      "current batch loss: 0.662570  [236800/900000]\n",
      "current batch loss: 0.675718  [243200/900000]\n",
      "current batch loss: 0.667197  [249600/900000]\n",
      "current batch loss: 0.644579  [256000/900000]\n",
      "current batch loss: 0.672719  [262400/900000]\n",
      "current batch loss: 0.601647  [268800/900000]\n",
      "current batch loss: 0.638917  [275200/900000]\n",
      "current batch loss: 0.630408  [281600/900000]\n",
      "current batch loss: 0.671394  [288000/900000]\n",
      "current batch loss: 0.652171  [294400/900000]\n",
      "current batch loss: 0.652939  [300800/900000]\n",
      "current batch loss: 0.583038  [307200/900000]\n",
      "current batch loss: 0.575782  [313600/900000]\n",
      "current batch loss: 0.591865  [320000/900000]\n",
      "current batch loss: 0.647570  [326400/900000]\n",
      "current batch loss: 0.600294  [332800/900000]\n",
      "current batch loss: 0.634311  [339200/900000]\n",
      "current batch loss: 0.623729  [345600/900000]\n",
      "current batch loss: 0.621266  [352000/900000]\n",
      "current batch loss: 0.557113  [358400/900000]\n",
      "current batch loss: 0.590699  [364800/900000]\n",
      "current batch loss: 0.646648  [371200/900000]\n",
      "current batch loss: 0.630311  [377600/900000]\n",
      "current batch loss: 0.624676  [384000/900000]\n",
      "current batch loss: 0.656303  [390400/900000]\n",
      "current batch loss: 0.598852  [396800/900000]\n",
      "current batch loss: 0.577588  [403200/900000]\n",
      "current batch loss: 0.681855  [409600/900000]\n",
      "current batch loss: 0.720247  [416000/900000]\n",
      "current batch loss: 0.623559  [422400/900000]\n",
      "current batch loss: 0.641764  [428800/900000]\n",
      "current batch loss: 0.597927  [435200/900000]\n",
      "current batch loss: 0.668612  [441600/900000]\n",
      "current batch loss: 0.712375  [448000/900000]\n",
      "current batch loss: 0.649254  [454400/900000]\n",
      "current batch loss: 0.701089  [460800/900000]\n",
      "current batch loss: 0.611195  [467200/900000]\n",
      "current batch loss: 0.673645  [473600/900000]\n",
      "current batch loss: 0.663082  [480000/900000]\n",
      "current batch loss: 0.632555  [486400/900000]\n",
      "current batch loss: 0.642146  [492800/900000]\n",
      "current batch loss: 0.686499  [499200/900000]\n",
      "current batch loss: 0.654993  [505600/900000]\n",
      "current batch loss: 0.670924  [512000/900000]\n",
      "current batch loss: 0.652644  [518400/900000]\n",
      "current batch loss: 0.627484  [524800/900000]\n",
      "current batch loss: 0.625406  [531200/900000]\n",
      "current batch loss: 0.661492  [537600/900000]\n",
      "current batch loss: 0.648943  [544000/900000]\n",
      "current batch loss: 0.607026  [550400/900000]\n",
      "current batch loss: 0.636793  [556800/900000]\n",
      "current batch loss: 0.602647  [563200/900000]\n",
      "current batch loss: 0.591300  [569600/900000]\n",
      "current batch loss: 0.606140  [576000/900000]\n",
      "current batch loss: 0.638086  [582400/900000]\n",
      "current batch loss: 0.623647  [588800/900000]\n",
      "current batch loss: 0.605447  [595200/900000]\n",
      "current batch loss: 0.596781  [601600/900000]\n",
      "current batch loss: 0.589762  [608000/900000]\n",
      "current batch loss: 0.668605  [614400/900000]\n",
      "current batch loss: 0.739005  [620800/900000]\n",
      "current batch loss: 0.639163  [627200/900000]\n",
      "current batch loss: 0.687764  [633600/900000]\n",
      "current batch loss: 0.679140  [640000/900000]\n",
      "current batch loss: 0.668237  [646400/900000]\n",
      "current batch loss: 0.615029  [652800/900000]\n",
      "current batch loss: 0.585587  [659200/900000]\n",
      "current batch loss: 0.633242  [665600/900000]\n",
      "current batch loss: 0.655676  [672000/900000]\n",
      "current batch loss: 0.601520  [678400/900000]\n",
      "current batch loss: 0.591394  [684800/900000]\n",
      "current batch loss: 0.616925  [691200/900000]\n",
      "current batch loss: 0.677580  [697600/900000]\n",
      "current batch loss: 0.576685  [704000/900000]\n",
      "current batch loss: 0.595723  [710400/900000]\n",
      "current batch loss: 0.548902  [716800/900000]\n",
      "current batch loss: 0.650298  [723200/900000]\n",
      "current batch loss: 0.623249  [729600/900000]\n",
      "current batch loss: 0.653468  [736000/900000]\n",
      "current batch loss: 0.613494  [742400/900000]\n",
      "current batch loss: 0.586752  [748800/900000]\n",
      "current batch loss: 0.619492  [755200/900000]\n",
      "current batch loss: 0.662554  [761600/900000]\n",
      "current batch loss: 0.624701  [768000/900000]\n",
      "current batch loss: 0.685798  [774400/900000]\n",
      "current batch loss: 0.656592  [780800/900000]\n",
      "current batch loss: 0.631011  [787200/900000]\n",
      "current batch loss: 0.662309  [793600/900000]\n",
      "current batch loss: 0.672690  [800000/900000]\n",
      "current batch loss: 0.596043  [806400/900000]\n",
      "current batch loss: 0.631658  [812800/900000]\n",
      "current batch loss: 0.590928  [819200/900000]\n",
      "current batch loss: 0.650415  [825600/900000]\n",
      "current batch loss: 0.666241  [832000/900000]\n",
      "current batch loss: 0.602032  [838400/900000]\n",
      "current batch loss: 0.606744  [844800/900000]\n",
      "current batch loss: 0.612097  [851200/900000]\n",
      "current batch loss: 0.692177  [857600/900000]\n",
      "current batch loss: 0.696754  [864000/900000]\n",
      "current batch loss: 0.610472  [870400/900000]\n",
      "current batch loss: 0.556962  [876800/900000]\n",
      "current batch loss: 0.620157  [883200/900000]\n",
      "current batch loss: 0.553691  [889600/900000]\n",
      "current batch loss: 0.589945  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.639450\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.637963\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 8\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.595218  [    0/900000]\n",
      "current batch loss: 0.666810  [ 6400/900000]\n",
      "current batch loss: 0.601810  [12800/900000]\n",
      "current batch loss: 0.700408  [19200/900000]\n",
      "current batch loss: 0.639105  [25600/900000]\n",
      "current batch loss: 0.636295  [32000/900000]\n",
      "current batch loss: 0.590179  [38400/900000]\n",
      "current batch loss: 0.640944  [44800/900000]\n",
      "current batch loss: 0.607105  [51200/900000]\n",
      "current batch loss: 0.613899  [57600/900000]\n",
      "current batch loss: 0.595976  [64000/900000]\n",
      "current batch loss: 0.578185  [70400/900000]\n",
      "current batch loss: 0.608030  [76800/900000]\n",
      "current batch loss: 0.705159  [83200/900000]\n",
      "current batch loss: 0.667478  [89600/900000]\n",
      "current batch loss: 0.615835  [96000/900000]\n",
      "current batch loss: 0.533354  [102400/900000]\n",
      "current batch loss: 0.612569  [108800/900000]\n",
      "current batch loss: 0.638374  [115200/900000]\n",
      "current batch loss: 0.667737  [121600/900000]\n",
      "current batch loss: 0.651058  [128000/900000]\n",
      "current batch loss: 0.628948  [134400/900000]\n",
      "current batch loss: 0.632564  [140800/900000]\n",
      "current batch loss: 0.581779  [147200/900000]\n",
      "current batch loss: 0.674334  [153600/900000]\n",
      "current batch loss: 0.655056  [160000/900000]\n",
      "current batch loss: 0.704243  [166400/900000]\n",
      "current batch loss: 0.662914  [172800/900000]\n",
      "current batch loss: 0.600123  [179200/900000]\n",
      "current batch loss: 0.624851  [185600/900000]\n",
      "current batch loss: 0.661047  [192000/900000]\n",
      "current batch loss: 0.570961  [198400/900000]\n",
      "current batch loss: 0.742478  [204800/900000]\n",
      "current batch loss: 0.706219  [211200/900000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.603637  [217600/900000]\n",
      "current batch loss: 0.614599  [224000/900000]\n",
      "current batch loss: 0.639601  [230400/900000]\n",
      "current batch loss: 0.628706  [236800/900000]\n",
      "current batch loss: 0.614135  [243200/900000]\n",
      "current batch loss: 0.573478  [249600/900000]\n",
      "current batch loss: 0.658279  [256000/900000]\n",
      "current batch loss: 0.670208  [262400/900000]\n",
      "current batch loss: 0.642357  [268800/900000]\n",
      "current batch loss: 0.666003  [275200/900000]\n",
      "current batch loss: 0.642420  [281600/900000]\n",
      "current batch loss: 0.621226  [288000/900000]\n",
      "current batch loss: 0.589553  [294400/900000]\n",
      "current batch loss: 0.578815  [300800/900000]\n",
      "current batch loss: 0.691715  [307200/900000]\n",
      "current batch loss: 0.610213  [313600/900000]\n",
      "current batch loss: 0.645558  [320000/900000]\n",
      "current batch loss: 0.574924  [326400/900000]\n",
      "current batch loss: 0.605131  [332800/900000]\n",
      "current batch loss: 0.651979  [339200/900000]\n",
      "current batch loss: 0.597354  [345600/900000]\n",
      "current batch loss: 0.633928  [352000/900000]\n",
      "current batch loss: 0.651090  [358400/900000]\n",
      "current batch loss: 0.641008  [364800/900000]\n",
      "current batch loss: 0.593232  [371200/900000]\n",
      "current batch loss: 0.601678  [377600/900000]\n",
      "current batch loss: 0.652186  [384000/900000]\n",
      "current batch loss: 0.681951  [390400/900000]\n",
      "current batch loss: 0.663653  [396800/900000]\n",
      "current batch loss: 0.639004  [403200/900000]\n",
      "current batch loss: 0.642241  [409600/900000]\n",
      "current batch loss: 0.680274  [416000/900000]\n",
      "current batch loss: 0.649717  [422400/900000]\n",
      "current batch loss: 0.678770  [428800/900000]\n",
      "current batch loss: 0.644275  [435200/900000]\n",
      "current batch loss: 0.610566  [441600/900000]\n",
      "current batch loss: 0.615781  [448000/900000]\n",
      "current batch loss: 0.670452  [454400/900000]\n",
      "current batch loss: 0.618581  [460800/900000]\n",
      "current batch loss: 0.596449  [467200/900000]\n",
      "current batch loss: 0.563926  [473600/900000]\n",
      "current batch loss: 0.656864  [480000/900000]\n",
      "current batch loss: 0.664556  [486400/900000]\n",
      "current batch loss: 0.712997  [492800/900000]\n",
      "current batch loss: 0.621933  [499200/900000]\n",
      "current batch loss: 0.666520  [505600/900000]\n",
      "current batch loss: 0.640643  [512000/900000]\n",
      "current batch loss: 0.622882  [518400/900000]\n",
      "current batch loss: 0.600571  [524800/900000]\n",
      "current batch loss: 0.621900  [531200/900000]\n",
      "current batch loss: 0.599064  [537600/900000]\n",
      "current batch loss: 0.615802  [544000/900000]\n",
      "current batch loss: 0.666114  [550400/900000]\n",
      "current batch loss: 0.632758  [556800/900000]\n",
      "current batch loss: 0.661873  [563200/900000]\n",
      "current batch loss: 0.576521  [569600/900000]\n",
      "current batch loss: 0.649954  [576000/900000]\n",
      "current batch loss: 0.633197  [582400/900000]\n",
      "current batch loss: 0.622676  [588800/900000]\n",
      "current batch loss: 0.625746  [595200/900000]\n",
      "current batch loss: 0.622889  [601600/900000]\n",
      "current batch loss: 0.634315  [608000/900000]\n",
      "current batch loss: 0.689774  [614400/900000]\n",
      "current batch loss: 0.704205  [620800/900000]\n",
      "current batch loss: 0.712591  [627200/900000]\n",
      "current batch loss: 0.712404  [633600/900000]\n",
      "current batch loss: 0.587530  [640000/900000]\n",
      "current batch loss: 0.681424  [646400/900000]\n",
      "current batch loss: 0.680721  [652800/900000]\n",
      "current batch loss: 0.662769  [659200/900000]\n",
      "current batch loss: 0.553924  [665600/900000]\n",
      "current batch loss: 0.660480  [672000/900000]\n",
      "current batch loss: 0.656880  [678400/900000]\n",
      "current batch loss: 0.698127  [684800/900000]\n",
      "current batch loss: 0.513915  [691200/900000]\n",
      "current batch loss: 0.643189  [697600/900000]\n",
      "current batch loss: 0.651017  [704000/900000]\n",
      "current batch loss: 0.628147  [710400/900000]\n",
      "current batch loss: 0.625522  [716800/900000]\n",
      "current batch loss: 0.632731  [723200/900000]\n",
      "current batch loss: 0.668843  [729600/900000]\n",
      "current batch loss: 0.628840  [736000/900000]\n",
      "current batch loss: 0.605827  [742400/900000]\n",
      "current batch loss: 0.660566  [748800/900000]\n",
      "current batch loss: 0.657958  [755200/900000]\n",
      "current batch loss: 0.653999  [761600/900000]\n",
      "current batch loss: 0.572516  [768000/900000]\n",
      "current batch loss: 0.640406  [774400/900000]\n",
      "current batch loss: 0.669202  [780800/900000]\n",
      "current batch loss: 0.562282  [787200/900000]\n",
      "current batch loss: 0.574584  [793600/900000]\n",
      "current batch loss: 0.646371  [800000/900000]\n",
      "current batch loss: 0.724461  [806400/900000]\n",
      "current batch loss: 0.599315  [812800/900000]\n",
      "current batch loss: 0.662714  [819200/900000]\n",
      "current batch loss: 0.644109  [825600/900000]\n",
      "current batch loss: 0.668507  [832000/900000]\n",
      "current batch loss: 0.627880  [838400/900000]\n",
      "current batch loss: 0.659267  [844800/900000]\n",
      "current batch loss: 0.590555  [851200/900000]\n",
      "current batch loss: 0.673749  [857600/900000]\n",
      "current batch loss: 0.605438  [864000/900000]\n",
      "current batch loss: 0.645272  [870400/900000]\n",
      "current batch loss: 0.613785  [876800/900000]\n",
      "current batch loss: 0.643776  [883200/900000]\n",
      "current batch loss: 0.716154  [889600/900000]\n",
      "current batch loss: 0.622783  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.634845\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.633777\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 9\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.647408  [    0/900000]\n",
      "current batch loss: 0.637595  [ 6400/900000]\n",
      "current batch loss: 0.628122  [12800/900000]\n",
      "current batch loss: 0.649231  [19200/900000]\n",
      "current batch loss: 0.650013  [25600/900000]\n",
      "current batch loss: 0.561787  [32000/900000]\n",
      "current batch loss: 0.643202  [38400/900000]\n",
      "current batch loss: 0.629550  [44800/900000]\n",
      "current batch loss: 0.642355  [51200/900000]\n",
      "current batch loss: 0.634080  [57600/900000]\n",
      "current batch loss: 0.643735  [64000/900000]\n",
      "current batch loss: 0.648386  [70400/900000]\n",
      "current batch loss: 0.681512  [76800/900000]\n",
      "current batch loss: 0.625540  [83200/900000]\n",
      "current batch loss: 0.665952  [89600/900000]\n",
      "current batch loss: 0.623561  [96000/900000]\n",
      "current batch loss: 0.589973  [102400/900000]\n",
      "current batch loss: 0.641782  [108800/900000]\n",
      "current batch loss: 0.573536  [115200/900000]\n",
      "current batch loss: 0.579826  [121600/900000]\n",
      "current batch loss: 0.625844  [128000/900000]\n",
      "current batch loss: 0.537361  [134400/900000]\n",
      "current batch loss: 0.629809  [140800/900000]\n",
      "current batch loss: 0.580943  [147200/900000]\n",
      "current batch loss: 0.650779  [153600/900000]\n",
      "current batch loss: 0.616485  [160000/900000]\n",
      "current batch loss: 0.574170  [166400/900000]\n",
      "current batch loss: 0.564833  [172800/900000]\n",
      "current batch loss: 0.733299  [179200/900000]\n",
      "current batch loss: 0.701765  [185600/900000]\n",
      "current batch loss: 0.609245  [192000/900000]\n",
      "current batch loss: 0.687135  [198400/900000]\n",
      "current batch loss: 0.606789  [204800/900000]\n",
      "current batch loss: 0.568005  [211200/900000]\n",
      "current batch loss: 0.614762  [217600/900000]\n",
      "current batch loss: 0.602722  [224000/900000]\n",
      "current batch loss: 0.634375  [230400/900000]\n",
      "current batch loss: 0.601671  [236800/900000]\n",
      "current batch loss: 0.636381  [243200/900000]\n",
      "current batch loss: 0.690339  [249600/900000]\n",
      "current batch loss: 0.616228  [256000/900000]\n",
      "current batch loss: 0.663466  [262400/900000]\n",
      "current batch loss: 0.624130  [268800/900000]\n",
      "current batch loss: 0.667783  [275200/900000]\n",
      "current batch loss: 0.632130  [281600/900000]\n",
      "current batch loss: 0.597645  [288000/900000]\n",
      "current batch loss: 0.601756  [294400/900000]\n",
      "current batch loss: 0.618959  [300800/900000]\n",
      "current batch loss: 0.653108  [307200/900000]\n",
      "current batch loss: 0.583155  [313600/900000]\n",
      "current batch loss: 0.622783  [320000/900000]\n",
      "current batch loss: 0.526262  [326400/900000]\n",
      "current batch loss: 0.676102  [332800/900000]\n",
      "current batch loss: 0.684614  [339200/900000]\n",
      "current batch loss: 0.624540  [345600/900000]\n",
      "current batch loss: 0.648958  [352000/900000]\n",
      "current batch loss: 0.621098  [358400/900000]\n",
      "current batch loss: 0.588253  [364800/900000]\n",
      "current batch loss: 0.626148  [371200/900000]\n",
      "current batch loss: 0.691582  [377600/900000]\n",
      "current batch loss: 0.606842  [384000/900000]\n",
      "current batch loss: 0.656459  [390400/900000]\n",
      "current batch loss: 0.657226  [396800/900000]\n",
      "current batch loss: 0.685090  [403200/900000]\n",
      "current batch loss: 0.656157  [409600/900000]\n",
      "current batch loss: 0.596681  [416000/900000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.546934  [422400/900000]\n",
      "current batch loss: 0.581645  [428800/900000]\n",
      "current batch loss: 0.659314  [435200/900000]\n",
      "current batch loss: 0.678767  [441600/900000]\n",
      "current batch loss: 0.633080  [448000/900000]\n",
      "current batch loss: 0.582318  [454400/900000]\n",
      "current batch loss: 0.641036  [460800/900000]\n",
      "current batch loss: 0.652679  [467200/900000]\n",
      "current batch loss: 0.613057  [473600/900000]\n",
      "current batch loss: 0.630119  [480000/900000]\n",
      "current batch loss: 0.623345  [486400/900000]\n",
      "current batch loss: 0.676144  [492800/900000]\n",
      "current batch loss: 0.604971  [499200/900000]\n",
      "current batch loss: 0.595550  [505600/900000]\n",
      "current batch loss: 0.605736  [512000/900000]\n",
      "current batch loss: 0.626662  [518400/900000]\n",
      "current batch loss: 0.665844  [524800/900000]\n",
      "current batch loss: 0.690694  [531200/900000]\n",
      "current batch loss: 0.663083  [537600/900000]\n",
      "current batch loss: 0.677255  [544000/900000]\n",
      "current batch loss: 0.681592  [550400/900000]\n",
      "current batch loss: 0.692090  [556800/900000]\n",
      "current batch loss: 0.703043  [563200/900000]\n",
      "current batch loss: 0.578109  [569600/900000]\n",
      "current batch loss: 0.650376  [576000/900000]\n",
      "current batch loss: 0.673699  [582400/900000]\n",
      "current batch loss: 0.640751  [588800/900000]\n",
      "current batch loss: 0.636966  [595200/900000]\n",
      "current batch loss: 0.715121  [601600/900000]\n",
      "current batch loss: 0.609137  [608000/900000]\n",
      "current batch loss: 0.616723  [614400/900000]\n",
      "current batch loss: 0.649003  [620800/900000]\n",
      "current batch loss: 0.568095  [627200/900000]\n",
      "current batch loss: 0.588959  [633600/900000]\n",
      "current batch loss: 0.621203  [640000/900000]\n",
      "current batch loss: 0.562571  [646400/900000]\n",
      "current batch loss: 0.618671  [652800/900000]\n",
      "current batch loss: 0.674085  [659200/900000]\n",
      "current batch loss: 0.541588  [665600/900000]\n",
      "current batch loss: 0.645432  [672000/900000]\n",
      "current batch loss: 0.650817  [678400/900000]\n",
      "current batch loss: 0.570776  [684800/900000]\n",
      "current batch loss: 0.621468  [691200/900000]\n",
      "current batch loss: 0.660356  [697600/900000]\n",
      "current batch loss: 0.639871  [704000/900000]\n",
      "current batch loss: 0.614350  [710400/900000]\n",
      "current batch loss: 0.612047  [716800/900000]\n",
      "current batch loss: 0.685973  [723200/900000]\n",
      "current batch loss: 0.597916  [729600/900000]\n",
      "current batch loss: 0.634986  [736000/900000]\n",
      "current batch loss: 0.608152  [742400/900000]\n",
      "current batch loss: 0.625018  [748800/900000]\n",
      "current batch loss: 0.690108  [755200/900000]\n",
      "current batch loss: 0.700358  [761600/900000]\n",
      "current batch loss: 0.618517  [768000/900000]\n",
      "current batch loss: 0.704510  [774400/900000]\n",
      "current batch loss: 0.626827  [780800/900000]\n",
      "current batch loss: 0.677986  [787200/900000]\n",
      "current batch loss: 0.682107  [793600/900000]\n",
      "current batch loss: 0.647446  [800000/900000]\n",
      "current batch loss: 0.609743  [806400/900000]\n",
      "current batch loss: 0.683126  [812800/900000]\n",
      "current batch loss: 0.653615  [819200/900000]\n",
      "current batch loss: 0.641072  [825600/900000]\n",
      "current batch loss: 0.718006  [832000/900000]\n",
      "current batch loss: 0.634195  [838400/900000]\n",
      "current batch loss: 0.628545  [844800/900000]\n",
      "current batch loss: 0.684060  [851200/900000]\n",
      "current batch loss: 0.663163  [857600/900000]\n",
      "current batch loss: 0.623571  [864000/900000]\n",
      "current batch loss: 0.639037  [870400/900000]\n",
      "current batch loss: 0.645174  [876800/900000]\n",
      "current batch loss: 0.610675  [883200/900000]\n",
      "current batch loss: 0.604060  [889600/900000]\n",
      "current batch loss: 0.587182  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.637138\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.636160\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 10\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.654419  [    0/900000]\n",
      "current batch loss: 0.648513  [ 6400/900000]\n",
      "current batch loss: 0.616132  [12800/900000]\n",
      "current batch loss: 0.592641  [19200/900000]\n",
      "current batch loss: 0.630415  [25600/900000]\n",
      "current batch loss: 0.697956  [32000/900000]\n",
      "current batch loss: 0.627644  [38400/900000]\n",
      "current batch loss: 0.646178  [44800/900000]\n",
      "current batch loss: 0.625275  [51200/900000]\n",
      "current batch loss: 0.636501  [57600/900000]\n",
      "current batch loss: 0.549693  [64000/900000]\n",
      "current batch loss: 0.619459  [70400/900000]\n",
      "current batch loss: 0.615175  [76800/900000]\n",
      "current batch loss: 0.631233  [83200/900000]\n",
      "current batch loss: 0.618736  [89600/900000]\n",
      "current batch loss: 0.713925  [96000/900000]\n",
      "current batch loss: 0.643224  [102400/900000]\n",
      "current batch loss: 0.628416  [108800/900000]\n",
      "current batch loss: 0.656836  [115200/900000]\n",
      "current batch loss: 0.621833  [121600/900000]\n",
      "current batch loss: 0.676051  [128000/900000]\n",
      "current batch loss: 0.660443  [134400/900000]\n",
      "current batch loss: 0.639799  [140800/900000]\n",
      "current batch loss: 0.638232  [147200/900000]\n",
      "current batch loss: 0.690227  [153600/900000]\n",
      "current batch loss: 0.642093  [160000/900000]\n",
      "current batch loss: 0.651015  [166400/900000]\n",
      "current batch loss: 0.641654  [172800/900000]\n",
      "current batch loss: 0.593963  [179200/900000]\n",
      "current batch loss: 0.613524  [185600/900000]\n",
      "current batch loss: 0.620737  [192000/900000]\n",
      "current batch loss: 0.742172  [198400/900000]\n",
      "current batch loss: 0.649665  [204800/900000]\n",
      "current batch loss: 0.628909  [211200/900000]\n",
      "current batch loss: 0.658439  [217600/900000]\n",
      "current batch loss: 0.644250  [224000/900000]\n",
      "current batch loss: 0.648824  [230400/900000]\n",
      "current batch loss: 0.610001  [236800/900000]\n",
      "current batch loss: 0.632522  [243200/900000]\n",
      "current batch loss: 0.618584  [249600/900000]\n",
      "current batch loss: 0.691733  [256000/900000]\n",
      "current batch loss: 0.569672  [262400/900000]\n",
      "current batch loss: 0.655103  [268800/900000]\n",
      "current batch loss: 0.689600  [275200/900000]\n",
      "current batch loss: 0.686323  [281600/900000]\n",
      "current batch loss: 0.631968  [288000/900000]\n",
      "current batch loss: 0.662789  [294400/900000]\n",
      "current batch loss: 0.570820  [300800/900000]\n",
      "current batch loss: 0.595275  [307200/900000]\n",
      "current batch loss: 0.628765  [313600/900000]\n",
      "current batch loss: 0.613610  [320000/900000]\n",
      "current batch loss: 0.596060  [326400/900000]\n",
      "current batch loss: 0.689344  [332800/900000]\n",
      "current batch loss: 0.618235  [339200/900000]\n",
      "current batch loss: 0.614324  [345600/900000]\n",
      "current batch loss: 0.636253  [352000/900000]\n",
      "current batch loss: 0.664025  [358400/900000]\n",
      "current batch loss: 0.666749  [364800/900000]\n",
      "current batch loss: 0.571553  [371200/900000]\n",
      "current batch loss: 0.629733  [377600/900000]\n",
      "current batch loss: 0.706890  [384000/900000]\n",
      "current batch loss: 0.625559  [390400/900000]\n",
      "current batch loss: 0.674096  [396800/900000]\n",
      "current batch loss: 0.606703  [403200/900000]\n",
      "current batch loss: 0.620668  [409600/900000]\n",
      "current batch loss: 0.672898  [416000/900000]\n",
      "current batch loss: 0.628033  [422400/900000]\n",
      "current batch loss: 0.558544  [428800/900000]\n",
      "current batch loss: 0.662477  [435200/900000]\n",
      "current batch loss: 0.647369  [441600/900000]\n",
      "current batch loss: 0.645714  [448000/900000]\n",
      "current batch loss: 0.556056  [454400/900000]\n",
      "current batch loss: 0.611436  [460800/900000]\n",
      "current batch loss: 0.592937  [467200/900000]\n",
      "current batch loss: 0.653410  [473600/900000]\n",
      "current batch loss: 0.670076  [480000/900000]\n",
      "current batch loss: 0.617662  [486400/900000]\n",
      "current batch loss: 0.631035  [492800/900000]\n",
      "current batch loss: 0.621594  [499200/900000]\n",
      "current batch loss: 0.601621  [505600/900000]\n",
      "current batch loss: 0.600830  [512000/900000]\n",
      "current batch loss: 0.661288  [518400/900000]\n",
      "current batch loss: 0.639380  [524800/900000]\n",
      "current batch loss: 0.638710  [531200/900000]\n",
      "current batch loss: 0.653891  [537600/900000]\n",
      "current batch loss: 0.598616  [544000/900000]\n",
      "current batch loss: 0.608622  [550400/900000]\n",
      "current batch loss: 0.624919  [556800/900000]\n",
      "current batch loss: 0.545811  [563200/900000]\n",
      "current batch loss: 0.620571  [569600/900000]\n",
      "current batch loss: 0.674490  [576000/900000]\n",
      "current batch loss: 0.604903  [582400/900000]\n",
      "current batch loss: 0.620588  [588800/900000]\n",
      "current batch loss: 0.669093  [595200/900000]\n",
      "current batch loss: 0.648536  [601600/900000]\n",
      "current batch loss: 0.695416  [608000/900000]\n",
      "current batch loss: 0.641108  [614400/900000]\n",
      "current batch loss: 0.602398  [620800/900000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.583619  [627200/900000]\n",
      "current batch loss: 0.639400  [633600/900000]\n",
      "current batch loss: 0.751953  [640000/900000]\n",
      "current batch loss: 0.684662  [646400/900000]\n",
      "current batch loss: 0.668753  [652800/900000]\n",
      "current batch loss: 0.666546  [659200/900000]\n",
      "current batch loss: 0.602793  [665600/900000]\n",
      "current batch loss: 0.602042  [672000/900000]\n",
      "current batch loss: 0.588906  [678400/900000]\n",
      "current batch loss: 0.682076  [684800/900000]\n",
      "current batch loss: 0.628097  [691200/900000]\n",
      "current batch loss: 0.628503  [697600/900000]\n",
      "current batch loss: 0.664072  [704000/900000]\n",
      "current batch loss: 0.686840  [710400/900000]\n",
      "current batch loss: 0.645003  [716800/900000]\n",
      "current batch loss: 0.630089  [723200/900000]\n",
      "current batch loss: 0.643062  [729600/900000]\n",
      "current batch loss: 0.604407  [736000/900000]\n",
      "current batch loss: 0.645454  [742400/900000]\n",
      "current batch loss: 0.620416  [748800/900000]\n",
      "current batch loss: 0.645492  [755200/900000]\n",
      "current batch loss: 0.680579  [761600/900000]\n",
      "current batch loss: 0.654460  [768000/900000]\n",
      "current batch loss: 0.571976  [774400/900000]\n",
      "current batch loss: 0.651899  [780800/900000]\n",
      "current batch loss: 0.592408  [787200/900000]\n",
      "current batch loss: 0.637152  [793600/900000]\n",
      "current batch loss: 0.650659  [800000/900000]\n",
      "current batch loss: 0.564326  [806400/900000]\n",
      "current batch loss: 0.608475  [812800/900000]\n",
      "current batch loss: 0.660376  [819200/900000]\n",
      "current batch loss: 0.645648  [825600/900000]\n",
      "current batch loss: 0.706979  [832000/900000]\n",
      "current batch loss: 0.621044  [838400/900000]\n",
      "current batch loss: 0.623028  [844800/900000]\n",
      "current batch loss: 0.608231  [851200/900000]\n",
      "current batch loss: 0.633114  [857600/900000]\n",
      "current batch loss: 0.593764  [864000/900000]\n",
      "current batch loss: 0.669277  [870400/900000]\n",
      "current batch loss: 0.587490  [876800/900000]\n",
      "current batch loss: 0.611657  [883200/900000]\n",
      "current batch loss: 0.578432  [889600/900000]\n",
      "current batch loss: 0.669138  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.635407\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.633995\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 11\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.617884  [    0/900000]\n",
      "current batch loss: 0.622967  [ 6400/900000]\n",
      "current batch loss: 0.626548  [12800/900000]\n",
      "current batch loss: 0.629021  [19200/900000]\n",
      "current batch loss: 0.617655  [25600/900000]\n",
      "current batch loss: 0.577451  [32000/900000]\n",
      "current batch loss: 0.615351  [38400/900000]\n",
      "current batch loss: 0.687487  [44800/900000]\n",
      "current batch loss: 0.651629  [51200/900000]\n",
      "current batch loss: 0.641270  [57600/900000]\n",
      "current batch loss: 0.577719  [64000/900000]\n",
      "current batch loss: 0.607028  [70400/900000]\n",
      "current batch loss: 0.598490  [76800/900000]\n",
      "current batch loss: 0.646288  [83200/900000]\n",
      "current batch loss: 0.666069  [89600/900000]\n",
      "current batch loss: 0.650519  [96000/900000]\n",
      "current batch loss: 0.612189  [102400/900000]\n",
      "current batch loss: 0.668363  [108800/900000]\n",
      "current batch loss: 0.696522  [115200/900000]\n",
      "current batch loss: 0.525031  [121600/900000]\n",
      "current batch loss: 0.638483  [128000/900000]\n",
      "current batch loss: 0.569480  [134400/900000]\n",
      "current batch loss: 0.610921  [140800/900000]\n",
      "current batch loss: 0.669184  [147200/900000]\n",
      "current batch loss: 0.614340  [153600/900000]\n",
      "current batch loss: 0.606843  [160000/900000]\n",
      "current batch loss: 0.674090  [166400/900000]\n",
      "current batch loss: 0.664867  [172800/900000]\n",
      "current batch loss: 0.599789  [179200/900000]\n",
      "current batch loss: 0.695189  [185600/900000]\n",
      "current batch loss: 0.631670  [192000/900000]\n",
      "current batch loss: 0.626557  [198400/900000]\n",
      "current batch loss: 0.639667  [204800/900000]\n",
      "current batch loss: 0.647138  [211200/900000]\n",
      "current batch loss: 0.627854  [217600/900000]\n",
      "current batch loss: 0.662855  [224000/900000]\n",
      "current batch loss: 0.676500  [230400/900000]\n",
      "current batch loss: 0.744173  [236800/900000]\n",
      "current batch loss: 0.659266  [243200/900000]\n",
      "current batch loss: 0.611045  [249600/900000]\n",
      "current batch loss: 0.665328  [256000/900000]\n",
      "current batch loss: 0.602802  [262400/900000]\n",
      "current batch loss: 0.632675  [268800/900000]\n",
      "current batch loss: 0.564391  [275200/900000]\n",
      "current batch loss: 0.623426  [281600/900000]\n",
      "current batch loss: 0.675349  [288000/900000]\n",
      "current batch loss: 0.658423  [294400/900000]\n",
      "current batch loss: 0.595013  [300800/900000]\n",
      "current batch loss: 0.676337  [307200/900000]\n",
      "current batch loss: 0.583384  [313600/900000]\n",
      "current batch loss: 0.625561  [320000/900000]\n",
      "current batch loss: 0.638645  [326400/900000]\n",
      "current batch loss: 0.640979  [332800/900000]\n",
      "current batch loss: 0.591377  [339200/900000]\n",
      "current batch loss: 0.622843  [345600/900000]\n",
      "current batch loss: 0.674050  [352000/900000]\n",
      "current batch loss: 0.602096  [358400/900000]\n",
      "current batch loss: 0.654432  [364800/900000]\n",
      "current batch loss: 0.603288  [371200/900000]\n",
      "current batch loss: 0.573363  [377600/900000]\n",
      "current batch loss: 0.690651  [384000/900000]\n",
      "current batch loss: 0.631016  [390400/900000]\n",
      "current batch loss: 0.670286  [396800/900000]\n",
      "current batch loss: 0.508359  [403200/900000]\n",
      "current batch loss: 0.637585  [409600/900000]\n",
      "current batch loss: 0.614317  [416000/900000]\n",
      "current batch loss: 0.721020  [422400/900000]\n",
      "current batch loss: 0.658645  [428800/900000]\n",
      "current batch loss: 0.644410  [435200/900000]\n",
      "current batch loss: 0.635432  [441600/900000]\n",
      "current batch loss: 0.700649  [448000/900000]\n",
      "current batch loss: 0.644108  [454400/900000]\n",
      "current batch loss: 0.739006  [460800/900000]\n",
      "current batch loss: 0.610397  [467200/900000]\n",
      "current batch loss: 0.682803  [473600/900000]\n",
      "current batch loss: 0.567480  [480000/900000]\n",
      "current batch loss: 0.672263  [486400/900000]\n",
      "current batch loss: 0.561767  [492800/900000]\n",
      "current batch loss: 0.678756  [499200/900000]\n",
      "current batch loss: 0.684029  [505600/900000]\n",
      "current batch loss: 0.640488  [512000/900000]\n",
      "current batch loss: 0.628640  [518400/900000]\n",
      "current batch loss: 0.674614  [524800/900000]\n",
      "current batch loss: 0.614168  [531200/900000]\n",
      "current batch loss: 0.667479  [537600/900000]\n",
      "current batch loss: 0.612279  [544000/900000]\n",
      "current batch loss: 0.616721  [550400/900000]\n",
      "current batch loss: 0.591044  [556800/900000]\n",
      "current batch loss: 0.652949  [563200/900000]\n",
      "current batch loss: 0.653126  [569600/900000]\n",
      "current batch loss: 0.607641  [576000/900000]\n",
      "current batch loss: 0.640184  [582400/900000]\n",
      "current batch loss: 0.666239  [588800/900000]\n",
      "current batch loss: 0.653704  [595200/900000]\n",
      "current batch loss: 0.649849  [601600/900000]\n",
      "current batch loss: 0.595397  [608000/900000]\n",
      "current batch loss: 0.619410  [614400/900000]\n",
      "current batch loss: 0.616883  [620800/900000]\n",
      "current batch loss: 0.571067  [627200/900000]\n",
      "current batch loss: 0.696666  [633600/900000]\n",
      "current batch loss: 0.563661  [640000/900000]\n",
      "current batch loss: 0.559676  [646400/900000]\n",
      "current batch loss: 0.604454  [652800/900000]\n",
      "current batch loss: 0.606629  [659200/900000]\n",
      "current batch loss: 0.584927  [665600/900000]\n",
      "current batch loss: 0.651053  [672000/900000]\n",
      "current batch loss: 0.561841  [678400/900000]\n",
      "current batch loss: 0.611256  [684800/900000]\n",
      "current batch loss: 0.694734  [691200/900000]\n",
      "current batch loss: 0.657876  [697600/900000]\n",
      "current batch loss: 0.676050  [704000/900000]\n",
      "current batch loss: 0.571948  [710400/900000]\n",
      "current batch loss: 0.653009  [716800/900000]\n",
      "current batch loss: 0.681789  [723200/900000]\n",
      "current batch loss: 0.618206  [729600/900000]\n",
      "current batch loss: 0.634335  [736000/900000]\n",
      "current batch loss: 0.653132  [742400/900000]\n",
      "current batch loss: 0.684952  [748800/900000]\n",
      "current batch loss: 0.694905  [755200/900000]\n",
      "current batch loss: 0.628963  [761600/900000]\n",
      "current batch loss: 0.657330  [768000/900000]\n",
      "current batch loss: 0.596827  [774400/900000]\n",
      "current batch loss: 0.626517  [780800/900000]\n",
      "current batch loss: 0.617743  [787200/900000]\n",
      "current batch loss: 0.640527  [793600/900000]\n",
      "current batch loss: 0.630947  [800000/900000]\n",
      "current batch loss: 0.620474  [806400/900000]\n",
      "current batch loss: 0.636300  [812800/900000]\n",
      "current batch loss: 0.619972  [819200/900000]\n",
      "current batch loss: 0.577299  [825600/900000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.611436  [832000/900000]\n",
      "current batch loss: 0.639132  [838400/900000]\n",
      "current batch loss: 0.625492  [844800/900000]\n",
      "current batch loss: 0.563824  [851200/900000]\n",
      "current batch loss: 0.620819  [857600/900000]\n",
      "current batch loss: 0.689866  [864000/900000]\n",
      "current batch loss: 0.640354  [870400/900000]\n",
      "current batch loss: 0.623126  [876800/900000]\n",
      "current batch loss: 0.624796  [883200/900000]\n",
      "current batch loss: 0.652112  [889600/900000]\n",
      "current batch loss: 0.701094  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.632212\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.631067\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 12\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.673454  [    0/900000]\n",
      "current batch loss: 0.665754  [ 6400/900000]\n",
      "current batch loss: 0.727277  [12800/900000]\n",
      "current batch loss: 0.617559  [19200/900000]\n",
      "current batch loss: 0.629239  [25600/900000]\n",
      "current batch loss: 0.662476  [32000/900000]\n",
      "current batch loss: 0.580548  [38400/900000]\n",
      "current batch loss: 0.542446  [44800/900000]\n",
      "current batch loss: 0.588675  [51200/900000]\n",
      "current batch loss: 0.611588  [57600/900000]\n",
      "current batch loss: 0.644949  [64000/900000]\n",
      "current batch loss: 0.668602  [70400/900000]\n",
      "current batch loss: 0.530507  [76800/900000]\n",
      "current batch loss: 0.668058  [83200/900000]\n",
      "current batch loss: 0.665135  [89600/900000]\n",
      "current batch loss: 0.669522  [96000/900000]\n",
      "current batch loss: 0.695456  [102400/900000]\n",
      "current batch loss: 0.623540  [108800/900000]\n",
      "current batch loss: 0.703657  [115200/900000]\n",
      "current batch loss: 0.642068  [121600/900000]\n",
      "current batch loss: 0.645241  [128000/900000]\n",
      "current batch loss: 0.611571  [134400/900000]\n",
      "current batch loss: 0.614983  [140800/900000]\n",
      "current batch loss: 0.688862  [147200/900000]\n",
      "current batch loss: 0.690864  [153600/900000]\n",
      "current batch loss: 0.642512  [160000/900000]\n",
      "current batch loss: 0.574541  [166400/900000]\n",
      "current batch loss: 0.619062  [172800/900000]\n",
      "current batch loss: 0.662985  [179200/900000]\n",
      "current batch loss: 0.764022  [185600/900000]\n",
      "current batch loss: 0.624390  [192000/900000]\n",
      "current batch loss: 0.643873  [198400/900000]\n",
      "current batch loss: 0.682783  [204800/900000]\n",
      "current batch loss: 0.620830  [211200/900000]\n",
      "current batch loss: 0.615094  [217600/900000]\n",
      "current batch loss: 0.568199  [224000/900000]\n",
      "current batch loss: 0.591045  [230400/900000]\n",
      "current batch loss: 0.731835  [236800/900000]\n",
      "current batch loss: 0.635624  [243200/900000]\n",
      "current batch loss: 0.621574  [249600/900000]\n",
      "current batch loss: 0.638642  [256000/900000]\n",
      "current batch loss: 0.645052  [262400/900000]\n",
      "current batch loss: 0.685600  [268800/900000]\n",
      "current batch loss: 0.608268  [275200/900000]\n",
      "current batch loss: 0.618458  [281600/900000]\n",
      "current batch loss: 0.663685  [288000/900000]\n",
      "current batch loss: 0.669689  [294400/900000]\n",
      "current batch loss: 0.584632  [300800/900000]\n",
      "current batch loss: 0.571988  [307200/900000]\n",
      "current batch loss: 0.700506  [313600/900000]\n",
      "current batch loss: 0.669054  [320000/900000]\n",
      "current batch loss: 0.596706  [326400/900000]\n",
      "current batch loss: 0.516363  [332800/900000]\n",
      "current batch loss: 0.582253  [339200/900000]\n",
      "current batch loss: 0.567822  [345600/900000]\n",
      "current batch loss: 0.612627  [352000/900000]\n",
      "current batch loss: 0.671559  [358400/900000]\n",
      "current batch loss: 0.590155  [364800/900000]\n",
      "current batch loss: 0.640725  [371200/900000]\n",
      "current batch loss: 0.625741  [377600/900000]\n",
      "current batch loss: 0.534371  [384000/900000]\n",
      "current batch loss: 0.679561  [390400/900000]\n",
      "current batch loss: 0.689774  [396800/900000]\n",
      "current batch loss: 0.689139  [403200/900000]\n",
      "current batch loss: 0.686550  [409600/900000]\n",
      "current batch loss: 0.669075  [416000/900000]\n",
      "current batch loss: 0.628854  [422400/900000]\n",
      "current batch loss: 0.676481  [428800/900000]\n",
      "current batch loss: 0.634787  [435200/900000]\n",
      "current batch loss: 0.667712  [441600/900000]\n",
      "current batch loss: 0.599714  [448000/900000]\n",
      "current batch loss: 0.690410  [454400/900000]\n",
      "current batch loss: 0.605162  [460800/900000]\n",
      "current batch loss: 0.604086  [467200/900000]\n",
      "current batch loss: 0.626338  [473600/900000]\n",
      "current batch loss: 0.690295  [480000/900000]\n",
      "current batch loss: 0.637252  [486400/900000]\n",
      "current batch loss: 0.722986  [492800/900000]\n",
      "current batch loss: 0.727202  [499200/900000]\n",
      "current batch loss: 0.618620  [505600/900000]\n",
      "current batch loss: 0.771113  [512000/900000]\n",
      "current batch loss: 0.630716  [518400/900000]\n",
      "current batch loss: 0.601915  [524800/900000]\n",
      "current batch loss: 0.627760  [531200/900000]\n",
      "current batch loss: 0.648118  [537600/900000]\n",
      "current batch loss: 0.693242  [544000/900000]\n",
      "current batch loss: 0.662466  [550400/900000]\n",
      "current batch loss: 0.618632  [556800/900000]\n",
      "current batch loss: 0.613365  [563200/900000]\n",
      "current batch loss: 0.615220  [569600/900000]\n",
      "current batch loss: 0.613051  [576000/900000]\n",
      "current batch loss: 0.636523  [582400/900000]\n",
      "current batch loss: 0.672073  [588800/900000]\n",
      "current batch loss: 0.623250  [595200/900000]\n",
      "current batch loss: 0.711187  [601600/900000]\n",
      "current batch loss: 0.560486  [608000/900000]\n",
      "current batch loss: 0.657212  [614400/900000]\n",
      "current batch loss: 0.618809  [620800/900000]\n",
      "current batch loss: 0.623190  [627200/900000]\n",
      "current batch loss: 0.661656  [633600/900000]\n",
      "current batch loss: 0.596504  [640000/900000]\n",
      "current batch loss: 0.676580  [646400/900000]\n",
      "current batch loss: 0.704125  [652800/900000]\n",
      "current batch loss: 0.683012  [659200/900000]\n",
      "current batch loss: 0.676514  [665600/900000]\n",
      "current batch loss: 0.608325  [672000/900000]\n",
      "current batch loss: 0.622243  [678400/900000]\n",
      "current batch loss: 0.537143  [684800/900000]\n",
      "current batch loss: 0.659151  [691200/900000]\n",
      "current batch loss: 0.589547  [697600/900000]\n",
      "current batch loss: 0.614978  [704000/900000]\n",
      "current batch loss: 0.653524  [710400/900000]\n",
      "current batch loss: 0.521728  [716800/900000]\n",
      "current batch loss: 0.619484  [723200/900000]\n",
      "current batch loss: 0.585913  [729600/900000]\n",
      "current batch loss: 0.654103  [736000/900000]\n",
      "current batch loss: 0.616963  [742400/900000]\n",
      "current batch loss: 0.679994  [748800/900000]\n",
      "current batch loss: 0.701082  [755200/900000]\n",
      "current batch loss: 0.652383  [761600/900000]\n",
      "current batch loss: 0.612274  [768000/900000]\n",
      "current batch loss: 0.605047  [774400/900000]\n",
      "current batch loss: 0.658393  [780800/900000]\n",
      "current batch loss: 0.610384  [787200/900000]\n",
      "current batch loss: 0.661792  [793600/900000]\n",
      "current batch loss: 0.660874  [800000/900000]\n",
      "current batch loss: 0.605498  [806400/900000]\n",
      "current batch loss: 0.601539  [812800/900000]\n",
      "current batch loss: 0.649062  [819200/900000]\n",
      "current batch loss: 0.625546  [825600/900000]\n",
      "current batch loss: 0.620587  [832000/900000]\n",
      "current batch loss: 0.643949  [838400/900000]\n",
      "current batch loss: 0.654734  [844800/900000]\n",
      "current batch loss: 0.677175  [851200/900000]\n",
      "current batch loss: 0.661250  [857600/900000]\n",
      "current batch loss: 0.574770  [864000/900000]\n",
      "current batch loss: 0.613038  [870400/900000]\n",
      "current batch loss: 0.652102  [876800/900000]\n",
      "current batch loss: 0.575420  [883200/900000]\n",
      "current batch loss: 0.612638  [889600/900000]\n",
      "current batch loss: 0.606081  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.636064\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.634588\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 13\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.652640  [    0/900000]\n",
      "current batch loss: 0.640423  [ 6400/900000]\n",
      "current batch loss: 0.631923  [12800/900000]\n",
      "current batch loss: 0.650030  [19200/900000]\n",
      "current batch loss: 0.624706  [25600/900000]\n",
      "current batch loss: 0.636459  [32000/900000]\n",
      "current batch loss: 0.625340  [38400/900000]\n",
      "current batch loss: 0.649611  [44800/900000]\n",
      "current batch loss: 0.551771  [51200/900000]\n",
      "current batch loss: 0.681861  [57600/900000]\n",
      "current batch loss: 0.565901  [64000/900000]\n",
      "current batch loss: 0.642779  [70400/900000]\n",
      "current batch loss: 0.621431  [76800/900000]\n",
      "current batch loss: 0.633303  [83200/900000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.593857  [89600/900000]\n",
      "current batch loss: 0.632999  [96000/900000]\n",
      "current batch loss: 0.570806  [102400/900000]\n",
      "current batch loss: 0.667827  [108800/900000]\n",
      "current batch loss: 0.653249  [115200/900000]\n",
      "current batch loss: 0.645623  [121600/900000]\n",
      "current batch loss: 0.607545  [128000/900000]\n",
      "current batch loss: 0.661648  [134400/900000]\n",
      "current batch loss: 0.645932  [140800/900000]\n",
      "current batch loss: 0.617792  [147200/900000]\n",
      "current batch loss: 0.571467  [153600/900000]\n",
      "current batch loss: 0.578904  [160000/900000]\n",
      "current batch loss: 0.598560  [166400/900000]\n",
      "current batch loss: 0.678884  [172800/900000]\n",
      "current batch loss: 0.538372  [179200/900000]\n",
      "current batch loss: 0.580159  [185600/900000]\n",
      "current batch loss: 0.636896  [192000/900000]\n",
      "current batch loss: 0.608990  [198400/900000]\n",
      "current batch loss: 0.661868  [204800/900000]\n",
      "current batch loss: 0.708401  [211200/900000]\n",
      "current batch loss: 0.679929  [217600/900000]\n",
      "current batch loss: 0.627013  [224000/900000]\n",
      "current batch loss: 0.575517  [230400/900000]\n",
      "current batch loss: 0.617205  [236800/900000]\n",
      "current batch loss: 0.599231  [243200/900000]\n",
      "current batch loss: 0.677211  [249600/900000]\n",
      "current batch loss: 0.594698  [256000/900000]\n",
      "current batch loss: 0.698703  [262400/900000]\n",
      "current batch loss: 0.668172  [268800/900000]\n",
      "current batch loss: 0.707228  [275200/900000]\n",
      "current batch loss: 0.624447  [281600/900000]\n",
      "current batch loss: 0.698519  [288000/900000]\n",
      "current batch loss: 0.647475  [294400/900000]\n",
      "current batch loss: 0.630069  [300800/900000]\n",
      "current batch loss: 0.662358  [307200/900000]\n",
      "current batch loss: 0.627425  [313600/900000]\n",
      "current batch loss: 0.656021  [320000/900000]\n",
      "current batch loss: 0.624704  [326400/900000]\n",
      "current batch loss: 0.624596  [332800/900000]\n",
      "current batch loss: 0.695771  [339200/900000]\n",
      "current batch loss: 0.670860  [345600/900000]\n",
      "current batch loss: 0.676301  [352000/900000]\n",
      "current batch loss: 0.653109  [358400/900000]\n",
      "current batch loss: 0.611247  [364800/900000]\n",
      "current batch loss: 0.657808  [371200/900000]\n",
      "current batch loss: 0.661372  [377600/900000]\n",
      "current batch loss: 0.621019  [384000/900000]\n",
      "current batch loss: 0.597362  [390400/900000]\n",
      "current batch loss: 0.642598  [396800/900000]\n",
      "current batch loss: 0.567909  [403200/900000]\n",
      "current batch loss: 0.621616  [409600/900000]\n",
      "current batch loss: 0.654541  [416000/900000]\n",
      "current batch loss: 0.633835  [422400/900000]\n",
      "current batch loss: 0.664656  [428800/900000]\n",
      "current batch loss: 0.565440  [435200/900000]\n",
      "current batch loss: 0.720434  [441600/900000]\n",
      "current batch loss: 0.620028  [448000/900000]\n",
      "current batch loss: 0.603935  [454400/900000]\n",
      "current batch loss: 0.684449  [460800/900000]\n",
      "current batch loss: 0.656095  [467200/900000]\n",
      "current batch loss: 0.572142  [473600/900000]\n",
      "current batch loss: 0.620759  [480000/900000]\n",
      "current batch loss: 0.696400  [486400/900000]\n",
      "current batch loss: 0.651589  [492800/900000]\n",
      "current batch loss: 0.615454  [499200/900000]\n",
      "current batch loss: 0.635891  [505600/900000]\n",
      "current batch loss: 0.624423  [512000/900000]\n",
      "current batch loss: 0.691601  [518400/900000]\n",
      "current batch loss: 0.641757  [524800/900000]\n",
      "current batch loss: 0.640913  [531200/900000]\n",
      "current batch loss: 0.584652  [537600/900000]\n",
      "current batch loss: 0.653735  [544000/900000]\n",
      "current batch loss: 0.567726  [550400/900000]\n",
      "current batch loss: 0.649957  [556800/900000]\n",
      "current batch loss: 0.547637  [563200/900000]\n",
      "current batch loss: 0.600648  [569600/900000]\n",
      "current batch loss: 0.663079  [576000/900000]\n",
      "current batch loss: 0.616122  [582400/900000]\n",
      "current batch loss: 0.633093  [588800/900000]\n",
      "current batch loss: 0.663829  [595200/900000]\n",
      "current batch loss: 0.650289  [601600/900000]\n",
      "current batch loss: 0.619357  [608000/900000]\n",
      "current batch loss: 0.650611  [614400/900000]\n",
      "current batch loss: 0.681002  [620800/900000]\n",
      "current batch loss: 0.548921  [627200/900000]\n",
      "current batch loss: 0.654272  [633600/900000]\n",
      "current batch loss: 0.723155  [640000/900000]\n",
      "current batch loss: 0.662620  [646400/900000]\n",
      "current batch loss: 0.655125  [652800/900000]\n",
      "current batch loss: 0.604493  [659200/900000]\n",
      "current batch loss: 0.597808  [665600/900000]\n",
      "current batch loss: 0.662456  [672000/900000]\n",
      "current batch loss: 0.592447  [678400/900000]\n",
      "current batch loss: 0.665322  [684800/900000]\n",
      "current batch loss: 0.538157  [691200/900000]\n",
      "current batch loss: 0.673966  [697600/900000]\n",
      "current batch loss: 0.689124  [704000/900000]\n",
      "current batch loss: 0.698155  [710400/900000]\n",
      "current batch loss: 0.597395  [716800/900000]\n",
      "current batch loss: 0.608086  [723200/900000]\n",
      "current batch loss: 0.653561  [729600/900000]\n",
      "current batch loss: 0.608456  [736000/900000]\n",
      "current batch loss: 0.656685  [742400/900000]\n",
      "current batch loss: 0.669297  [748800/900000]\n",
      "current batch loss: 0.582316  [755200/900000]\n",
      "current batch loss: 0.704171  [761600/900000]\n",
      "current batch loss: 0.640778  [768000/900000]\n",
      "current batch loss: 0.650455  [774400/900000]\n",
      "current batch loss: 0.656713  [780800/900000]\n",
      "current batch loss: 0.663658  [787200/900000]\n",
      "current batch loss: 0.683186  [793600/900000]\n",
      "current batch loss: 0.687263  [800000/900000]\n",
      "current batch loss: 0.619235  [806400/900000]\n",
      "current batch loss: 0.607065  [812800/900000]\n",
      "current batch loss: 0.637822  [819200/900000]\n",
      "current batch loss: 0.543849  [825600/900000]\n",
      "current batch loss: 0.632955  [832000/900000]\n",
      "current batch loss: 0.634816  [838400/900000]\n",
      "current batch loss: 0.666949  [844800/900000]\n",
      "current batch loss: 0.578215  [851200/900000]\n",
      "current batch loss: 0.630030  [857600/900000]\n",
      "current batch loss: 0.627919  [864000/900000]\n",
      "current batch loss: 0.600385  [870400/900000]\n",
      "current batch loss: 0.630327  [876800/900000]\n",
      "current batch loss: 0.656185  [883200/900000]\n",
      "current batch loss: 0.647681  [889600/900000]\n",
      "current batch loss: 0.681784  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.634443\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.633179\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 14\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.593445  [    0/900000]\n",
      "current batch loss: 0.579824  [ 6400/900000]\n",
      "current batch loss: 0.536393  [12800/900000]\n",
      "current batch loss: 0.575434  [19200/900000]\n",
      "current batch loss: 0.572150  [25600/900000]\n",
      "current batch loss: 0.613537  [32000/900000]\n",
      "current batch loss: 0.596594  [38400/900000]\n",
      "current batch loss: 0.553838  [44800/900000]\n",
      "current batch loss: 0.555400  [51200/900000]\n",
      "current batch loss: 0.617188  [57600/900000]\n",
      "current batch loss: 0.582256  [64000/900000]\n",
      "current batch loss: 0.585755  [70400/900000]\n",
      "current batch loss: 0.631465  [76800/900000]\n",
      "current batch loss: 0.603574  [83200/900000]\n",
      "current batch loss: 0.609608  [89600/900000]\n",
      "current batch loss: 0.714252  [96000/900000]\n",
      "current batch loss: 0.619505  [102400/900000]\n",
      "current batch loss: 0.593018  [108800/900000]\n",
      "current batch loss: 0.629033  [115200/900000]\n",
      "current batch loss: 0.555620  [121600/900000]\n",
      "current batch loss: 0.610693  [128000/900000]\n",
      "current batch loss: 0.676877  [134400/900000]\n",
      "current batch loss: 0.650083  [140800/900000]\n",
      "current batch loss: 0.608668  [147200/900000]\n",
      "current batch loss: 0.612102  [153600/900000]\n",
      "current batch loss: 0.575384  [160000/900000]\n",
      "current batch loss: 0.674523  [166400/900000]\n",
      "current batch loss: 0.605169  [172800/900000]\n",
      "current batch loss: 0.580342  [179200/900000]\n",
      "current batch loss: 0.662983  [185600/900000]\n",
      "current batch loss: 0.632446  [192000/900000]\n",
      "current batch loss: 0.597728  [198400/900000]\n",
      "current batch loss: 0.604354  [204800/900000]\n",
      "current batch loss: 0.634908  [211200/900000]\n",
      "current batch loss: 0.647932  [217600/900000]\n",
      "current batch loss: 0.663019  [224000/900000]\n",
      "current batch loss: 0.638851  [230400/900000]\n",
      "current batch loss: 0.735161  [236800/900000]\n",
      "current batch loss: 0.627337  [243200/900000]\n",
      "current batch loss: 0.598359  [249600/900000]\n",
      "current batch loss: 0.629938  [256000/900000]\n",
      "current batch loss: 0.606118  [262400/900000]\n",
      "current batch loss: 0.640800  [268800/900000]\n",
      "current batch loss: 0.624237  [275200/900000]\n",
      "current batch loss: 0.628932  [281600/900000]\n",
      "current batch loss: 0.591648  [288000/900000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.615519  [294400/900000]\n",
      "current batch loss: 0.612653  [300800/900000]\n",
      "current batch loss: 0.691819  [307200/900000]\n",
      "current batch loss: 0.661141  [313600/900000]\n",
      "current batch loss: 0.608813  [320000/900000]\n",
      "current batch loss: 0.635709  [326400/900000]\n",
      "current batch loss: 0.629324  [332800/900000]\n",
      "current batch loss: 0.574409  [339200/900000]\n",
      "current batch loss: 0.589234  [345600/900000]\n",
      "current batch loss: 0.591508  [352000/900000]\n",
      "current batch loss: 0.667282  [358400/900000]\n",
      "current batch loss: 0.661180  [364800/900000]\n",
      "current batch loss: 0.559615  [371200/900000]\n",
      "current batch loss: 0.696058  [377600/900000]\n",
      "current batch loss: 0.658041  [384000/900000]\n",
      "current batch loss: 0.607162  [390400/900000]\n",
      "current batch loss: 0.635948  [396800/900000]\n",
      "current batch loss: 0.588183  [403200/900000]\n",
      "current batch loss: 0.697211  [409600/900000]\n",
      "current batch loss: 0.561055  [416000/900000]\n",
      "current batch loss: 0.662413  [422400/900000]\n",
      "current batch loss: 0.632188  [428800/900000]\n",
      "current batch loss: 0.576672  [435200/900000]\n",
      "current batch loss: 0.653523  [441600/900000]\n",
      "current batch loss: 0.645538  [448000/900000]\n",
      "current batch loss: 0.580060  [454400/900000]\n",
      "current batch loss: 0.607872  [460800/900000]\n",
      "current batch loss: 0.618105  [467200/900000]\n",
      "current batch loss: 0.591118  [473600/900000]\n",
      "current batch loss: 0.638517  [480000/900000]\n",
      "current batch loss: 0.571126  [486400/900000]\n",
      "current batch loss: 0.679331  [492800/900000]\n",
      "current batch loss: 0.609668  [499200/900000]\n",
      "current batch loss: 0.601847  [505600/900000]\n",
      "current batch loss: 0.638462  [512000/900000]\n",
      "current batch loss: 0.674626  [518400/900000]\n",
      "current batch loss: 0.599630  [524800/900000]\n",
      "current batch loss: 0.669464  [531200/900000]\n",
      "current batch loss: 0.672833  [537600/900000]\n",
      "current batch loss: 0.600075  [544000/900000]\n",
      "current batch loss: 0.700676  [550400/900000]\n",
      "current batch loss: 0.729081  [556800/900000]\n",
      "current batch loss: 0.547574  [563200/900000]\n",
      "current batch loss: 0.672954  [569600/900000]\n",
      "current batch loss: 0.636784  [576000/900000]\n",
      "current batch loss: 0.534415  [582400/900000]\n",
      "current batch loss: 0.589052  [588800/900000]\n",
      "current batch loss: 0.669724  [595200/900000]\n",
      "current batch loss: 0.610843  [601600/900000]\n",
      "current batch loss: 0.666972  [608000/900000]\n",
      "current batch loss: 0.635617  [614400/900000]\n",
      "current batch loss: 0.612537  [620800/900000]\n",
      "current batch loss: 0.649027  [627200/900000]\n",
      "current batch loss: 0.595423  [633600/900000]\n",
      "current batch loss: 0.642186  [640000/900000]\n",
      "current batch loss: 0.620853  [646400/900000]\n",
      "current batch loss: 0.570181  [652800/900000]\n",
      "current batch loss: 0.623581  [659200/900000]\n",
      "current batch loss: 0.610734  [665600/900000]\n",
      "current batch loss: 0.637163  [672000/900000]\n",
      "current batch loss: 0.647926  [678400/900000]\n",
      "current batch loss: 0.668238  [684800/900000]\n",
      "current batch loss: 0.599407  [691200/900000]\n",
      "current batch loss: 0.672513  [697600/900000]\n",
      "current batch loss: 0.629102  [704000/900000]\n",
      "current batch loss: 0.675376  [710400/900000]\n",
      "current batch loss: 0.617055  [716800/900000]\n",
      "current batch loss: 0.584395  [723200/900000]\n",
      "current batch loss: 0.619410  [729600/900000]\n",
      "current batch loss: 0.681794  [736000/900000]\n",
      "current batch loss: 0.672140  [742400/900000]\n",
      "current batch loss: 0.607897  [748800/900000]\n",
      "current batch loss: 0.664357  [755200/900000]\n",
      "current batch loss: 0.648103  [761600/900000]\n",
      "current batch loss: 0.656829  [768000/900000]\n",
      "current batch loss: 0.666714  [774400/900000]\n",
      "current batch loss: 0.686422  [780800/900000]\n",
      "current batch loss: 0.765714  [787200/900000]\n",
      "current batch loss: 0.620227  [793600/900000]\n",
      "current batch loss: 0.639108  [800000/900000]\n",
      "current batch loss: 0.649877  [806400/900000]\n",
      "current batch loss: 0.628607  [812800/900000]\n",
      "current batch loss: 0.620633  [819200/900000]\n",
      "current batch loss: 0.604478  [825600/900000]\n",
      "current batch loss: 0.554615  [832000/900000]\n",
      "current batch loss: 0.688148  [838400/900000]\n",
      "current batch loss: 0.694081  [844800/900000]\n",
      "current batch loss: 0.596930  [851200/900000]\n",
      "current batch loss: 0.608761  [857600/900000]\n",
      "current batch loss: 0.611864  [864000/900000]\n",
      "current batch loss: 0.595496  [870400/900000]\n",
      "current batch loss: 0.628286  [876800/900000]\n",
      "current batch loss: 0.629806  [883200/900000]\n",
      "current batch loss: 0.592933  [889600/900000]\n",
      "current batch loss: 0.648483  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.632253\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.631269\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 15\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.649423  [    0/900000]\n",
      "current batch loss: 0.576213  [ 6400/900000]\n",
      "current batch loss: 0.631967  [12800/900000]\n",
      "current batch loss: 0.629310  [19200/900000]\n",
      "current batch loss: 0.622506  [25600/900000]\n",
      "current batch loss: 0.638254  [32000/900000]\n",
      "current batch loss: 0.638630  [38400/900000]\n",
      "current batch loss: 0.649113  [44800/900000]\n",
      "current batch loss: 0.689861  [51200/900000]\n",
      "current batch loss: 0.718423  [57600/900000]\n",
      "current batch loss: 0.594595  [64000/900000]\n",
      "current batch loss: 0.629232  [70400/900000]\n",
      "current batch loss: 0.635576  [76800/900000]\n",
      "current batch loss: 0.651483  [83200/900000]\n",
      "current batch loss: 0.714573  [89600/900000]\n",
      "current batch loss: 0.615214  [96000/900000]\n",
      "current batch loss: 0.574148  [102400/900000]\n",
      "current batch loss: 0.676904  [108800/900000]\n",
      "current batch loss: 0.615220  [115200/900000]\n",
      "current batch loss: 0.659004  [121600/900000]\n",
      "current batch loss: 0.620511  [128000/900000]\n",
      "current batch loss: 0.593147  [134400/900000]\n",
      "current batch loss: 0.628875  [140800/900000]\n",
      "current batch loss: 0.622198  [147200/900000]\n",
      "current batch loss: 0.612645  [153600/900000]\n",
      "current batch loss: 0.602685  [160000/900000]\n",
      "current batch loss: 0.664026  [166400/900000]\n",
      "current batch loss: 0.710173  [172800/900000]\n",
      "current batch loss: 0.586651  [179200/900000]\n",
      "current batch loss: 0.563744  [185600/900000]\n",
      "current batch loss: 0.651141  [192000/900000]\n",
      "current batch loss: 0.667809  [198400/900000]\n",
      "current batch loss: 0.590309  [204800/900000]\n",
      "current batch loss: 0.646406  [211200/900000]\n",
      "current batch loss: 0.664468  [217600/900000]\n",
      "current batch loss: 0.631767  [224000/900000]\n",
      "current batch loss: 0.634549  [230400/900000]\n",
      "current batch loss: 0.617875  [236800/900000]\n",
      "current batch loss: 0.654544  [243200/900000]\n",
      "current batch loss: 0.680471  [249600/900000]\n",
      "current batch loss: 0.647009  [256000/900000]\n",
      "current batch loss: 0.564763  [262400/900000]\n",
      "current batch loss: 0.668507  [268800/900000]\n",
      "current batch loss: 0.699728  [275200/900000]\n",
      "current batch loss: 0.675829  [281600/900000]\n",
      "current batch loss: 0.601520  [288000/900000]\n",
      "current batch loss: 0.605974  [294400/900000]\n",
      "current batch loss: 0.597977  [300800/900000]\n",
      "current batch loss: 0.648641  [307200/900000]\n",
      "current batch loss: 0.614497  [313600/900000]\n",
      "current batch loss: 0.610706  [320000/900000]\n",
      "current batch loss: 0.674749  [326400/900000]\n",
      "current batch loss: 0.601457  [332800/900000]\n",
      "current batch loss: 0.637736  [339200/900000]\n",
      "current batch loss: 0.560472  [345600/900000]\n",
      "current batch loss: 0.584958  [352000/900000]\n",
      "current batch loss: 0.632079  [358400/900000]\n",
      "current batch loss: 0.642345  [364800/900000]\n",
      "current batch loss: 0.604451  [371200/900000]\n",
      "current batch loss: 0.698573  [377600/900000]\n",
      "current batch loss: 0.694850  [384000/900000]\n",
      "current batch loss: 0.626293  [390400/900000]\n",
      "current batch loss: 0.618159  [396800/900000]\n",
      "current batch loss: 0.578390  [403200/900000]\n",
      "current batch loss: 0.655323  [409600/900000]\n",
      "current batch loss: 0.655338  [416000/900000]\n",
      "current batch loss: 0.618101  [422400/900000]\n",
      "current batch loss: 0.586627  [428800/900000]\n",
      "current batch loss: 0.597957  [435200/900000]\n",
      "current batch loss: 0.692030  [441600/900000]\n",
      "current batch loss: 0.649232  [448000/900000]\n",
      "current batch loss: 0.650912  [454400/900000]\n",
      "current batch loss: 0.630449  [460800/900000]\n",
      "current batch loss: 0.599360  [467200/900000]\n",
      "current batch loss: 0.629042  [473600/900000]\n",
      "current batch loss: 0.606722  [480000/900000]\n",
      "current batch loss: 0.643960  [486400/900000]\n",
      "current batch loss: 0.603421  [492800/900000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.630970  [499200/900000]\n",
      "current batch loss: 0.642899  [505600/900000]\n",
      "current batch loss: 0.598598  [512000/900000]\n",
      "current batch loss: 0.664073  [518400/900000]\n",
      "current batch loss: 0.686770  [524800/900000]\n",
      "current batch loss: 0.671782  [531200/900000]\n",
      "current batch loss: 0.608384  [537600/900000]\n",
      "current batch loss: 0.571035  [544000/900000]\n",
      "current batch loss: 0.708669  [550400/900000]\n",
      "current batch loss: 0.638367  [556800/900000]\n",
      "current batch loss: 0.668606  [563200/900000]\n",
      "current batch loss: 0.604352  [569600/900000]\n",
      "current batch loss: 0.615124  [576000/900000]\n",
      "current batch loss: 0.644295  [582400/900000]\n",
      "current batch loss: 0.637727  [588800/900000]\n",
      "current batch loss: 0.623361  [595200/900000]\n",
      "current batch loss: 0.614222  [601600/900000]\n",
      "current batch loss: 0.594322  [608000/900000]\n",
      "current batch loss: 0.631039  [614400/900000]\n",
      "current batch loss: 0.669776  [620800/900000]\n",
      "current batch loss: 0.595652  [627200/900000]\n",
      "current batch loss: 0.595510  [633600/900000]\n",
      "current batch loss: 0.669536  [640000/900000]\n",
      "current batch loss: 0.651176  [646400/900000]\n",
      "current batch loss: 0.645638  [652800/900000]\n",
      "current batch loss: 0.655679  [659200/900000]\n",
      "current batch loss: 0.632097  [665600/900000]\n",
      "current batch loss: 0.622746  [672000/900000]\n",
      "current batch loss: 0.616360  [678400/900000]\n",
      "current batch loss: 0.636219  [684800/900000]\n",
      "current batch loss: 0.700858  [691200/900000]\n",
      "current batch loss: 0.572169  [697600/900000]\n",
      "current batch loss: 0.620908  [704000/900000]\n",
      "current batch loss: 0.679177  [710400/900000]\n",
      "current batch loss: 0.670290  [716800/900000]\n",
      "current batch loss: 0.598741  [723200/900000]\n",
      "current batch loss: 0.637226  [729600/900000]\n",
      "current batch loss: 0.621325  [736000/900000]\n",
      "current batch loss: 0.706436  [742400/900000]\n",
      "current batch loss: 0.622237  [748800/900000]\n",
      "current batch loss: 0.654017  [755200/900000]\n",
      "current batch loss: 0.653077  [761600/900000]\n",
      "current batch loss: 0.620925  [768000/900000]\n",
      "current batch loss: 0.614420  [774400/900000]\n",
      "current batch loss: 0.645608  [780800/900000]\n",
      "current batch loss: 0.617879  [787200/900000]\n",
      "current batch loss: 0.665991  [793600/900000]\n",
      "current batch loss: 0.561751  [800000/900000]\n",
      "current batch loss: 0.647827  [806400/900000]\n",
      "current batch loss: 0.574507  [812800/900000]\n",
      "current batch loss: 0.600304  [819200/900000]\n",
      "current batch loss: 0.602544  [825600/900000]\n",
      "current batch loss: 0.607283  [832000/900000]\n",
      "current batch loss: 0.637042  [838400/900000]\n",
      "current batch loss: 0.607495  [844800/900000]\n",
      "current batch loss: 0.606887  [851200/900000]\n",
      "current batch loss: 0.600431  [857600/900000]\n",
      "current batch loss: 0.672099  [864000/900000]\n",
      "current batch loss: 0.659139  [870400/900000]\n",
      "current batch loss: 0.669486  [876800/900000]\n",
      "current batch loss: 0.656538  [883200/900000]\n",
      "current batch loss: 0.573364  [889600/900000]\n",
      "current batch loss: 0.630351  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.637744\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.636588\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 16\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.642374  [    0/900000]\n",
      "current batch loss: 0.619031  [ 6400/900000]\n",
      "current batch loss: 0.579724  [12800/900000]\n",
      "current batch loss: 0.587343  [19200/900000]\n",
      "current batch loss: 0.673237  [25600/900000]\n",
      "current batch loss: 0.623887  [32000/900000]\n",
      "current batch loss: 0.641886  [38400/900000]\n",
      "current batch loss: 0.680538  [44800/900000]\n",
      "current batch loss: 0.569499  [51200/900000]\n",
      "current batch loss: 0.656389  [57600/900000]\n",
      "current batch loss: 0.656578  [64000/900000]\n",
      "current batch loss: 0.599760  [70400/900000]\n",
      "current batch loss: 0.543815  [76800/900000]\n",
      "current batch loss: 0.589839  [83200/900000]\n",
      "current batch loss: 0.820230  [89600/900000]\n",
      "current batch loss: 0.638833  [96000/900000]\n",
      "current batch loss: 0.694427  [102400/900000]\n",
      "current batch loss: 0.584918  [108800/900000]\n",
      "current batch loss: 0.586378  [115200/900000]\n",
      "current batch loss: 0.658304  [121600/900000]\n",
      "current batch loss: 0.651060  [128000/900000]\n",
      "current batch loss: 0.654197  [134400/900000]\n",
      "current batch loss: 0.620769  [140800/900000]\n",
      "current batch loss: 0.585397  [147200/900000]\n",
      "current batch loss: 0.664024  [153600/900000]\n",
      "current batch loss: 0.591902  [160000/900000]\n",
      "current batch loss: 0.623687  [166400/900000]\n",
      "current batch loss: 0.647475  [172800/900000]\n",
      "current batch loss: 0.647895  [179200/900000]\n",
      "current batch loss: 0.580148  [185600/900000]\n",
      "current batch loss: 0.568156  [192000/900000]\n",
      "current batch loss: 0.657775  [198400/900000]\n",
      "current batch loss: 0.705721  [204800/900000]\n",
      "current batch loss: 0.620306  [211200/900000]\n",
      "current batch loss: 0.690461  [217600/900000]\n",
      "current batch loss: 0.660018  [224000/900000]\n",
      "current batch loss: 0.638538  [230400/900000]\n",
      "current batch loss: 0.628235  [236800/900000]\n",
      "current batch loss: 0.702035  [243200/900000]\n",
      "current batch loss: 0.641336  [249600/900000]\n",
      "current batch loss: 0.595165  [256000/900000]\n",
      "current batch loss: 0.679319  [262400/900000]\n",
      "current batch loss: 0.604547  [268800/900000]\n",
      "current batch loss: 0.611486  [275200/900000]\n",
      "current batch loss: 0.632350  [281600/900000]\n",
      "current batch loss: 0.602247  [288000/900000]\n",
      "current batch loss: 0.625121  [294400/900000]\n",
      "current batch loss: 0.614986  [300800/900000]\n",
      "current batch loss: 0.632152  [307200/900000]\n",
      "current batch loss: 0.670074  [313600/900000]\n",
      "current batch loss: 0.672349  [320000/900000]\n",
      "current batch loss: 0.621224  [326400/900000]\n",
      "current batch loss: 0.670059  [332800/900000]\n",
      "current batch loss: 0.655048  [339200/900000]\n",
      "current batch loss: 0.568258  [345600/900000]\n",
      "current batch loss: 0.674642  [352000/900000]\n",
      "current batch loss: 0.655950  [358400/900000]\n",
      "current batch loss: 0.587534  [364800/900000]\n",
      "current batch loss: 0.601255  [371200/900000]\n",
      "current batch loss: 0.628445  [377600/900000]\n",
      "current batch loss: 0.675714  [384000/900000]\n",
      "current batch loss: 0.629521  [390400/900000]\n",
      "current batch loss: 0.687781  [396800/900000]\n",
      "current batch loss: 0.571142  [403200/900000]\n",
      "current batch loss: 0.638229  [409600/900000]\n",
      "current batch loss: 0.644570  [416000/900000]\n",
      "current batch loss: 0.671118  [422400/900000]\n",
      "current batch loss: 0.606869  [428800/900000]\n",
      "current batch loss: 0.574278  [435200/900000]\n",
      "current batch loss: 0.602075  [441600/900000]\n",
      "current batch loss: 0.626136  [448000/900000]\n",
      "current batch loss: 0.635494  [454400/900000]\n",
      "current batch loss: 0.561806  [460800/900000]\n",
      "current batch loss: 0.615758  [467200/900000]\n",
      "current batch loss: 0.611575  [473600/900000]\n",
      "current batch loss: 0.603402  [480000/900000]\n",
      "current batch loss: 0.673792  [486400/900000]\n",
      "current batch loss: 0.605743  [492800/900000]\n",
      "current batch loss: 0.584105  [499200/900000]\n",
      "current batch loss: 0.617726  [505600/900000]\n",
      "current batch loss: 0.582661  [512000/900000]\n",
      "current batch loss: 0.653899  [518400/900000]\n",
      "current batch loss: 0.620398  [524800/900000]\n",
      "current batch loss: 0.647389  [531200/900000]\n",
      "current batch loss: 0.660521  [537600/900000]\n",
      "current batch loss: 0.649044  [544000/900000]\n",
      "current batch loss: 0.655098  [550400/900000]\n",
      "current batch loss: 0.683214  [556800/900000]\n",
      "current batch loss: 0.597044  [563200/900000]\n",
      "current batch loss: 0.680843  [569600/900000]\n",
      "current batch loss: 0.656605  [576000/900000]\n",
      "current batch loss: 0.635997  [582400/900000]\n",
      "current batch loss: 0.604102  [588800/900000]\n",
      "current batch loss: 0.632541  [595200/900000]\n",
      "current batch loss: 0.669421  [601600/900000]\n",
      "current batch loss: 0.674828  [608000/900000]\n",
      "current batch loss: 0.622441  [614400/900000]\n",
      "current batch loss: 0.649174  [620800/900000]\n",
      "current batch loss: 0.673751  [627200/900000]\n",
      "current batch loss: 0.640320  [633600/900000]\n",
      "current batch loss: 0.626340  [640000/900000]\n",
      "current batch loss: 0.608879  [646400/900000]\n",
      "current batch loss: 0.599648  [652800/900000]\n",
      "current batch loss: 0.711782  [659200/900000]\n",
      "current batch loss: 0.533705  [665600/900000]\n",
      "current batch loss: 0.616654  [672000/900000]\n",
      "current batch loss: 0.644778  [678400/900000]\n",
      "current batch loss: 0.673090  [684800/900000]\n",
      "current batch loss: 0.682591  [691200/900000]\n",
      "current batch loss: 0.663991  [697600/900000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.596175  [704000/900000]\n",
      "current batch loss: 0.643327  [710400/900000]\n",
      "current batch loss: 0.633808  [716800/900000]\n",
      "current batch loss: 0.646568  [723200/900000]\n",
      "current batch loss: 0.622604  [729600/900000]\n",
      "current batch loss: 0.699017  [736000/900000]\n",
      "current batch loss: 0.606295  [742400/900000]\n",
      "current batch loss: 0.603748  [748800/900000]\n",
      "current batch loss: 0.649154  [755200/900000]\n",
      "current batch loss: 0.589875  [761600/900000]\n",
      "current batch loss: 0.648792  [768000/900000]\n",
      "current batch loss: 0.656912  [774400/900000]\n",
      "current batch loss: 0.638099  [780800/900000]\n",
      "current batch loss: 0.603777  [787200/900000]\n",
      "current batch loss: 0.684455  [793600/900000]\n",
      "current batch loss: 0.614265  [800000/900000]\n",
      "current batch loss: 0.553021  [806400/900000]\n",
      "current batch loss: 0.633105  [812800/900000]\n",
      "current batch loss: 0.605900  [819200/900000]\n",
      "current batch loss: 0.538036  [825600/900000]\n",
      "current batch loss: 0.571238  [832000/900000]\n",
      "current batch loss: 0.672662  [838400/900000]\n",
      "current batch loss: 0.632474  [844800/900000]\n",
      "current batch loss: 0.595407  [851200/900000]\n",
      "current batch loss: 0.663339  [857600/900000]\n",
      "current batch loss: 0.597244  [864000/900000]\n",
      "current batch loss: 0.623599  [870400/900000]\n",
      "current batch loss: 0.665819  [876800/900000]\n",
      "current batch loss: 0.669408  [883200/900000]\n",
      "current batch loss: 0.628468  [889600/900000]\n",
      "current batch loss: 0.633900  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.635504\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.634123\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 17\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.602685  [    0/900000]\n",
      "current batch loss: 0.632588  [ 6400/900000]\n",
      "current batch loss: 0.644066  [12800/900000]\n",
      "current batch loss: 0.620492  [19200/900000]\n",
      "current batch loss: 0.651280  [25600/900000]\n",
      "current batch loss: 0.628748  [32000/900000]\n",
      "current batch loss: 0.693334  [38400/900000]\n",
      "current batch loss: 0.624824  [44800/900000]\n",
      "current batch loss: 0.672062  [51200/900000]\n",
      "current batch loss: 0.605437  [57600/900000]\n",
      "current batch loss: 0.624363  [64000/900000]\n",
      "current batch loss: 0.666836  [70400/900000]\n",
      "current batch loss: 0.677153  [76800/900000]\n",
      "current batch loss: 0.575268  [83200/900000]\n",
      "current batch loss: 0.621651  [89600/900000]\n",
      "current batch loss: 0.673837  [96000/900000]\n",
      "current batch loss: 0.625759  [102400/900000]\n",
      "current batch loss: 0.600987  [108800/900000]\n",
      "current batch loss: 0.634040  [115200/900000]\n",
      "current batch loss: 0.590619  [121600/900000]\n",
      "current batch loss: 0.648722  [128000/900000]\n",
      "current batch loss: 0.631774  [134400/900000]\n",
      "current batch loss: 0.661827  [140800/900000]\n",
      "current batch loss: 0.632043  [147200/900000]\n",
      "current batch loss: 0.633376  [153600/900000]\n",
      "current batch loss: 0.615283  [160000/900000]\n",
      "current batch loss: 0.582917  [166400/900000]\n",
      "current batch loss: 0.674099  [172800/900000]\n",
      "current batch loss: 0.656041  [179200/900000]\n",
      "current batch loss: 0.668936  [185600/900000]\n",
      "current batch loss: 0.637708  [192000/900000]\n",
      "current batch loss: 0.708924  [198400/900000]\n",
      "current batch loss: 0.629592  [204800/900000]\n",
      "current batch loss: 0.656975  [211200/900000]\n",
      "current batch loss: 0.662981  [217600/900000]\n",
      "current batch loss: 0.654703  [224000/900000]\n",
      "current batch loss: 0.614393  [230400/900000]\n",
      "current batch loss: 0.625150  [236800/900000]\n",
      "current batch loss: 0.706178  [243200/900000]\n",
      "current batch loss: 0.636642  [249600/900000]\n",
      "current batch loss: 0.666281  [256000/900000]\n",
      "current batch loss: 0.607177  [262400/900000]\n",
      "current batch loss: 0.694476  [268800/900000]\n",
      "current batch loss: 0.615363  [275200/900000]\n",
      "current batch loss: 0.633515  [281600/900000]\n",
      "current batch loss: 0.564358  [288000/900000]\n",
      "current batch loss: 0.629585  [294400/900000]\n",
      "current batch loss: 0.613595  [300800/900000]\n",
      "current batch loss: 0.605453  [307200/900000]\n",
      "current batch loss: 0.632937  [313600/900000]\n",
      "current batch loss: 0.650641  [320000/900000]\n",
      "current batch loss: 0.596606  [326400/900000]\n",
      "current batch loss: 0.610978  [332800/900000]\n",
      "current batch loss: 0.666659  [339200/900000]\n",
      "current batch loss: 0.596236  [345600/900000]\n",
      "current batch loss: 0.679520  [352000/900000]\n",
      "current batch loss: 0.611335  [358400/900000]\n",
      "current batch loss: 0.615895  [364800/900000]\n",
      "current batch loss: 0.611762  [371200/900000]\n",
      "current batch loss: 0.619539  [377600/900000]\n",
      "current batch loss: 0.629197  [384000/900000]\n",
      "current batch loss: 0.630290  [390400/900000]\n",
      "current batch loss: 0.565607  [396800/900000]\n",
      "current batch loss: 0.722175  [403200/900000]\n",
      "current batch loss: 0.623764  [409600/900000]\n",
      "current batch loss: 0.577618  [416000/900000]\n",
      "current batch loss: 0.588804  [422400/900000]\n",
      "current batch loss: 0.595694  [428800/900000]\n",
      "current batch loss: 0.724438  [435200/900000]\n",
      "current batch loss: 0.660072  [441600/900000]\n",
      "current batch loss: 0.628034  [448000/900000]\n",
      "current batch loss: 0.605311  [454400/900000]\n",
      "current batch loss: 0.655495  [460800/900000]\n",
      "current batch loss: 0.613495  [467200/900000]\n",
      "current batch loss: 0.577309  [473600/900000]\n",
      "current batch loss: 0.631743  [480000/900000]\n",
      "current batch loss: 0.621140  [486400/900000]\n",
      "current batch loss: 0.646584  [492800/900000]\n",
      "current batch loss: 0.642611  [499200/900000]\n",
      "current batch loss: 0.584662  [505600/900000]\n",
      "current batch loss: 0.606251  [512000/900000]\n",
      "current batch loss: 0.646091  [518400/900000]\n",
      "current batch loss: 0.561335  [524800/900000]\n",
      "current batch loss: 0.704704  [531200/900000]\n",
      "current batch loss: 0.621896  [537600/900000]\n",
      "current batch loss: 0.686838  [544000/900000]\n",
      "current batch loss: 0.605160  [550400/900000]\n",
      "current batch loss: 0.606656  [556800/900000]\n",
      "current batch loss: 0.611446  [563200/900000]\n",
      "current batch loss: 0.598265  [569600/900000]\n",
      "current batch loss: 0.635274  [576000/900000]\n",
      "current batch loss: 0.649552  [582400/900000]\n",
      "current batch loss: 0.687384  [588800/900000]\n",
      "current batch loss: 0.590010  [595200/900000]\n",
      "current batch loss: 0.705804  [601600/900000]\n",
      "current batch loss: 0.714482  [608000/900000]\n",
      "current batch loss: 0.573785  [614400/900000]\n",
      "current batch loss: 0.570787  [620800/900000]\n",
      "current batch loss: 0.549940  [627200/900000]\n",
      "current batch loss: 0.737155  [633600/900000]\n",
      "current batch loss: 0.642905  [640000/900000]\n",
      "current batch loss: 0.647074  [646400/900000]\n",
      "current batch loss: 0.592184  [652800/900000]\n",
      "current batch loss: 0.626603  [659200/900000]\n",
      "current batch loss: 0.657210  [665600/900000]\n",
      "current batch loss: 0.651387  [672000/900000]\n",
      "current batch loss: 0.630262  [678400/900000]\n",
      "current batch loss: 0.658815  [684800/900000]\n",
      "current batch loss: 0.607488  [691200/900000]\n",
      "current batch loss: 0.574216  [697600/900000]\n",
      "current batch loss: 0.635507  [704000/900000]\n",
      "current batch loss: 0.600801  [710400/900000]\n",
      "current batch loss: 0.569261  [716800/900000]\n",
      "current batch loss: 0.719289  [723200/900000]\n",
      "current batch loss: 0.617052  [729600/900000]\n",
      "current batch loss: 0.589539  [736000/900000]\n",
      "current batch loss: 0.576992  [742400/900000]\n",
      "current batch loss: 0.632228  [748800/900000]\n",
      "current batch loss: 0.655193  [755200/900000]\n",
      "current batch loss: 0.633420  [761600/900000]\n",
      "current batch loss: 0.684703  [768000/900000]\n",
      "current batch loss: 0.594842  [774400/900000]\n",
      "current batch loss: 0.654495  [780800/900000]\n",
      "current batch loss: 0.564934  [787200/900000]\n",
      "current batch loss: 0.592777  [793600/900000]\n",
      "current batch loss: 0.599960  [800000/900000]\n",
      "current batch loss: 0.576127  [806400/900000]\n",
      "current batch loss: 0.593991  [812800/900000]\n",
      "current batch loss: 0.662789  [819200/900000]\n",
      "current batch loss: 0.694368  [825600/900000]\n",
      "current batch loss: 0.632523  [832000/900000]\n",
      "current batch loss: 0.684938  [838400/900000]\n",
      "current batch loss: 0.652600  [844800/900000]\n",
      "current batch loss: 0.635471  [851200/900000]\n",
      "current batch loss: 0.635961  [857600/900000]\n",
      "current batch loss: 0.581343  [864000/900000]\n",
      "current batch loss: 0.678677  [870400/900000]\n",
      "current batch loss: 0.567306  [876800/900000]\n",
      "current batch loss: 0.642247  [883200/900000]\n",
      "current batch loss: 0.657013  [889600/900000]\n",
      "current batch loss: 0.655468  [896000/900000]\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg trn loss per batch: 0.631309\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.630149\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 18\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.535918  [    0/900000]\n",
      "current batch loss: 0.640870  [ 6400/900000]\n",
      "current batch loss: 0.637583  [12800/900000]\n",
      "current batch loss: 0.639264  [19200/900000]\n",
      "current batch loss: 0.624748  [25600/900000]\n",
      "current batch loss: 0.590372  [32000/900000]\n",
      "current batch loss: 0.567270  [38400/900000]\n",
      "current batch loss: 0.607458  [44800/900000]\n",
      "current batch loss: 0.638532  [51200/900000]\n",
      "current batch loss: 0.598634  [57600/900000]\n",
      "current batch loss: 0.613329  [64000/900000]\n",
      "current batch loss: 0.616194  [70400/900000]\n",
      "current batch loss: 0.702103  [76800/900000]\n",
      "current batch loss: 0.628829  [83200/900000]\n",
      "current batch loss: 0.625349  [89600/900000]\n",
      "current batch loss: 0.592075  [96000/900000]\n",
      "current batch loss: 0.624179  [102400/900000]\n",
      "current batch loss: 0.629984  [108800/900000]\n",
      "current batch loss: 0.568178  [115200/900000]\n",
      "current batch loss: 0.687524  [121600/900000]\n",
      "current batch loss: 0.618176  [128000/900000]\n",
      "current batch loss: 0.677268  [134400/900000]\n",
      "current batch loss: 0.579105  [140800/900000]\n",
      "current batch loss: 0.593447  [147200/900000]\n",
      "current batch loss: 0.630794  [153600/900000]\n",
      "current batch loss: 0.624053  [160000/900000]\n",
      "current batch loss: 0.655000  [166400/900000]\n",
      "current batch loss: 0.592046  [172800/900000]\n",
      "current batch loss: 0.655168  [179200/900000]\n",
      "current batch loss: 0.663989  [185600/900000]\n",
      "current batch loss: 0.633105  [192000/900000]\n",
      "current batch loss: 0.587892  [198400/900000]\n",
      "current batch loss: 0.609253  [204800/900000]\n",
      "current batch loss: 0.563922  [211200/900000]\n",
      "current batch loss: 0.656067  [217600/900000]\n",
      "current batch loss: 0.692882  [224000/900000]\n",
      "current batch loss: 0.566851  [230400/900000]\n",
      "current batch loss: 0.608355  [236800/900000]\n",
      "current batch loss: 0.609798  [243200/900000]\n",
      "current batch loss: 0.590016  [249600/900000]\n",
      "current batch loss: 0.619349  [256000/900000]\n",
      "current batch loss: 0.631202  [262400/900000]\n",
      "current batch loss: 0.688986  [268800/900000]\n",
      "current batch loss: 0.615027  [275200/900000]\n",
      "current batch loss: 0.588179  [281600/900000]\n",
      "current batch loss: 0.584604  [288000/900000]\n",
      "current batch loss: 0.543558  [294400/900000]\n",
      "current batch loss: 0.639532  [300800/900000]\n",
      "current batch loss: 0.683484  [307200/900000]\n",
      "current batch loss: 0.639283  [313600/900000]\n",
      "current batch loss: 0.668478  [320000/900000]\n",
      "current batch loss: 0.617581  [326400/900000]\n",
      "current batch loss: 0.650921  [332800/900000]\n",
      "current batch loss: 0.611324  [339200/900000]\n",
      "current batch loss: 0.564672  [345600/900000]\n",
      "current batch loss: 0.646859  [352000/900000]\n",
      "current batch loss: 0.592118  [358400/900000]\n",
      "current batch loss: 0.590328  [364800/900000]\n",
      "current batch loss: 0.649208  [371200/900000]\n",
      "current batch loss: 0.599030  [377600/900000]\n",
      "current batch loss: 0.645710  [384000/900000]\n",
      "current batch loss: 0.606438  [390400/900000]\n",
      "current batch loss: 0.595459  [396800/900000]\n",
      "current batch loss: 0.653124  [403200/900000]\n",
      "current batch loss: 0.685050  [409600/900000]\n",
      "current batch loss: 0.619059  [416000/900000]\n",
      "current batch loss: 0.633695  [422400/900000]\n",
      "current batch loss: 0.638071  [428800/900000]\n",
      "current batch loss: 0.622912  [435200/900000]\n",
      "current batch loss: 0.685377  [441600/900000]\n",
      "current batch loss: 0.710152  [448000/900000]\n",
      "current batch loss: 0.571851  [454400/900000]\n",
      "current batch loss: 0.578440  [460800/900000]\n",
      "current batch loss: 0.662100  [467200/900000]\n",
      "current batch loss: 0.599985  [473600/900000]\n",
      "current batch loss: 0.693671  [480000/900000]\n",
      "current batch loss: 0.610036  [486400/900000]\n",
      "current batch loss: 0.687613  [492800/900000]\n",
      "current batch loss: 0.685762  [499200/900000]\n",
      "current batch loss: 0.666356  [505600/900000]\n",
      "current batch loss: 0.662820  [512000/900000]\n",
      "current batch loss: 0.659740  [518400/900000]\n",
      "current batch loss: 0.577966  [524800/900000]\n",
      "current batch loss: 0.606827  [531200/900000]\n",
      "current batch loss: 0.631695  [537600/900000]\n",
      "current batch loss: 0.643229  [544000/900000]\n",
      "current batch loss: 0.627648  [550400/900000]\n",
      "current batch loss: 0.598293  [556800/900000]\n",
      "current batch loss: 0.595069  [563200/900000]\n",
      "current batch loss: 0.659539  [569600/900000]\n",
      "current batch loss: 0.681210  [576000/900000]\n",
      "current batch loss: 0.575174  [582400/900000]\n",
      "current batch loss: 0.623524  [588800/900000]\n",
      "current batch loss: 0.681283  [595200/900000]\n",
      "current batch loss: 0.580162  [601600/900000]\n",
      "current batch loss: 0.659271  [608000/900000]\n",
      "current batch loss: 0.628847  [614400/900000]\n",
      "current batch loss: 0.645644  [620800/900000]\n",
      "current batch loss: 0.623146  [627200/900000]\n",
      "current batch loss: 0.607300  [633600/900000]\n",
      "current batch loss: 0.664435  [640000/900000]\n",
      "current batch loss: 0.597573  [646400/900000]\n",
      "current batch loss: 0.697367  [652800/900000]\n",
      "current batch loss: 0.609111  [659200/900000]\n",
      "current batch loss: 0.613211  [665600/900000]\n",
      "current batch loss: 0.574025  [672000/900000]\n",
      "current batch loss: 0.640109  [678400/900000]\n",
      "current batch loss: 0.624935  [684800/900000]\n",
      "current batch loss: 0.624935  [691200/900000]\n",
      "current batch loss: 0.647034  [697600/900000]\n",
      "current batch loss: 0.636844  [704000/900000]\n",
      "current batch loss: 0.623183  [710400/900000]\n",
      "current batch loss: 0.667375  [716800/900000]\n",
      "current batch loss: 0.648238  [723200/900000]\n",
      "current batch loss: 0.547467  [729600/900000]\n",
      "current batch loss: 0.616061  [736000/900000]\n",
      "current batch loss: 0.627772  [742400/900000]\n",
      "current batch loss: 0.643368  [748800/900000]\n",
      "current batch loss: 0.609024  [755200/900000]\n",
      "current batch loss: 0.560084  [761600/900000]\n",
      "current batch loss: 0.685185  [768000/900000]\n",
      "current batch loss: 0.682913  [774400/900000]\n",
      "current batch loss: 0.677451  [780800/900000]\n",
      "current batch loss: 0.689270  [787200/900000]\n",
      "current batch loss: 0.682300  [793600/900000]\n",
      "current batch loss: 0.592121  [800000/900000]\n",
      "current batch loss: 0.625239  [806400/900000]\n",
      "current batch loss: 0.636926  [812800/900000]\n",
      "current batch loss: 0.644920  [819200/900000]\n",
      "current batch loss: 0.672528  [825600/900000]\n",
      "current batch loss: 0.592788  [832000/900000]\n",
      "current batch loss: 0.619164  [838400/900000]\n",
      "current batch loss: 0.614778  [844800/900000]\n",
      "current batch loss: 0.581793  [851200/900000]\n",
      "current batch loss: 0.685857  [857600/900000]\n",
      "current batch loss: 0.628748  [864000/900000]\n",
      "current batch loss: 0.632384  [870400/900000]\n",
      "current batch loss: 0.612382  [876800/900000]\n",
      "current batch loss: 0.700130  [883200/900000]\n",
      "current batch loss: 0.593273  [889600/900000]\n",
      "current batch loss: 0.573677  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.636490\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.635609\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 19\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.638267  [    0/900000]\n",
      "current batch loss: 0.612177  [ 6400/900000]\n",
      "current batch loss: 0.648971  [12800/900000]\n",
      "current batch loss: 0.594513  [19200/900000]\n",
      "current batch loss: 0.660840  [25600/900000]\n",
      "current batch loss: 0.630170  [32000/900000]\n",
      "current batch loss: 0.637301  [38400/900000]\n",
      "current batch loss: 0.644514  [44800/900000]\n",
      "current batch loss: 0.763657  [51200/900000]\n",
      "current batch loss: 0.690110  [57600/900000]\n",
      "current batch loss: 0.664691  [64000/900000]\n",
      "current batch loss: 0.624814  [70400/900000]\n",
      "current batch loss: 0.683921  [76800/900000]\n",
      "current batch loss: 0.612510  [83200/900000]\n",
      "current batch loss: 0.645891  [89600/900000]\n",
      "current batch loss: 0.643370  [96000/900000]\n",
      "current batch loss: 0.676446  [102400/900000]\n",
      "current batch loss: 0.721899  [108800/900000]\n",
      "current batch loss: 0.663005  [115200/900000]\n",
      "current batch loss: 0.656269  [121600/900000]\n",
      "current batch loss: 0.610358  [128000/900000]\n",
      "current batch loss: 0.590974  [134400/900000]\n",
      "current batch loss: 0.650556  [140800/900000]\n",
      "current batch loss: 0.640947  [147200/900000]\n",
      "current batch loss: 0.554667  [153600/900000]\n",
      "current batch loss: 0.584535  [160000/900000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.669059  [166400/900000]\n",
      "current batch loss: 0.635470  [172800/900000]\n",
      "current batch loss: 0.639011  [179200/900000]\n",
      "current batch loss: 0.720471  [185600/900000]\n",
      "current batch loss: 0.656927  [192000/900000]\n",
      "current batch loss: 0.594398  [198400/900000]\n",
      "current batch loss: 0.712204  [204800/900000]\n",
      "current batch loss: 0.636934  [211200/900000]\n",
      "current batch loss: 0.698414  [217600/900000]\n",
      "current batch loss: 0.699503  [224000/900000]\n",
      "current batch loss: 0.644345  [230400/900000]\n",
      "current batch loss: 0.576322  [236800/900000]\n",
      "current batch loss: 0.619424  [243200/900000]\n",
      "current batch loss: 0.613655  [249600/900000]\n",
      "current batch loss: 0.694011  [256000/900000]\n",
      "current batch loss: 0.653647  [262400/900000]\n",
      "current batch loss: 0.600496  [268800/900000]\n",
      "current batch loss: 0.605327  [275200/900000]\n",
      "current batch loss: 0.609516  [281600/900000]\n",
      "current batch loss: 0.575112  [288000/900000]\n",
      "current batch loss: 0.712628  [294400/900000]\n",
      "current batch loss: 0.773949  [300800/900000]\n",
      "current batch loss: 0.674210  [307200/900000]\n",
      "current batch loss: 0.565323  [313600/900000]\n",
      "current batch loss: 0.636153  [320000/900000]\n",
      "current batch loss: 0.654968  [326400/900000]\n",
      "current batch loss: 0.598283  [332800/900000]\n",
      "current batch loss: 0.581797  [339200/900000]\n",
      "current batch loss: 0.616212  [345600/900000]\n",
      "current batch loss: 0.673962  [352000/900000]\n",
      "current batch loss: 0.599070  [358400/900000]\n",
      "current batch loss: 0.591972  [364800/900000]\n",
      "current batch loss: 0.657800  [371200/900000]\n",
      "current batch loss: 0.603182  [377600/900000]\n",
      "current batch loss: 0.583398  [384000/900000]\n",
      "current batch loss: 0.622551  [390400/900000]\n",
      "current batch loss: 0.659072  [396800/900000]\n",
      "current batch loss: 0.589227  [403200/900000]\n",
      "current batch loss: 0.669845  [409600/900000]\n",
      "current batch loss: 0.642654  [416000/900000]\n",
      "current batch loss: 0.675405  [422400/900000]\n",
      "current batch loss: 0.638920  [428800/900000]\n",
      "current batch loss: 0.627561  [435200/900000]\n",
      "current batch loss: 0.619066  [441600/900000]\n",
      "current batch loss: 0.588184  [448000/900000]\n",
      "current batch loss: 0.633940  [454400/900000]\n",
      "current batch loss: 0.693620  [460800/900000]\n",
      "current batch loss: 0.591612  [467200/900000]\n",
      "current batch loss: 0.694326  [473600/900000]\n",
      "current batch loss: 0.638062  [480000/900000]\n",
      "current batch loss: 0.601550  [486400/900000]\n",
      "current batch loss: 0.695682  [492800/900000]\n",
      "current batch loss: 0.679286  [499200/900000]\n",
      "current batch loss: 0.669231  [505600/900000]\n",
      "current batch loss: 0.615429  [512000/900000]\n",
      "current batch loss: 0.634286  [518400/900000]\n",
      "current batch loss: 0.621266  [524800/900000]\n",
      "current batch loss: 0.618216  [531200/900000]\n",
      "current batch loss: 0.643233  [537600/900000]\n",
      "current batch loss: 0.628096  [544000/900000]\n",
      "current batch loss: 0.657846  [550400/900000]\n",
      "current batch loss: 0.595502  [556800/900000]\n",
      "current batch loss: 0.662650  [563200/900000]\n",
      "current batch loss: 0.676720  [569600/900000]\n",
      "current batch loss: 0.584119  [576000/900000]\n",
      "current batch loss: 0.626471  [582400/900000]\n",
      "current batch loss: 0.599160  [588800/900000]\n",
      "current batch loss: 0.607158  [595200/900000]\n",
      "current batch loss: 0.697390  [601600/900000]\n",
      "current batch loss: 0.696656  [608000/900000]\n",
      "current batch loss: 0.649483  [614400/900000]\n",
      "current batch loss: 0.642322  [620800/900000]\n",
      "current batch loss: 0.612606  [627200/900000]\n",
      "current batch loss: 0.650108  [633600/900000]\n",
      "current batch loss: 0.603159  [640000/900000]\n",
      "current batch loss: 0.582209  [646400/900000]\n",
      "current batch loss: 0.640904  [652800/900000]\n",
      "current batch loss: 0.641905  [659200/900000]\n",
      "current batch loss: 0.662741  [665600/900000]\n",
      "current batch loss: 0.597879  [672000/900000]\n",
      "current batch loss: 0.733341  [678400/900000]\n",
      "current batch loss: 0.630595  [684800/900000]\n",
      "current batch loss: 0.578972  [691200/900000]\n",
      "current batch loss: 0.653038  [697600/900000]\n",
      "current batch loss: 0.588978  [704000/900000]\n",
      "current batch loss: 0.581228  [710400/900000]\n",
      "current batch loss: 0.624324  [716800/900000]\n",
      "current batch loss: 0.612396  [723200/900000]\n",
      "current batch loss: 0.635261  [729600/900000]\n",
      "current batch loss: 0.658265  [736000/900000]\n",
      "current batch loss: 0.656235  [742400/900000]\n",
      "current batch loss: 0.692843  [748800/900000]\n",
      "current batch loss: 0.594888  [755200/900000]\n",
      "current batch loss: 0.621826  [761600/900000]\n",
      "current batch loss: 0.619523  [768000/900000]\n",
      "current batch loss: 0.604270  [774400/900000]\n",
      "current batch loss: 0.587806  [780800/900000]\n",
      "current batch loss: 0.633249  [787200/900000]\n",
      "current batch loss: 0.676700  [793600/900000]\n",
      "current batch loss: 0.658166  [800000/900000]\n",
      "current batch loss: 0.732623  [806400/900000]\n",
      "current batch loss: 0.651822  [812800/900000]\n",
      "current batch loss: 0.590882  [819200/900000]\n",
      "current batch loss: 0.652680  [825600/900000]\n",
      "current batch loss: 0.692722  [832000/900000]\n",
      "current batch loss: 0.655046  [838400/900000]\n",
      "current batch loss: 0.653934  [844800/900000]\n",
      "current batch loss: 0.661865  [851200/900000]\n",
      "current batch loss: 0.667318  [857600/900000]\n",
      "current batch loss: 0.536691  [864000/900000]\n",
      "current batch loss: 0.656749  [870400/900000]\n",
      "current batch loss: 0.776969  [876800/900000]\n",
      "current batch loss: 0.696573  [883200/900000]\n",
      "current batch loss: 0.631818  [889600/900000]\n",
      "current batch loss: 0.670210  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.637400\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.636598\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 20\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.622375  [    0/900000]\n",
      "current batch loss: 0.700353  [ 6400/900000]\n",
      "current batch loss: 0.615926  [12800/900000]\n",
      "current batch loss: 0.620207  [19200/900000]\n",
      "current batch loss: 0.609144  [25600/900000]\n",
      "current batch loss: 0.624420  [32000/900000]\n",
      "current batch loss: 0.567428  [38400/900000]\n",
      "current batch loss: 0.563916  [44800/900000]\n",
      "current batch loss: 0.616657  [51200/900000]\n",
      "current batch loss: 0.638732  [57600/900000]\n",
      "current batch loss: 0.597571  [64000/900000]\n",
      "current batch loss: 0.615898  [70400/900000]\n",
      "current batch loss: 0.628593  [76800/900000]\n",
      "current batch loss: 0.669487  [83200/900000]\n",
      "current batch loss: 0.586171  [89600/900000]\n",
      "current batch loss: 0.671325  [96000/900000]\n",
      "current batch loss: 0.619118  [102400/900000]\n",
      "current batch loss: 0.568517  [108800/900000]\n",
      "current batch loss: 0.641486  [115200/900000]\n",
      "current batch loss: 0.609860  [121600/900000]\n",
      "current batch loss: 0.684382  [128000/900000]\n",
      "current batch loss: 0.623155  [134400/900000]\n",
      "current batch loss: 0.600957  [140800/900000]\n",
      "current batch loss: 0.634239  [147200/900000]\n",
      "current batch loss: 0.655612  [153600/900000]\n",
      "current batch loss: 0.587284  [160000/900000]\n",
      "current batch loss: 0.593493  [166400/900000]\n",
      "current batch loss: 0.624475  [172800/900000]\n",
      "current batch loss: 0.624423  [179200/900000]\n",
      "current batch loss: 0.588179  [185600/900000]\n",
      "current batch loss: 0.638726  [192000/900000]\n",
      "current batch loss: 0.596633  [198400/900000]\n",
      "current batch loss: 0.649202  [204800/900000]\n",
      "current batch loss: 0.664839  [211200/900000]\n",
      "current batch loss: 0.678386  [217600/900000]\n",
      "current batch loss: 0.678310  [224000/900000]\n",
      "current batch loss: 0.589937  [230400/900000]\n",
      "current batch loss: 0.670231  [236800/900000]\n",
      "current batch loss: 0.574460  [243200/900000]\n",
      "current batch loss: 0.590530  [249600/900000]\n",
      "current batch loss: 0.652131  [256000/900000]\n",
      "current batch loss: 0.642163  [262400/900000]\n",
      "current batch loss: 0.621060  [268800/900000]\n",
      "current batch loss: 0.579964  [275200/900000]\n",
      "current batch loss: 0.649546  [281600/900000]\n",
      "current batch loss: 0.617517  [288000/900000]\n",
      "current batch loss: 0.709379  [294400/900000]\n",
      "current batch loss: 0.643913  [300800/900000]\n",
      "current batch loss: 0.616185  [307200/900000]\n",
      "current batch loss: 0.605987  [313600/900000]\n",
      "current batch loss: 0.632267  [320000/900000]\n",
      "current batch loss: 0.622563  [326400/900000]\n",
      "current batch loss: 0.624180  [332800/900000]\n",
      "current batch loss: 0.608847  [339200/900000]\n",
      "current batch loss: 0.629706  [345600/900000]\n",
      "current batch loss: 0.682654  [352000/900000]\n",
      "current batch loss: 0.579220  [358400/900000]\n",
      "current batch loss: 0.740575  [364800/900000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.619981  [371200/900000]\n",
      "current batch loss: 0.650997  [377600/900000]\n",
      "current batch loss: 0.632740  [384000/900000]\n",
      "current batch loss: 0.649406  [390400/900000]\n",
      "current batch loss: 0.709812  [396800/900000]\n",
      "current batch loss: 0.641416  [403200/900000]\n",
      "current batch loss: 0.720238  [409600/900000]\n",
      "current batch loss: 0.623068  [416000/900000]\n",
      "current batch loss: 0.651503  [422400/900000]\n",
      "current batch loss: 0.695293  [428800/900000]\n",
      "current batch loss: 0.696659  [435200/900000]\n",
      "current batch loss: 0.690838  [441600/900000]\n",
      "current batch loss: 0.662399  [448000/900000]\n",
      "current batch loss: 0.625372  [454400/900000]\n",
      "current batch loss: 0.662240  [460800/900000]\n",
      "current batch loss: 0.632165  [467200/900000]\n",
      "current batch loss: 0.649024  [473600/900000]\n",
      "current batch loss: 0.602395  [480000/900000]\n",
      "current batch loss: 0.579077  [486400/900000]\n",
      "current batch loss: 0.609945  [492800/900000]\n",
      "current batch loss: 0.719513  [499200/900000]\n",
      "current batch loss: 0.562503  [505600/900000]\n",
      "current batch loss: 0.579968  [512000/900000]\n",
      "current batch loss: 0.575759  [518400/900000]\n",
      "current batch loss: 0.620907  [524800/900000]\n",
      "current batch loss: 0.702440  [531200/900000]\n",
      "current batch loss: 0.676729  [537600/900000]\n",
      "current batch loss: 0.587192  [544000/900000]\n",
      "current batch loss: 0.662720  [550400/900000]\n",
      "current batch loss: 0.650241  [556800/900000]\n",
      "current batch loss: 0.643740  [563200/900000]\n",
      "current batch loss: 0.595222  [569600/900000]\n",
      "current batch loss: 0.654123  [576000/900000]\n",
      "current batch loss: 0.633906  [582400/900000]\n",
      "current batch loss: 0.699638  [588800/900000]\n",
      "current batch loss: 0.565040  [595200/900000]\n",
      "current batch loss: 0.597687  [601600/900000]\n",
      "current batch loss: 0.677196  [608000/900000]\n",
      "current batch loss: 0.623645  [614400/900000]\n",
      "current batch loss: 0.613940  [620800/900000]\n",
      "current batch loss: 0.622106  [627200/900000]\n",
      "current batch loss: 0.594511  [633600/900000]\n",
      "current batch loss: 0.617677  [640000/900000]\n",
      "current batch loss: 0.668565  [646400/900000]\n",
      "current batch loss: 0.665859  [652800/900000]\n",
      "current batch loss: 0.629112  [659200/900000]\n",
      "current batch loss: 0.651146  [665600/900000]\n",
      "current batch loss: 0.616154  [672000/900000]\n",
      "current batch loss: 0.592912  [678400/900000]\n",
      "current batch loss: 0.660574  [684800/900000]\n",
      "current batch loss: 0.583364  [691200/900000]\n",
      "current batch loss: 0.630454  [697600/900000]\n",
      "current batch loss: 0.657380  [704000/900000]\n",
      "current batch loss: 0.658757  [710400/900000]\n",
      "current batch loss: 0.648493  [716800/900000]\n",
      "current batch loss: 0.696951  [723200/900000]\n",
      "current batch loss: 0.584514  [729600/900000]\n",
      "current batch loss: 0.666728  [736000/900000]\n",
      "current batch loss: 0.593151  [742400/900000]\n",
      "current batch loss: 0.605325  [748800/900000]\n",
      "current batch loss: 0.641285  [755200/900000]\n",
      "current batch loss: 0.640954  [761600/900000]\n",
      "current batch loss: 0.634880  [768000/900000]\n",
      "current batch loss: 0.645918  [774400/900000]\n",
      "current batch loss: 0.701601  [780800/900000]\n",
      "current batch loss: 0.663101  [787200/900000]\n",
      "current batch loss: 0.615844  [793600/900000]\n",
      "current batch loss: 0.633693  [800000/900000]\n",
      "current batch loss: 0.617260  [806400/900000]\n",
      "current batch loss: 0.657794  [812800/900000]\n",
      "current batch loss: 0.628695  [819200/900000]\n",
      "current batch loss: 0.611063  [825600/900000]\n",
      "current batch loss: 0.633659  [832000/900000]\n",
      "current batch loss: 0.532028  [838400/900000]\n",
      "current batch loss: 0.556426  [844800/900000]\n",
      "current batch loss: 0.638785  [851200/900000]\n",
      "current batch loss: 0.689076  [857600/900000]\n",
      "current batch loss: 0.629695  [864000/900000]\n",
      "current batch loss: 0.648331  [870400/900000]\n",
      "current batch loss: 0.626019  [876800/900000]\n",
      "current batch loss: 0.571014  [883200/900000]\n",
      "current batch loss: 0.660428  [889600/900000]\n",
      "current batch loss: 0.667941  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.631280\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.630193\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 21\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.632121  [    0/900000]\n",
      "current batch loss: 0.681724  [ 6400/900000]\n",
      "current batch loss: 0.571393  [12800/900000]\n",
      "current batch loss: 0.622650  [19200/900000]\n",
      "current batch loss: 0.669688  [25600/900000]\n",
      "current batch loss: 0.631679  [32000/900000]\n",
      "current batch loss: 0.642424  [38400/900000]\n",
      "current batch loss: 0.704291  [44800/900000]\n",
      "current batch loss: 0.614326  [51200/900000]\n",
      "current batch loss: 0.633921  [57600/900000]\n",
      "current batch loss: 0.716726  [64000/900000]\n",
      "current batch loss: 0.657842  [70400/900000]\n",
      "current batch loss: 0.619336  [76800/900000]\n",
      "current batch loss: 0.659648  [83200/900000]\n",
      "current batch loss: 0.557375  [89600/900000]\n",
      "current batch loss: 0.632872  [96000/900000]\n",
      "current batch loss: 0.707418  [102400/900000]\n",
      "current batch loss: 0.650466  [108800/900000]\n",
      "current batch loss: 0.567909  [115200/900000]\n",
      "current batch loss: 0.602938  [121600/900000]\n",
      "current batch loss: 0.667431  [128000/900000]\n",
      "current batch loss: 0.617978  [134400/900000]\n",
      "current batch loss: 0.590371  [140800/900000]\n",
      "current batch loss: 0.592720  [147200/900000]\n",
      "current batch loss: 0.583899  [153600/900000]\n",
      "current batch loss: 0.616164  [160000/900000]\n",
      "current batch loss: 0.691180  [166400/900000]\n",
      "current batch loss: 0.583067  [172800/900000]\n",
      "current batch loss: 0.665357  [179200/900000]\n",
      "current batch loss: 0.599018  [185600/900000]\n",
      "current batch loss: 0.613545  [192000/900000]\n",
      "current batch loss: 0.621567  [198400/900000]\n",
      "current batch loss: 0.636883  [204800/900000]\n",
      "current batch loss: 0.664471  [211200/900000]\n",
      "current batch loss: 0.635893  [217600/900000]\n",
      "current batch loss: 0.562036  [224000/900000]\n",
      "current batch loss: 0.625109  [230400/900000]\n",
      "current batch loss: 0.555832  [236800/900000]\n",
      "current batch loss: 0.610313  [243200/900000]\n",
      "current batch loss: 0.637511  [249600/900000]\n",
      "current batch loss: 0.594010  [256000/900000]\n",
      "current batch loss: 0.655793  [262400/900000]\n",
      "current batch loss: 0.593472  [268800/900000]\n",
      "current batch loss: 0.593808  [275200/900000]\n",
      "current batch loss: 0.663986  [281600/900000]\n",
      "current batch loss: 0.679233  [288000/900000]\n",
      "current batch loss: 0.662079  [294400/900000]\n",
      "current batch loss: 0.665438  [300800/900000]\n",
      "current batch loss: 0.607225  [307200/900000]\n",
      "current batch loss: 0.631909  [313600/900000]\n",
      "current batch loss: 0.636827  [320000/900000]\n",
      "current batch loss: 0.674596  [326400/900000]\n",
      "current batch loss: 0.662389  [332800/900000]\n",
      "current batch loss: 0.596179  [339200/900000]\n",
      "current batch loss: 0.723969  [345600/900000]\n",
      "current batch loss: 0.588202  [352000/900000]\n",
      "current batch loss: 0.629353  [358400/900000]\n",
      "current batch loss: 0.613855  [364800/900000]\n",
      "current batch loss: 0.564565  [371200/900000]\n",
      "current batch loss: 0.633658  [377600/900000]\n",
      "current batch loss: 0.678949  [384000/900000]\n",
      "current batch loss: 0.623739  [390400/900000]\n",
      "current batch loss: 0.605541  [396800/900000]\n",
      "current batch loss: 0.639871  [403200/900000]\n",
      "current batch loss: 0.629561  [409600/900000]\n",
      "current batch loss: 0.637608  [416000/900000]\n",
      "current batch loss: 0.569542  [422400/900000]\n",
      "current batch loss: 0.639707  [428800/900000]\n",
      "current batch loss: 0.612978  [435200/900000]\n",
      "current batch loss: 0.585027  [441600/900000]\n",
      "current batch loss: 0.623254  [448000/900000]\n",
      "current batch loss: 0.640535  [454400/900000]\n",
      "current batch loss: 0.621939  [460800/900000]\n",
      "current batch loss: 0.651079  [467200/900000]\n",
      "current batch loss: 0.592024  [473600/900000]\n",
      "current batch loss: 0.625263  [480000/900000]\n",
      "current batch loss: 0.659495  [486400/900000]\n",
      "current batch loss: 0.612240  [492800/900000]\n",
      "current batch loss: 0.616105  [499200/900000]\n",
      "current batch loss: 0.655619  [505600/900000]\n",
      "current batch loss: 0.680939  [512000/900000]\n",
      "current batch loss: 0.587378  [518400/900000]\n",
      "current batch loss: 0.685651  [524800/900000]\n",
      "current batch loss: 0.682175  [531200/900000]\n",
      "current batch loss: 0.675885  [537600/900000]\n",
      "current batch loss: 0.571595  [544000/900000]\n",
      "current batch loss: 0.673174  [550400/900000]\n",
      "current batch loss: 0.564098  [556800/900000]\n",
      "current batch loss: 0.637232  [563200/900000]\n",
      "current batch loss: 0.622647  [569600/900000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.661607  [576000/900000]\n",
      "current batch loss: 0.632717  [582400/900000]\n",
      "current batch loss: 0.722638  [588800/900000]\n",
      "current batch loss: 0.611967  [595200/900000]\n",
      "current batch loss: 0.646322  [601600/900000]\n",
      "current batch loss: 0.634894  [608000/900000]\n",
      "current batch loss: 0.499464  [614400/900000]\n",
      "current batch loss: 0.644968  [620800/900000]\n",
      "current batch loss: 0.594893  [627200/900000]\n",
      "current batch loss: 0.708544  [633600/900000]\n",
      "current batch loss: 0.670169  [640000/900000]\n",
      "current batch loss: 0.645975  [646400/900000]\n",
      "current batch loss: 0.622390  [652800/900000]\n",
      "current batch loss: 0.574364  [659200/900000]\n",
      "current batch loss: 0.679808  [665600/900000]\n",
      "current batch loss: 0.593019  [672000/900000]\n",
      "current batch loss: 0.659826  [678400/900000]\n",
      "current batch loss: 0.582656  [684800/900000]\n",
      "current batch loss: 0.571165  [691200/900000]\n",
      "current batch loss: 0.625531  [697600/900000]\n",
      "current batch loss: 0.599168  [704000/900000]\n",
      "current batch loss: 0.618279  [710400/900000]\n",
      "current batch loss: 0.692595  [716800/900000]\n",
      "current batch loss: 0.688976  [723200/900000]\n",
      "current batch loss: 0.663047  [729600/900000]\n",
      "current batch loss: 0.723332  [736000/900000]\n",
      "current batch loss: 0.559583  [742400/900000]\n",
      "current batch loss: 0.630934  [748800/900000]\n",
      "current batch loss: 0.713953  [755200/900000]\n",
      "current batch loss: 0.652481  [761600/900000]\n",
      "current batch loss: 0.615209  [768000/900000]\n",
      "current batch loss: 0.559203  [774400/900000]\n",
      "current batch loss: 0.619910  [780800/900000]\n",
      "current batch loss: 0.550897  [787200/900000]\n",
      "current batch loss: 0.653600  [793600/900000]\n",
      "current batch loss: 0.669992  [800000/900000]\n",
      "current batch loss: 0.620475  [806400/900000]\n",
      "current batch loss: 0.620153  [812800/900000]\n",
      "current batch loss: 0.663647  [819200/900000]\n",
      "current batch loss: 0.611473  [825600/900000]\n",
      "current batch loss: 0.596011  [832000/900000]\n",
      "current batch loss: 0.605571  [838400/900000]\n",
      "current batch loss: 0.588377  [844800/900000]\n",
      "current batch loss: 0.659050  [851200/900000]\n",
      "current batch loss: 0.640761  [857600/900000]\n",
      "current batch loss: 0.577139  [864000/900000]\n",
      "current batch loss: 0.661643  [870400/900000]\n",
      "current batch loss: 0.580361  [876800/900000]\n",
      "current batch loss: 0.568635  [883200/900000]\n",
      "current batch loss: 0.685193  [889600/900000]\n",
      "current batch loss: 0.625754  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.631637\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.630774\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 22\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.621996  [    0/900000]\n",
      "current batch loss: 0.573832  [ 6400/900000]\n",
      "current batch loss: 0.670402  [12800/900000]\n",
      "current batch loss: 0.583170  [19200/900000]\n",
      "current batch loss: 0.617633  [25600/900000]\n",
      "current batch loss: 0.572488  [32000/900000]\n",
      "current batch loss: 0.638450  [38400/900000]\n",
      "current batch loss: 0.621053  [44800/900000]\n",
      "current batch loss: 0.638531  [51200/900000]\n",
      "current batch loss: 0.562607  [57600/900000]\n",
      "current batch loss: 0.623658  [64000/900000]\n",
      "current batch loss: 0.656974  [70400/900000]\n",
      "current batch loss: 0.644002  [76800/900000]\n",
      "current batch loss: 0.624452  [83200/900000]\n",
      "current batch loss: 0.604650  [89600/900000]\n",
      "current batch loss: 0.625644  [96000/900000]\n",
      "current batch loss: 0.645371  [102400/900000]\n",
      "current batch loss: 0.567376  [108800/900000]\n",
      "current batch loss: 0.629518  [115200/900000]\n",
      "current batch loss: 0.644488  [121600/900000]\n",
      "current batch loss: 0.604628  [128000/900000]\n",
      "current batch loss: 0.611336  [134400/900000]\n",
      "current batch loss: 0.646119  [140800/900000]\n",
      "current batch loss: 0.653035  [147200/900000]\n",
      "current batch loss: 0.594304  [153600/900000]\n",
      "current batch loss: 0.609658  [160000/900000]\n",
      "current batch loss: 0.672902  [166400/900000]\n",
      "current batch loss: 0.670872  [172800/900000]\n",
      "current batch loss: 0.638206  [179200/900000]\n",
      "current batch loss: 0.671714  [185600/900000]\n",
      "current batch loss: 0.620019  [192000/900000]\n",
      "current batch loss: 0.629610  [198400/900000]\n",
      "current batch loss: 0.551857  [204800/900000]\n",
      "current batch loss: 0.545824  [211200/900000]\n",
      "current batch loss: 0.604923  [217600/900000]\n",
      "current batch loss: 0.671072  [224000/900000]\n",
      "current batch loss: 0.736880  [230400/900000]\n",
      "current batch loss: 0.643733  [236800/900000]\n",
      "current batch loss: 0.656020  [243200/900000]\n",
      "current batch loss: 0.621678  [249600/900000]\n",
      "current batch loss: 0.689966  [256000/900000]\n",
      "current batch loss: 0.578913  [262400/900000]\n",
      "current batch loss: 0.660042  [268800/900000]\n",
      "current batch loss: 0.618638  [275200/900000]\n",
      "current batch loss: 0.594078  [281600/900000]\n",
      "current batch loss: 0.549217  [288000/900000]\n",
      "current batch loss: 0.617457  [294400/900000]\n",
      "current batch loss: 0.735511  [300800/900000]\n",
      "current batch loss: 0.631224  [307200/900000]\n",
      "current batch loss: 0.576595  [313600/900000]\n",
      "current batch loss: 0.588328  [320000/900000]\n",
      "current batch loss: 0.616712  [326400/900000]\n",
      "current batch loss: 0.648729  [332800/900000]\n",
      "current batch loss: 0.633314  [339200/900000]\n",
      "current batch loss: 0.701993  [345600/900000]\n",
      "current batch loss: 0.660144  [352000/900000]\n",
      "current batch loss: 0.606697  [358400/900000]\n",
      "current batch loss: 0.635033  [364800/900000]\n",
      "current batch loss: 0.581915  [371200/900000]\n",
      "current batch loss: 0.629624  [377600/900000]\n",
      "current batch loss: 0.663635  [384000/900000]\n",
      "current batch loss: 0.663099  [390400/900000]\n",
      "current batch loss: 0.573961  [396800/900000]\n",
      "current batch loss: 0.624941  [403200/900000]\n",
      "current batch loss: 0.712818  [409600/900000]\n",
      "current batch loss: 0.620932  [416000/900000]\n",
      "current batch loss: 0.624101  [422400/900000]\n",
      "current batch loss: 0.605163  [428800/900000]\n",
      "current batch loss: 0.679131  [435200/900000]\n",
      "current batch loss: 0.550306  [441600/900000]\n",
      "current batch loss: 0.565892  [448000/900000]\n",
      "current batch loss: 0.571597  [454400/900000]\n",
      "current batch loss: 0.596250  [460800/900000]\n",
      "current batch loss: 0.755510  [467200/900000]\n",
      "current batch loss: 0.625870  [473600/900000]\n",
      "current batch loss: 0.638861  [480000/900000]\n",
      "current batch loss: 0.593093  [486400/900000]\n",
      "current batch loss: 0.610093  [492800/900000]\n",
      "current batch loss: 0.743510  [499200/900000]\n",
      "current batch loss: 0.613618  [505600/900000]\n",
      "current batch loss: 0.578713  [512000/900000]\n",
      "current batch loss: 0.651074  [518400/900000]\n",
      "current batch loss: 0.583183  [524800/900000]\n",
      "current batch loss: 0.641258  [531200/900000]\n",
      "current batch loss: 0.603185  [537600/900000]\n",
      "current batch loss: 0.600016  [544000/900000]\n",
      "current batch loss: 0.681920  [550400/900000]\n",
      "current batch loss: 0.602014  [556800/900000]\n",
      "current batch loss: 0.646068  [563200/900000]\n",
      "current batch loss: 0.633688  [569600/900000]\n",
      "current batch loss: 0.599649  [576000/900000]\n",
      "current batch loss: 0.620026  [582400/900000]\n",
      "current batch loss: 0.615415  [588800/900000]\n",
      "current batch loss: 0.585318  [595200/900000]\n",
      "current batch loss: 0.607755  [601600/900000]\n",
      "current batch loss: 0.606253  [608000/900000]\n",
      "current batch loss: 0.684055  [614400/900000]\n",
      "current batch loss: 0.692902  [620800/900000]\n",
      "current batch loss: 0.624121  [627200/900000]\n",
      "current batch loss: 0.656382  [633600/900000]\n",
      "current batch loss: 0.628958  [640000/900000]\n",
      "current batch loss: 0.653755  [646400/900000]\n",
      "current batch loss: 0.645479  [652800/900000]\n",
      "current batch loss: 0.588906  [659200/900000]\n",
      "current batch loss: 0.641193  [665600/900000]\n",
      "current batch loss: 0.554995  [672000/900000]\n",
      "current batch loss: 0.665172  [678400/900000]\n",
      "current batch loss: 0.650503  [684800/900000]\n",
      "current batch loss: 0.644295  [691200/900000]\n",
      "current batch loss: 0.642456  [697600/900000]\n",
      "current batch loss: 0.690917  [704000/900000]\n",
      "current batch loss: 0.586294  [710400/900000]\n",
      "current batch loss: 0.642988  [716800/900000]\n",
      "current batch loss: 0.692736  [723200/900000]\n",
      "current batch loss: 0.585733  [729600/900000]\n",
      "current batch loss: 0.636182  [736000/900000]\n",
      "current batch loss: 0.731810  [742400/900000]\n",
      "current batch loss: 0.627398  [748800/900000]\n",
      "current batch loss: 0.661622  [755200/900000]\n",
      "current batch loss: 0.611290  [761600/900000]\n",
      "current batch loss: 0.608190  [768000/900000]\n",
      "current batch loss: 0.646544  [774400/900000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.631243  [780800/900000]\n",
      "current batch loss: 0.634216  [787200/900000]\n",
      "current batch loss: 0.601736  [793600/900000]\n",
      "current batch loss: 0.541899  [800000/900000]\n",
      "current batch loss: 0.655512  [806400/900000]\n",
      "current batch loss: 0.571577  [812800/900000]\n",
      "current batch loss: 0.667344  [819200/900000]\n",
      "current batch loss: 0.643197  [825600/900000]\n",
      "current batch loss: 0.552620  [832000/900000]\n",
      "current batch loss: 0.649474  [838400/900000]\n",
      "current batch loss: 0.630344  [844800/900000]\n",
      "current batch loss: 0.628981  [851200/900000]\n",
      "current batch loss: 0.689238  [857600/900000]\n",
      "current batch loss: 0.648414  [864000/900000]\n",
      "current batch loss: 0.620342  [870400/900000]\n",
      "current batch loss: 0.783063  [876800/900000]\n",
      "current batch loss: 0.692324  [883200/900000]\n",
      "current batch loss: 0.611911  [889600/900000]\n",
      "current batch loss: 0.637167  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.631993\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.630955\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 23\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.645454  [    0/900000]\n",
      "current batch loss: 0.612308  [ 6400/900000]\n",
      "current batch loss: 0.535344  [12800/900000]\n",
      "current batch loss: 0.712707  [19200/900000]\n",
      "current batch loss: 0.605380  [25600/900000]\n",
      "current batch loss: 0.680746  [32000/900000]\n",
      "current batch loss: 0.657873  [38400/900000]\n",
      "current batch loss: 0.594108  [44800/900000]\n",
      "current batch loss: 0.622212  [51200/900000]\n",
      "current batch loss: 0.654566  [57600/900000]\n",
      "current batch loss: 0.594370  [64000/900000]\n",
      "current batch loss: 0.600078  [70400/900000]\n",
      "current batch loss: 0.645155  [76800/900000]\n",
      "current batch loss: 0.640357  [83200/900000]\n",
      "current batch loss: 0.642590  [89600/900000]\n",
      "current batch loss: 0.619285  [96000/900000]\n",
      "current batch loss: 0.600578  [102400/900000]\n",
      "current batch loss: 0.698181  [108800/900000]\n",
      "current batch loss: 0.612456  [115200/900000]\n",
      "current batch loss: 0.626779  [121600/900000]\n",
      "current batch loss: 0.582137  [128000/900000]\n",
      "current batch loss: 0.685999  [134400/900000]\n",
      "current batch loss: 0.634001  [140800/900000]\n",
      "current batch loss: 0.615068  [147200/900000]\n",
      "current batch loss: 0.639907  [153600/900000]\n",
      "current batch loss: 0.631190  [160000/900000]\n",
      "current batch loss: 0.602876  [166400/900000]\n",
      "current batch loss: 0.655714  [172800/900000]\n",
      "current batch loss: 0.648596  [179200/900000]\n",
      "current batch loss: 0.690430  [185600/900000]\n",
      "current batch loss: 0.636816  [192000/900000]\n",
      "current batch loss: 0.627627  [198400/900000]\n",
      "current batch loss: 0.688189  [204800/900000]\n",
      "current batch loss: 0.619879  [211200/900000]\n",
      "current batch loss: 0.602936  [217600/900000]\n",
      "current batch loss: 0.625436  [224000/900000]\n",
      "current batch loss: 0.610360  [230400/900000]\n",
      "current batch loss: 0.615956  [236800/900000]\n",
      "current batch loss: 0.625546  [243200/900000]\n",
      "current batch loss: 0.581196  [249600/900000]\n",
      "current batch loss: 0.641219  [256000/900000]\n",
      "current batch loss: 0.575016  [262400/900000]\n",
      "current batch loss: 0.626974  [268800/900000]\n",
      "current batch loss: 0.638826  [275200/900000]\n",
      "current batch loss: 0.615145  [281600/900000]\n",
      "current batch loss: 0.556807  [288000/900000]\n",
      "current batch loss: 0.706970  [294400/900000]\n",
      "current batch loss: 0.632983  [300800/900000]\n",
      "current batch loss: 0.603523  [307200/900000]\n",
      "current batch loss: 0.620284  [313600/900000]\n",
      "current batch loss: 0.592763  [320000/900000]\n",
      "current batch loss: 0.636744  [326400/900000]\n",
      "current batch loss: 0.635737  [332800/900000]\n",
      "current batch loss: 0.629432  [339200/900000]\n",
      "current batch loss: 0.702901  [345600/900000]\n",
      "current batch loss: 0.721103  [352000/900000]\n",
      "current batch loss: 0.594735  [358400/900000]\n",
      "current batch loss: 0.591946  [364800/900000]\n",
      "current batch loss: 0.576315  [371200/900000]\n",
      "current batch loss: 0.672881  [377600/900000]\n",
      "current batch loss: 0.599938  [384000/900000]\n",
      "current batch loss: 0.642831  [390400/900000]\n",
      "current batch loss: 0.612273  [396800/900000]\n",
      "current batch loss: 0.614201  [403200/900000]\n",
      "current batch loss: 0.549995  [409600/900000]\n",
      "current batch loss: 0.688424  [416000/900000]\n",
      "current batch loss: 0.628677  [422400/900000]\n",
      "current batch loss: 0.553998  [428800/900000]\n",
      "current batch loss: 0.653060  [435200/900000]\n",
      "current batch loss: 0.637419  [441600/900000]\n",
      "current batch loss: 0.672179  [448000/900000]\n",
      "current batch loss: 0.663589  [454400/900000]\n",
      "current batch loss: 0.597195  [460800/900000]\n",
      "current batch loss: 0.695959  [467200/900000]\n",
      "current batch loss: 0.659121  [473600/900000]\n",
      "current batch loss: 0.638767  [480000/900000]\n",
      "current batch loss: 0.620546  [486400/900000]\n",
      "current batch loss: 0.699803  [492800/900000]\n",
      "current batch loss: 0.622127  [499200/900000]\n",
      "current batch loss: 0.626415  [505600/900000]\n",
      "current batch loss: 0.667782  [512000/900000]\n",
      "current batch loss: 0.630621  [518400/900000]\n",
      "current batch loss: 0.643101  [524800/900000]\n",
      "current batch loss: 0.620822  [531200/900000]\n",
      "current batch loss: 0.633966  [537600/900000]\n",
      "current batch loss: 0.606187  [544000/900000]\n",
      "current batch loss: 0.642355  [550400/900000]\n",
      "current batch loss: 0.644103  [556800/900000]\n",
      "current batch loss: 0.716957  [563200/900000]\n",
      "current batch loss: 0.672465  [569600/900000]\n",
      "current batch loss: 0.620525  [576000/900000]\n",
      "current batch loss: 0.599112  [582400/900000]\n",
      "current batch loss: 0.610517  [588800/900000]\n",
      "current batch loss: 0.551565  [595200/900000]\n",
      "current batch loss: 0.591139  [601600/900000]\n",
      "current batch loss: 0.620120  [608000/900000]\n",
      "current batch loss: 0.674186  [614400/900000]\n",
      "current batch loss: 0.635523  [620800/900000]\n",
      "current batch loss: 0.628057  [627200/900000]\n",
      "current batch loss: 0.598408  [633600/900000]\n",
      "current batch loss: 0.653333  [640000/900000]\n",
      "current batch loss: 0.635238  [646400/900000]\n",
      "current batch loss: 0.653417  [652800/900000]\n",
      "current batch loss: 0.561468  [659200/900000]\n",
      "current batch loss: 0.575152  [665600/900000]\n",
      "current batch loss: 0.601455  [672000/900000]\n",
      "current batch loss: 0.589565  [678400/900000]\n",
      "current batch loss: 0.650770  [684800/900000]\n",
      "current batch loss: 0.611690  [691200/900000]\n",
      "current batch loss: 0.640000  [697600/900000]\n",
      "current batch loss: 0.577997  [704000/900000]\n",
      "current batch loss: 0.625310  [710400/900000]\n",
      "current batch loss: 0.651640  [716800/900000]\n",
      "current batch loss: 0.583910  [723200/900000]\n",
      "current batch loss: 0.663673  [729600/900000]\n",
      "current batch loss: 0.616202  [736000/900000]\n",
      "current batch loss: 0.695150  [742400/900000]\n",
      "current batch loss: 0.661563  [748800/900000]\n",
      "current batch loss: 0.638004  [755200/900000]\n",
      "current batch loss: 0.641821  [761600/900000]\n",
      "current batch loss: 0.613553  [768000/900000]\n",
      "current batch loss: 0.714684  [774400/900000]\n",
      "current batch loss: 0.656787  [780800/900000]\n",
      "current batch loss: 0.593390  [787200/900000]\n",
      "current batch loss: 0.614601  [793600/900000]\n",
      "current batch loss: 0.548857  [800000/900000]\n",
      "current batch loss: 0.582210  [806400/900000]\n",
      "current batch loss: 0.593491  [812800/900000]\n",
      "current batch loss: 0.667733  [819200/900000]\n",
      "current batch loss: 0.644851  [825600/900000]\n",
      "current batch loss: 0.712974  [832000/900000]\n",
      "current batch loss: 0.635889  [838400/900000]\n",
      "current batch loss: 0.589534  [844800/900000]\n",
      "current batch loss: 0.581596  [851200/900000]\n",
      "current batch loss: 0.555581  [857600/900000]\n",
      "current batch loss: 0.650183  [864000/900000]\n",
      "current batch loss: 0.681572  [870400/900000]\n",
      "current batch loss: 0.636665  [876800/900000]\n",
      "current batch loss: 0.710242  [883200/900000]\n",
      "current batch loss: 0.613965  [889600/900000]\n",
      "current batch loss: 0.621935  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.631244\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.629885\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 24\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.617249  [    0/900000]\n",
      "current batch loss: 0.611001  [ 6400/900000]\n",
      "current batch loss: 0.659273  [12800/900000]\n",
      "current batch loss: 0.673029  [19200/900000]\n",
      "current batch loss: 0.601395  [25600/900000]\n",
      "current batch loss: 0.650119  [32000/900000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.611287  [38400/900000]\n",
      "current batch loss: 0.651728  [44800/900000]\n",
      "current batch loss: 0.672573  [51200/900000]\n",
      "current batch loss: 0.651727  [57600/900000]\n",
      "current batch loss: 0.621257  [64000/900000]\n",
      "current batch loss: 0.594222  [70400/900000]\n",
      "current batch loss: 0.596133  [76800/900000]\n",
      "current batch loss: 0.631369  [83200/900000]\n",
      "current batch loss: 0.668531  [89600/900000]\n",
      "current batch loss: 0.655423  [96000/900000]\n",
      "current batch loss: 0.657085  [102400/900000]\n",
      "current batch loss: 0.609408  [108800/900000]\n",
      "current batch loss: 0.663918  [115200/900000]\n",
      "current batch loss: 0.572935  [121600/900000]\n",
      "current batch loss: 0.682463  [128000/900000]\n",
      "current batch loss: 0.643032  [134400/900000]\n",
      "current batch loss: 0.589199  [140800/900000]\n",
      "current batch loss: 0.613070  [147200/900000]\n",
      "current batch loss: 0.658146  [153600/900000]\n",
      "current batch loss: 0.596833  [160000/900000]\n",
      "current batch loss: 0.677182  [166400/900000]\n",
      "current batch loss: 0.592216  [172800/900000]\n",
      "current batch loss: 0.622887  [179200/900000]\n",
      "current batch loss: 0.596003  [185600/900000]\n",
      "current batch loss: 0.627577  [192000/900000]\n",
      "current batch loss: 0.711669  [198400/900000]\n",
      "current batch loss: 0.632002  [204800/900000]\n",
      "current batch loss: 0.654597  [211200/900000]\n",
      "current batch loss: 0.601973  [217600/900000]\n",
      "current batch loss: 0.578219  [224000/900000]\n",
      "current batch loss: 0.679083  [230400/900000]\n",
      "current batch loss: 0.601200  [236800/900000]\n",
      "current batch loss: 0.644955  [243200/900000]\n",
      "current batch loss: 0.585466  [249600/900000]\n",
      "current batch loss: 0.601831  [256000/900000]\n",
      "current batch loss: 0.658226  [262400/900000]\n",
      "current batch loss: 0.687349  [268800/900000]\n",
      "current batch loss: 0.617444  [275200/900000]\n",
      "current batch loss: 0.598284  [281600/900000]\n",
      "current batch loss: 0.581347  [288000/900000]\n",
      "current batch loss: 0.673900  [294400/900000]\n",
      "current batch loss: 0.587854  [300800/900000]\n",
      "current batch loss: 0.595215  [307200/900000]\n",
      "current batch loss: 0.604412  [313600/900000]\n",
      "current batch loss: 0.608860  [320000/900000]\n",
      "current batch loss: 0.661938  [326400/900000]\n",
      "current batch loss: 0.611594  [332800/900000]\n",
      "current batch loss: 0.603751  [339200/900000]\n",
      "current batch loss: 0.649641  [345600/900000]\n",
      "current batch loss: 0.658647  [352000/900000]\n",
      "current batch loss: 0.682962  [358400/900000]\n",
      "current batch loss: 0.640248  [364800/900000]\n",
      "current batch loss: 0.617292  [371200/900000]\n",
      "current batch loss: 0.592261  [377600/900000]\n",
      "current batch loss: 0.599345  [384000/900000]\n",
      "current batch loss: 0.653407  [390400/900000]\n",
      "current batch loss: 0.769915  [396800/900000]\n",
      "current batch loss: 0.625200  [403200/900000]\n",
      "current batch loss: 0.653643  [409600/900000]\n",
      "current batch loss: 0.613461  [416000/900000]\n",
      "current batch loss: 0.640794  [422400/900000]\n",
      "current batch loss: 0.653542  [428800/900000]\n",
      "current batch loss: 0.580198  [435200/900000]\n",
      "current batch loss: 0.616983  [441600/900000]\n",
      "current batch loss: 0.621073  [448000/900000]\n",
      "current batch loss: 0.611940  [454400/900000]\n",
      "current batch loss: 0.697297  [460800/900000]\n",
      "current batch loss: 0.668395  [467200/900000]\n",
      "current batch loss: 0.593655  [473600/900000]\n",
      "current batch loss: 0.690976  [480000/900000]\n",
      "current batch loss: 0.683066  [486400/900000]\n",
      "current batch loss: 0.586560  [492800/900000]\n",
      "current batch loss: 0.664869  [499200/900000]\n",
      "current batch loss: 0.633100  [505600/900000]\n",
      "current batch loss: 0.595535  [512000/900000]\n",
      "current batch loss: 0.600113  [518400/900000]\n",
      "current batch loss: 0.646996  [524800/900000]\n",
      "current batch loss: 0.612792  [531200/900000]\n",
      "current batch loss: 0.570701  [537600/900000]\n",
      "current batch loss: 0.621987  [544000/900000]\n",
      "current batch loss: 0.629274  [550400/900000]\n",
      "current batch loss: 0.672484  [556800/900000]\n",
      "current batch loss: 0.602802  [563200/900000]\n",
      "current batch loss: 0.640583  [569600/900000]\n",
      "current batch loss: 0.601982  [576000/900000]\n",
      "current batch loss: 0.660015  [582400/900000]\n",
      "current batch loss: 0.601873  [588800/900000]\n",
      "current batch loss: 0.572924  [595200/900000]\n",
      "current batch loss: 0.663689  [601600/900000]\n",
      "current batch loss: 0.639046  [608000/900000]\n",
      "current batch loss: 0.609780  [614400/900000]\n",
      "current batch loss: 0.583337  [620800/900000]\n",
      "current batch loss: 0.670960  [627200/900000]\n",
      "current batch loss: 0.641955  [633600/900000]\n",
      "current batch loss: 0.700170  [640000/900000]\n",
      "current batch loss: 0.664907  [646400/900000]\n",
      "current batch loss: 0.617910  [652800/900000]\n",
      "current batch loss: 0.696572  [659200/900000]\n",
      "current batch loss: 0.645184  [665600/900000]\n",
      "current batch loss: 0.630486  [672000/900000]\n",
      "current batch loss: 0.662511  [678400/900000]\n",
      "current batch loss: 0.602006  [684800/900000]\n",
      "current batch loss: 0.626504  [691200/900000]\n",
      "current batch loss: 0.588722  [697600/900000]\n",
      "current batch loss: 0.637978  [704000/900000]\n",
      "current batch loss: 0.610554  [710400/900000]\n",
      "current batch loss: 0.625187  [716800/900000]\n",
      "current batch loss: 0.664152  [723200/900000]\n",
      "current batch loss: 0.658031  [729600/900000]\n",
      "current batch loss: 0.623032  [736000/900000]\n",
      "current batch loss: 0.702604  [742400/900000]\n",
      "current batch loss: 0.614540  [748800/900000]\n",
      "current batch loss: 0.645234  [755200/900000]\n",
      "current batch loss: 0.580145  [761600/900000]\n",
      "current batch loss: 0.665443  [768000/900000]\n",
      "current batch loss: 0.596550  [774400/900000]\n",
      "current batch loss: 0.600945  [780800/900000]\n",
      "current batch loss: 0.673743  [787200/900000]\n",
      "current batch loss: 0.671333  [793600/900000]\n",
      "current batch loss: 0.661823  [800000/900000]\n",
      "current batch loss: 0.617390  [806400/900000]\n",
      "current batch loss: 0.605437  [812800/900000]\n",
      "current batch loss: 0.682275  [819200/900000]\n",
      "current batch loss: 0.667413  [825600/900000]\n",
      "current batch loss: 0.669605  [832000/900000]\n",
      "current batch loss: 0.660574  [838400/900000]\n",
      "current batch loss: 0.630960  [844800/900000]\n",
      "current batch loss: 0.613941  [851200/900000]\n",
      "current batch loss: 0.621576  [857600/900000]\n",
      "current batch loss: 0.652103  [864000/900000]\n",
      "current batch loss: 0.557298  [870400/900000]\n",
      "current batch loss: 0.624376  [876800/900000]\n",
      "current batch loss: 0.588525  [883200/900000]\n",
      "current batch loss: 0.692881  [889600/900000]\n",
      "current batch loss: 0.753791  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.630361\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.629209\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 25\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.668257  [    0/900000]\n",
      "current batch loss: 0.641031  [ 6400/900000]\n",
      "current batch loss: 0.654310  [12800/900000]\n",
      "current batch loss: 0.654630  [19200/900000]\n",
      "current batch loss: 0.697267  [25600/900000]\n",
      "current batch loss: 0.622190  [32000/900000]\n",
      "current batch loss: 0.608597  [38400/900000]\n",
      "current batch loss: 0.605389  [44800/900000]\n",
      "current batch loss: 0.604740  [51200/900000]\n",
      "current batch loss: 0.697433  [57600/900000]\n",
      "current batch loss: 0.647675  [64000/900000]\n",
      "current batch loss: 0.631555  [70400/900000]\n",
      "current batch loss: 0.620154  [76800/900000]\n",
      "current batch loss: 0.622295  [83200/900000]\n",
      "current batch loss: 0.646791  [89600/900000]\n",
      "current batch loss: 0.568642  [96000/900000]\n",
      "current batch loss: 0.603243  [102400/900000]\n",
      "current batch loss: 0.641545  [108800/900000]\n",
      "current batch loss: 0.654703  [115200/900000]\n",
      "current batch loss: 0.646505  [121600/900000]\n",
      "current batch loss: 0.648321  [128000/900000]\n",
      "current batch loss: 0.626233  [134400/900000]\n",
      "current batch loss: 0.509482  [140800/900000]\n",
      "current batch loss: 0.653621  [147200/900000]\n",
      "current batch loss: 0.625260  [153600/900000]\n",
      "current batch loss: 0.636266  [160000/900000]\n",
      "current batch loss: 0.592791  [166400/900000]\n",
      "current batch loss: 0.706937  [172800/900000]\n",
      "current batch loss: 0.588203  [179200/900000]\n",
      "current batch loss: 0.564001  [185600/900000]\n",
      "current batch loss: 0.616332  [192000/900000]\n",
      "current batch loss: 0.642593  [198400/900000]\n",
      "current batch loss: 0.616418  [204800/900000]\n",
      "current batch loss: 0.645693  [211200/900000]\n",
      "current batch loss: 0.663553  [217600/900000]\n",
      "current batch loss: 0.640887  [224000/900000]\n",
      "current batch loss: 0.627829  [230400/900000]\n",
      "current batch loss: 0.610857  [236800/900000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.639117  [243200/900000]\n",
      "current batch loss: 0.720315  [249600/900000]\n",
      "current batch loss: 0.583358  [256000/900000]\n",
      "current batch loss: 0.586240  [262400/900000]\n",
      "current batch loss: 0.604728  [268800/900000]\n",
      "current batch loss: 0.664438  [275200/900000]\n",
      "current batch loss: 0.596487  [281600/900000]\n",
      "current batch loss: 0.613651  [288000/900000]\n",
      "current batch loss: 0.654546  [294400/900000]\n",
      "current batch loss: 0.584001  [300800/900000]\n",
      "current batch loss: 0.597958  [307200/900000]\n",
      "current batch loss: 0.587742  [313600/900000]\n",
      "current batch loss: 0.706392  [320000/900000]\n",
      "current batch loss: 0.663559  [326400/900000]\n",
      "current batch loss: 0.689083  [332800/900000]\n",
      "current batch loss: 0.687571  [339200/900000]\n",
      "current batch loss: 0.684750  [345600/900000]\n",
      "current batch loss: 0.645408  [352000/900000]\n",
      "current batch loss: 0.657532  [358400/900000]\n",
      "current batch loss: 0.650995  [364800/900000]\n",
      "current batch loss: 0.617005  [371200/900000]\n",
      "current batch loss: 0.587829  [377600/900000]\n",
      "current batch loss: 0.629865  [384000/900000]\n",
      "current batch loss: 0.663560  [390400/900000]\n",
      "current batch loss: 0.595657  [396800/900000]\n",
      "current batch loss: 0.627724  [403200/900000]\n",
      "current batch loss: 0.698142  [409600/900000]\n",
      "current batch loss: 0.609862  [416000/900000]\n",
      "current batch loss: 0.646777  [422400/900000]\n",
      "current batch loss: 0.555373  [428800/900000]\n",
      "current batch loss: 0.598127  [435200/900000]\n",
      "current batch loss: 0.685028  [441600/900000]\n",
      "current batch loss: 0.587340  [448000/900000]\n",
      "current batch loss: 0.623753  [454400/900000]\n",
      "current batch loss: 0.690482  [460800/900000]\n",
      "current batch loss: 0.644407  [467200/900000]\n",
      "current batch loss: 0.630338  [473600/900000]\n",
      "current batch loss: 0.597418  [480000/900000]\n",
      "current batch loss: 0.570308  [486400/900000]\n",
      "current batch loss: 0.654376  [492800/900000]\n",
      "current batch loss: 0.612915  [499200/900000]\n",
      "current batch loss: 0.624681  [505600/900000]\n",
      "current batch loss: 0.625161  [512000/900000]\n",
      "current batch loss: 0.661379  [518400/900000]\n",
      "current batch loss: 0.602759  [524800/900000]\n",
      "current batch loss: 0.604106  [531200/900000]\n",
      "current batch loss: 0.676067  [537600/900000]\n",
      "current batch loss: 0.639870  [544000/900000]\n",
      "current batch loss: 0.596590  [550400/900000]\n",
      "current batch loss: 0.707954  [556800/900000]\n",
      "current batch loss: 0.573390  [563200/900000]\n",
      "current batch loss: 0.623677  [569600/900000]\n",
      "current batch loss: 0.616617  [576000/900000]\n",
      "current batch loss: 0.501624  [582400/900000]\n",
      "current batch loss: 0.595699  [588800/900000]\n",
      "current batch loss: 0.568854  [595200/900000]\n",
      "current batch loss: 0.632256  [601600/900000]\n",
      "current batch loss: 0.663777  [608000/900000]\n",
      "current batch loss: 0.653825  [614400/900000]\n",
      "current batch loss: 0.605637  [620800/900000]\n",
      "current batch loss: 0.611792  [627200/900000]\n",
      "current batch loss: 0.580295  [633600/900000]\n",
      "current batch loss: 0.655827  [640000/900000]\n",
      "current batch loss: 0.659658  [646400/900000]\n",
      "current batch loss: 0.630639  [652800/900000]\n",
      "current batch loss: 0.623750  [659200/900000]\n",
      "current batch loss: 0.560729  [665600/900000]\n",
      "current batch loss: 0.610507  [672000/900000]\n",
      "current batch loss: 0.628691  [678400/900000]\n",
      "current batch loss: 0.647074  [684800/900000]\n",
      "current batch loss: 0.607840  [691200/900000]\n",
      "current batch loss: 0.617246  [697600/900000]\n",
      "current batch loss: 0.715400  [704000/900000]\n",
      "current batch loss: 0.612864  [710400/900000]\n",
      "current batch loss: 0.546854  [716800/900000]\n",
      "current batch loss: 0.662019  [723200/900000]\n",
      "current batch loss: 0.601160  [729600/900000]\n",
      "current batch loss: 0.557915  [736000/900000]\n",
      "current batch loss: 0.652284  [742400/900000]\n",
      "current batch loss: 0.649570  [748800/900000]\n",
      "current batch loss: 0.607561  [755200/900000]\n",
      "current batch loss: 0.593153  [761600/900000]\n",
      "current batch loss: 0.599696  [768000/900000]\n",
      "current batch loss: 0.619915  [774400/900000]\n",
      "current batch loss: 0.637110  [780800/900000]\n",
      "current batch loss: 0.615504  [787200/900000]\n",
      "current batch loss: 0.620682  [793600/900000]\n",
      "current batch loss: 0.629624  [800000/900000]\n",
      "current batch loss: 0.724875  [806400/900000]\n",
      "current batch loss: 0.617647  [812800/900000]\n",
      "current batch loss: 0.597703  [819200/900000]\n",
      "current batch loss: 0.623571  [825600/900000]\n",
      "current batch loss: 0.660763  [832000/900000]\n",
      "current batch loss: 0.631876  [838400/900000]\n",
      "current batch loss: 0.628350  [844800/900000]\n",
      "current batch loss: 0.720011  [851200/900000]\n",
      "current batch loss: 0.583657  [857600/900000]\n",
      "current batch loss: 0.642943  [864000/900000]\n",
      "current batch loss: 0.608043  [870400/900000]\n",
      "current batch loss: 0.655464  [876800/900000]\n",
      "current batch loss: 0.654574  [883200/900000]\n",
      "current batch loss: 0.618325  [889600/900000]\n",
      "current batch loss: 0.627790  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.630474\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.629327\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 26\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.593373  [    0/900000]\n",
      "current batch loss: 0.700613  [ 6400/900000]\n",
      "current batch loss: 0.653695  [12800/900000]\n",
      "current batch loss: 0.633170  [19200/900000]\n",
      "current batch loss: 0.588547  [25600/900000]\n",
      "current batch loss: 0.640849  [32000/900000]\n",
      "current batch loss: 0.651233  [38400/900000]\n",
      "current batch loss: 0.579537  [44800/900000]\n",
      "current batch loss: 0.628281  [51200/900000]\n",
      "current batch loss: 0.593840  [57600/900000]\n",
      "current batch loss: 0.613694  [64000/900000]\n",
      "current batch loss: 0.632554  [70400/900000]\n",
      "current batch loss: 0.613235  [76800/900000]\n",
      "current batch loss: 0.684192  [83200/900000]\n",
      "current batch loss: 0.595180  [89600/900000]\n",
      "current batch loss: 0.700398  [96000/900000]\n",
      "current batch loss: 0.591642  [102400/900000]\n",
      "current batch loss: 0.629189  [108800/900000]\n",
      "current batch loss: 0.643511  [115200/900000]\n",
      "current batch loss: 0.636990  [121600/900000]\n",
      "current batch loss: 0.600565  [128000/900000]\n",
      "current batch loss: 0.617281  [134400/900000]\n",
      "current batch loss: 0.628596  [140800/900000]\n",
      "current batch loss: 0.635838  [147200/900000]\n",
      "current batch loss: 0.645505  [153600/900000]\n",
      "current batch loss: 0.642358  [160000/900000]\n",
      "current batch loss: 0.669486  [166400/900000]\n",
      "current batch loss: 0.547923  [172800/900000]\n",
      "current batch loss: 0.608005  [179200/900000]\n",
      "current batch loss: 0.643344  [185600/900000]\n",
      "current batch loss: 0.641998  [192000/900000]\n",
      "current batch loss: 0.646145  [198400/900000]\n",
      "current batch loss: 0.630393  [204800/900000]\n",
      "current batch loss: 0.659636  [211200/900000]\n",
      "current batch loss: 0.521873  [217600/900000]\n",
      "current batch loss: 0.657632  [224000/900000]\n",
      "current batch loss: 0.557708  [230400/900000]\n",
      "current batch loss: 0.657821  [236800/900000]\n",
      "current batch loss: 0.656928  [243200/900000]\n",
      "current batch loss: 0.643921  [249600/900000]\n",
      "current batch loss: 0.702036  [256000/900000]\n",
      "current batch loss: 0.618292  [262400/900000]\n",
      "current batch loss: 0.632011  [268800/900000]\n",
      "current batch loss: 0.662496  [275200/900000]\n",
      "current batch loss: 0.576672  [281600/900000]\n",
      "current batch loss: 0.608446  [288000/900000]\n",
      "current batch loss: 0.649975  [294400/900000]\n",
      "current batch loss: 0.647040  [300800/900000]\n",
      "current batch loss: 0.705631  [307200/900000]\n",
      "current batch loss: 0.622078  [313600/900000]\n",
      "current batch loss: 0.630811  [320000/900000]\n",
      "current batch loss: 0.651472  [326400/900000]\n",
      "current batch loss: 0.574366  [332800/900000]\n",
      "current batch loss: 0.501557  [339200/900000]\n",
      "current batch loss: 0.590625  [345600/900000]\n",
      "current batch loss: 0.654863  [352000/900000]\n",
      "current batch loss: 0.559838  [358400/900000]\n",
      "current batch loss: 0.628179  [364800/900000]\n",
      "current batch loss: 0.619230  [371200/900000]\n",
      "current batch loss: 0.656895  [377600/900000]\n",
      "current batch loss: 0.595461  [384000/900000]\n",
      "current batch loss: 0.645567  [390400/900000]\n",
      "current batch loss: 0.621199  [396800/900000]\n",
      "current batch loss: 0.645570  [403200/900000]\n",
      "current batch loss: 0.627327  [409600/900000]\n",
      "current batch loss: 0.595407  [416000/900000]\n",
      "current batch loss: 0.635881  [422400/900000]\n",
      "current batch loss: 0.642980  [428800/900000]\n",
      "current batch loss: 0.686028  [435200/900000]\n",
      "current batch loss: 0.654621  [441600/900000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.640012  [448000/900000]\n",
      "current batch loss: 0.642343  [454400/900000]\n",
      "current batch loss: 0.657604  [460800/900000]\n",
      "current batch loss: 0.589767  [467200/900000]\n",
      "current batch loss: 0.602117  [473600/900000]\n",
      "current batch loss: 0.626478  [480000/900000]\n",
      "current batch loss: 0.676841  [486400/900000]\n",
      "current batch loss: 0.643022  [492800/900000]\n",
      "current batch loss: 0.618980  [499200/900000]\n",
      "current batch loss: 0.635148  [505600/900000]\n",
      "current batch loss: 0.624733  [512000/900000]\n",
      "current batch loss: 0.618147  [518400/900000]\n",
      "current batch loss: 0.616696  [524800/900000]\n",
      "current batch loss: 0.615428  [531200/900000]\n",
      "current batch loss: 0.605178  [537600/900000]\n",
      "current batch loss: 0.623248  [544000/900000]\n",
      "current batch loss: 0.587145  [550400/900000]\n",
      "current batch loss: 0.692411  [556800/900000]\n",
      "current batch loss: 0.575623  [563200/900000]\n",
      "current batch loss: 0.598554  [569600/900000]\n",
      "current batch loss: 0.607011  [576000/900000]\n",
      "current batch loss: 0.628699  [582400/900000]\n",
      "current batch loss: 0.619699  [588800/900000]\n",
      "current batch loss: 0.616548  [595200/900000]\n",
      "current batch loss: 0.670972  [601600/900000]\n",
      "current batch loss: 0.629575  [608000/900000]\n",
      "current batch loss: 0.646399  [614400/900000]\n",
      "current batch loss: 0.643660  [620800/900000]\n",
      "current batch loss: 0.617215  [627200/900000]\n",
      "current batch loss: 0.623633  [633600/900000]\n",
      "current batch loss: 0.685133  [640000/900000]\n",
      "current batch loss: 0.666174  [646400/900000]\n",
      "current batch loss: 0.647389  [652800/900000]\n",
      "current batch loss: 0.603421  [659200/900000]\n",
      "current batch loss: 0.613378  [665600/900000]\n",
      "current batch loss: 0.599331  [672000/900000]\n",
      "current batch loss: 0.620595  [678400/900000]\n",
      "current batch loss: 0.598515  [684800/900000]\n",
      "current batch loss: 0.580291  [691200/900000]\n",
      "current batch loss: 0.627028  [697600/900000]\n",
      "current batch loss: 0.638735  [704000/900000]\n",
      "current batch loss: 0.651992  [710400/900000]\n",
      "current batch loss: 0.632324  [716800/900000]\n",
      "current batch loss: 0.679610  [723200/900000]\n",
      "current batch loss: 0.614881  [729600/900000]\n",
      "current batch loss: 0.624981  [736000/900000]\n",
      "current batch loss: 0.680236  [742400/900000]\n",
      "current batch loss: 0.585137  [748800/900000]\n",
      "current batch loss: 0.613436  [755200/900000]\n",
      "current batch loss: 0.586508  [761600/900000]\n",
      "current batch loss: 0.597980  [768000/900000]\n",
      "current batch loss: 0.635575  [774400/900000]\n",
      "current batch loss: 0.584015  [780800/900000]\n",
      "current batch loss: 0.613489  [787200/900000]\n",
      "current batch loss: 0.614138  [793600/900000]\n",
      "current batch loss: 0.664897  [800000/900000]\n",
      "current batch loss: 0.669475  [806400/900000]\n",
      "current batch loss: 0.657322  [812800/900000]\n",
      "current batch loss: 0.591928  [819200/900000]\n",
      "current batch loss: 0.601421  [825600/900000]\n",
      "current batch loss: 0.675001  [832000/900000]\n",
      "current batch loss: 0.538105  [838400/900000]\n",
      "current batch loss: 0.651338  [844800/900000]\n",
      "current batch loss: 0.632550  [851200/900000]\n",
      "current batch loss: 0.616465  [857600/900000]\n",
      "current batch loss: 0.626527  [864000/900000]\n",
      "current batch loss: 0.644929  [870400/900000]\n",
      "current batch loss: 0.582222  [876800/900000]\n",
      "current batch loss: 0.685333  [883200/900000]\n",
      "current batch loss: 0.610714  [889600/900000]\n",
      "current batch loss: 0.640530  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.630972\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.630039\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 27\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.649520  [    0/900000]\n",
      "current batch loss: 0.721135  [ 6400/900000]\n",
      "current batch loss: 0.588424  [12800/900000]\n",
      "current batch loss: 0.629661  [19200/900000]\n",
      "current batch loss: 0.594896  [25600/900000]\n",
      "current batch loss: 0.581207  [32000/900000]\n",
      "current batch loss: 0.676208  [38400/900000]\n",
      "current batch loss: 0.544585  [44800/900000]\n",
      "current batch loss: 0.622681  [51200/900000]\n",
      "current batch loss: 0.637270  [57600/900000]\n",
      "current batch loss: 0.638209  [64000/900000]\n",
      "current batch loss: 0.609980  [70400/900000]\n",
      "current batch loss: 0.668905  [76800/900000]\n",
      "current batch loss: 0.632452  [83200/900000]\n",
      "current batch loss: 0.616397  [89600/900000]\n",
      "current batch loss: 0.631370  [96000/900000]\n",
      "current batch loss: 0.640195  [102400/900000]\n",
      "current batch loss: 0.615634  [108800/900000]\n",
      "current batch loss: 0.552477  [115200/900000]\n",
      "current batch loss: 0.669145  [121600/900000]\n",
      "current batch loss: 0.647711  [128000/900000]\n",
      "current batch loss: 0.704550  [134400/900000]\n",
      "current batch loss: 0.639330  [140800/900000]\n",
      "current batch loss: 0.611122  [147200/900000]\n",
      "current batch loss: 0.593497  [153600/900000]\n",
      "current batch loss: 0.622157  [160000/900000]\n",
      "current batch loss: 0.697158  [166400/900000]\n",
      "current batch loss: 0.645242  [172800/900000]\n",
      "current batch loss: 0.587181  [179200/900000]\n",
      "current batch loss: 0.586046  [185600/900000]\n",
      "current batch loss: 0.575116  [192000/900000]\n",
      "current batch loss: 0.698094  [198400/900000]\n",
      "current batch loss: 0.631811  [204800/900000]\n",
      "current batch loss: 0.597069  [211200/900000]\n",
      "current batch loss: 0.647913  [217600/900000]\n",
      "current batch loss: 0.666505  [224000/900000]\n",
      "current batch loss: 0.658581  [230400/900000]\n",
      "current batch loss: 0.619267  [236800/900000]\n",
      "current batch loss: 0.611563  [243200/900000]\n",
      "current batch loss: 0.640593  [249600/900000]\n",
      "current batch loss: 0.614650  [256000/900000]\n",
      "current batch loss: 0.638434  [262400/900000]\n",
      "current batch loss: 0.619858  [268800/900000]\n",
      "current batch loss: 0.517190  [275200/900000]\n",
      "current batch loss: 0.610147  [281600/900000]\n",
      "current batch loss: 0.619053  [288000/900000]\n",
      "current batch loss: 0.695239  [294400/900000]\n",
      "current batch loss: 0.564163  [300800/900000]\n",
      "current batch loss: 0.655892  [307200/900000]\n",
      "current batch loss: 0.551153  [313600/900000]\n",
      "current batch loss: 0.574834  [320000/900000]\n",
      "current batch loss: 0.706236  [326400/900000]\n",
      "current batch loss: 0.569102  [332800/900000]\n",
      "current batch loss: 0.618013  [339200/900000]\n",
      "current batch loss: 0.698134  [345600/900000]\n",
      "current batch loss: 0.707527  [352000/900000]\n",
      "current batch loss: 0.694557  [358400/900000]\n",
      "current batch loss: 0.640349  [364800/900000]\n",
      "current batch loss: 0.630645  [371200/900000]\n",
      "current batch loss: 0.642582  [377600/900000]\n",
      "current batch loss: 0.600550  [384000/900000]\n",
      "current batch loss: 0.623355  [390400/900000]\n",
      "current batch loss: 0.643432  [396800/900000]\n",
      "current batch loss: 0.728617  [403200/900000]\n",
      "current batch loss: 0.585084  [409600/900000]\n",
      "current batch loss: 0.597750  [416000/900000]\n",
      "current batch loss: 0.659484  [422400/900000]\n",
      "current batch loss: 0.699624  [428800/900000]\n",
      "current batch loss: 0.598374  [435200/900000]\n",
      "current batch loss: 0.644631  [441600/900000]\n",
      "current batch loss: 0.627316  [448000/900000]\n",
      "current batch loss: 0.604070  [454400/900000]\n",
      "current batch loss: 0.574109  [460800/900000]\n",
      "current batch loss: 0.614012  [467200/900000]\n",
      "current batch loss: 0.622890  [473600/900000]\n",
      "current batch loss: 0.619323  [480000/900000]\n",
      "current batch loss: 0.700919  [486400/900000]\n",
      "current batch loss: 0.653828  [492800/900000]\n",
      "current batch loss: 0.648007  [499200/900000]\n",
      "current batch loss: 0.648332  [505600/900000]\n",
      "current batch loss: 0.626034  [512000/900000]\n",
      "current batch loss: 0.593134  [518400/900000]\n",
      "current batch loss: 0.560818  [524800/900000]\n",
      "current batch loss: 0.567495  [531200/900000]\n",
      "current batch loss: 0.663147  [537600/900000]\n",
      "current batch loss: 0.643915  [544000/900000]\n",
      "current batch loss: 0.700613  [550400/900000]\n",
      "current batch loss: 0.688609  [556800/900000]\n",
      "current batch loss: 0.620261  [563200/900000]\n",
      "current batch loss: 0.611923  [569600/900000]\n",
      "current batch loss: 0.655181  [576000/900000]\n",
      "current batch loss: 0.693077  [582400/900000]\n",
      "current batch loss: 0.673535  [588800/900000]\n",
      "current batch loss: 0.615419  [595200/900000]\n",
      "current batch loss: 0.556502  [601600/900000]\n",
      "current batch loss: 0.611503  [608000/900000]\n",
      "current batch loss: 0.701561  [614400/900000]\n",
      "current batch loss: 0.629383  [620800/900000]\n",
      "current batch loss: 0.671332  [627200/900000]\n",
      "current batch loss: 0.633467  [633600/900000]\n",
      "current batch loss: 0.649201  [640000/900000]\n",
      "current batch loss: 0.620881  [646400/900000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.704465  [652800/900000]\n",
      "current batch loss: 0.629604  [659200/900000]\n",
      "current batch loss: 0.585451  [665600/900000]\n",
      "current batch loss: 0.647751  [672000/900000]\n",
      "current batch loss: 0.677868  [678400/900000]\n",
      "current batch loss: 0.659379  [684800/900000]\n",
      "current batch loss: 0.632982  [691200/900000]\n",
      "current batch loss: 0.582798  [697600/900000]\n",
      "current batch loss: 0.624244  [704000/900000]\n",
      "current batch loss: 0.618523  [710400/900000]\n",
      "current batch loss: 0.655397  [716800/900000]\n",
      "current batch loss: 0.552493  [723200/900000]\n",
      "current batch loss: 0.587203  [729600/900000]\n",
      "current batch loss: 0.649939  [736000/900000]\n",
      "current batch loss: 0.658309  [742400/900000]\n",
      "current batch loss: 0.605154  [748800/900000]\n",
      "current batch loss: 0.607528  [755200/900000]\n",
      "current batch loss: 0.654608  [761600/900000]\n",
      "current batch loss: 0.622769  [768000/900000]\n",
      "current batch loss: 0.666897  [774400/900000]\n",
      "current batch loss: 0.625817  [780800/900000]\n",
      "current batch loss: 0.597693  [787200/900000]\n",
      "current batch loss: 0.640142  [793600/900000]\n",
      "current batch loss: 0.645346  [800000/900000]\n",
      "current batch loss: 0.611434  [806400/900000]\n",
      "current batch loss: 0.637740  [812800/900000]\n",
      "current batch loss: 0.650756  [819200/900000]\n",
      "current batch loss: 0.576396  [825600/900000]\n",
      "current batch loss: 0.588791  [832000/900000]\n",
      "current batch loss: 0.716559  [838400/900000]\n",
      "current batch loss: 0.622357  [844800/900000]\n",
      "current batch loss: 0.680605  [851200/900000]\n",
      "current batch loss: 0.621806  [857600/900000]\n",
      "current batch loss: 0.615911  [864000/900000]\n",
      "current batch loss: 0.682830  [870400/900000]\n",
      "current batch loss: 0.642445  [876800/900000]\n",
      "current batch loss: 0.656412  [883200/900000]\n",
      "current batch loss: 0.645460  [889600/900000]\n",
      "current batch loss: 0.541827  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.631208\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.629829\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 28\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.686061  [    0/900000]\n",
      "current batch loss: 0.669084  [ 6400/900000]\n",
      "current batch loss: 0.718281  [12800/900000]\n",
      "current batch loss: 0.651599  [19200/900000]\n",
      "current batch loss: 0.686773  [25600/900000]\n",
      "current batch loss: 0.668917  [32000/900000]\n",
      "current batch loss: 0.607728  [38400/900000]\n",
      "current batch loss: 0.612470  [44800/900000]\n",
      "current batch loss: 0.667863  [51200/900000]\n",
      "current batch loss: 0.637925  [57600/900000]\n",
      "current batch loss: 0.586253  [64000/900000]\n",
      "current batch loss: 0.651644  [70400/900000]\n",
      "current batch loss: 0.535276  [76800/900000]\n",
      "current batch loss: 0.656110  [83200/900000]\n",
      "current batch loss: 0.679898  [89600/900000]\n",
      "current batch loss: 0.642651  [96000/900000]\n",
      "current batch loss: 0.665970  [102400/900000]\n",
      "current batch loss: 0.668550  [108800/900000]\n",
      "current batch loss: 0.590041  [115200/900000]\n",
      "current batch loss: 0.620645  [121600/900000]\n",
      "current batch loss: 0.576628  [128000/900000]\n",
      "current batch loss: 0.654012  [134400/900000]\n",
      "current batch loss: 0.620930  [140800/900000]\n",
      "current batch loss: 0.650111  [147200/900000]\n",
      "current batch loss: 0.610233  [153600/900000]\n",
      "current batch loss: 0.606679  [160000/900000]\n",
      "current batch loss: 0.598062  [166400/900000]\n",
      "current batch loss: 0.562418  [172800/900000]\n",
      "current batch loss: 0.668963  [179200/900000]\n",
      "current batch loss: 0.574250  [185600/900000]\n",
      "current batch loss: 0.600717  [192000/900000]\n",
      "current batch loss: 0.623395  [198400/900000]\n",
      "current batch loss: 0.659299  [204800/900000]\n",
      "current batch loss: 0.609574  [211200/900000]\n",
      "current batch loss: 0.662855  [217600/900000]\n",
      "current batch loss: 0.748163  [224000/900000]\n",
      "current batch loss: 0.712658  [230400/900000]\n",
      "current batch loss: 0.684146  [236800/900000]\n",
      "current batch loss: 0.619738  [243200/900000]\n",
      "current batch loss: 0.586015  [249600/900000]\n",
      "current batch loss: 0.655067  [256000/900000]\n",
      "current batch loss: 0.644327  [262400/900000]\n",
      "current batch loss: 0.594575  [268800/900000]\n",
      "current batch loss: 0.705350  [275200/900000]\n",
      "current batch loss: 0.684488  [281600/900000]\n",
      "current batch loss: 0.609221  [288000/900000]\n",
      "current batch loss: 0.694152  [294400/900000]\n",
      "current batch loss: 0.637718  [300800/900000]\n",
      "current batch loss: 0.606299  [307200/900000]\n",
      "current batch loss: 0.701358  [313600/900000]\n",
      "current batch loss: 0.638177  [320000/900000]\n",
      "current batch loss: 0.631557  [326400/900000]\n",
      "current batch loss: 0.688575  [332800/900000]\n",
      "current batch loss: 0.633187  [339200/900000]\n",
      "current batch loss: 0.591883  [345600/900000]\n",
      "current batch loss: 0.688646  [352000/900000]\n",
      "current batch loss: 0.624742  [358400/900000]\n",
      "current batch loss: 0.645332  [364800/900000]\n",
      "current batch loss: 0.620149  [371200/900000]\n",
      "current batch loss: 0.609187  [377600/900000]\n",
      "current batch loss: 0.641617  [384000/900000]\n",
      "current batch loss: 0.586575  [390400/900000]\n",
      "current batch loss: 0.560708  [396800/900000]\n",
      "current batch loss: 0.667037  [403200/900000]\n",
      "current batch loss: 0.624688  [409600/900000]\n",
      "current batch loss: 0.563062  [416000/900000]\n",
      "current batch loss: 0.637224  [422400/900000]\n",
      "current batch loss: 0.574161  [428800/900000]\n",
      "current batch loss: 0.568597  [435200/900000]\n",
      "current batch loss: 0.658991  [441600/900000]\n",
      "current batch loss: 0.654891  [448000/900000]\n",
      "current batch loss: 0.633134  [454400/900000]\n",
      "current batch loss: 0.585425  [460800/900000]\n",
      "current batch loss: 0.682471  [467200/900000]\n",
      "current batch loss: 0.642160  [473600/900000]\n",
      "current batch loss: 0.671118  [480000/900000]\n",
      "current batch loss: 0.595359  [486400/900000]\n",
      "current batch loss: 0.702294  [492800/900000]\n",
      "current batch loss: 0.645794  [499200/900000]\n",
      "current batch loss: 0.579813  [505600/900000]\n",
      "current batch loss: 0.624381  [512000/900000]\n",
      "current batch loss: 0.608818  [518400/900000]\n",
      "current batch loss: 0.718909  [524800/900000]\n",
      "current batch loss: 0.647786  [531200/900000]\n",
      "current batch loss: 0.631697  [537600/900000]\n",
      "current batch loss: 0.624615  [544000/900000]\n",
      "current batch loss: 0.579482  [550400/900000]\n",
      "current batch loss: 0.641735  [556800/900000]\n",
      "current batch loss: 0.561274  [563200/900000]\n",
      "current batch loss: 0.736692  [569600/900000]\n",
      "current batch loss: 0.647827  [576000/900000]\n",
      "current batch loss: 0.621080  [582400/900000]\n",
      "current batch loss: 0.627174  [588800/900000]\n",
      "current batch loss: 0.608098  [595200/900000]\n",
      "current batch loss: 0.578293  [601600/900000]\n",
      "current batch loss: 0.611814  [608000/900000]\n",
      "current batch loss: 0.700398  [614400/900000]\n",
      "current batch loss: 0.548362  [620800/900000]\n",
      "current batch loss: 0.589314  [627200/900000]\n",
      "current batch loss: 0.635207  [633600/900000]\n",
      "current batch loss: 0.655074  [640000/900000]\n",
      "current batch loss: 0.616766  [646400/900000]\n",
      "current batch loss: 0.598436  [652800/900000]\n",
      "current batch loss: 0.568159  [659200/900000]\n",
      "current batch loss: 0.662699  [665600/900000]\n",
      "current batch loss: 0.630440  [672000/900000]\n",
      "current batch loss: 0.605829  [678400/900000]\n",
      "current batch loss: 0.672780  [684800/900000]\n",
      "current batch loss: 0.600403  [691200/900000]\n",
      "current batch loss: 0.625834  [697600/900000]\n",
      "current batch loss: 0.625208  [704000/900000]\n",
      "current batch loss: 0.630530  [710400/900000]\n",
      "current batch loss: 0.615605  [716800/900000]\n",
      "current batch loss: 0.581954  [723200/900000]\n",
      "current batch loss: 0.627652  [729600/900000]\n",
      "current batch loss: 0.640938  [736000/900000]\n",
      "current batch loss: 0.630326  [742400/900000]\n",
      "current batch loss: 0.604658  [748800/900000]\n",
      "current batch loss: 0.616605  [755200/900000]\n",
      "current batch loss: 0.678033  [761600/900000]\n",
      "current batch loss: 0.590443  [768000/900000]\n",
      "current batch loss: 0.616553  [774400/900000]\n",
      "current batch loss: 0.579013  [780800/900000]\n",
      "current batch loss: 0.620059  [787200/900000]\n",
      "current batch loss: 0.633649  [793600/900000]\n",
      "current batch loss: 0.601020  [800000/900000]\n",
      "current batch loss: 0.690158  [806400/900000]\n",
      "current batch loss: 0.607427  [812800/900000]\n",
      "current batch loss: 0.606544  [819200/900000]\n",
      "current batch loss: 0.576217  [825600/900000]\n",
      "current batch loss: 0.601853  [832000/900000]\n",
      "current batch loss: 0.620421  [838400/900000]\n",
      "current batch loss: 0.630980  [844800/900000]\n",
      "current batch loss: 0.607639  [851200/900000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.631135  [857600/900000]\n",
      "current batch loss: 0.669455  [864000/900000]\n",
      "current batch loss: 0.659683  [870400/900000]\n",
      "current batch loss: 0.641191  [876800/900000]\n",
      "current batch loss: 0.566777  [883200/900000]\n",
      "current batch loss: 0.649800  [889600/900000]\n",
      "current batch loss: 0.716614  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.631518\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.630471\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 29\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.632910  [    0/900000]\n",
      "current batch loss: 0.636934  [ 6400/900000]\n",
      "current batch loss: 0.713517  [12800/900000]\n",
      "current batch loss: 0.673285  [19200/900000]\n",
      "current batch loss: 0.588973  [25600/900000]\n",
      "current batch loss: 0.667910  [32000/900000]\n",
      "current batch loss: 0.653142  [38400/900000]\n",
      "current batch loss: 0.655472  [44800/900000]\n",
      "current batch loss: 0.661257  [51200/900000]\n",
      "current batch loss: 0.639110  [57600/900000]\n",
      "current batch loss: 0.594018  [64000/900000]\n",
      "current batch loss: 0.583996  [70400/900000]\n",
      "current batch loss: 0.605266  [76800/900000]\n",
      "current batch loss: 0.658601  [83200/900000]\n",
      "current batch loss: 0.679554  [89600/900000]\n",
      "current batch loss: 0.583780  [96000/900000]\n",
      "current batch loss: 0.620289  [102400/900000]\n",
      "current batch loss: 0.674240  [108800/900000]\n",
      "current batch loss: 0.628927  [115200/900000]\n",
      "current batch loss: 0.703630  [121600/900000]\n",
      "current batch loss: 0.654862  [128000/900000]\n",
      "current batch loss: 0.655178  [134400/900000]\n",
      "current batch loss: 0.626255  [140800/900000]\n",
      "current batch loss: 0.617194  [147200/900000]\n",
      "current batch loss: 0.693728  [153600/900000]\n",
      "current batch loss: 0.627731  [160000/900000]\n",
      "current batch loss: 0.642478  [166400/900000]\n",
      "current batch loss: 0.659510  [172800/900000]\n",
      "current batch loss: 0.616181  [179200/900000]\n",
      "current batch loss: 0.670702  [185600/900000]\n",
      "current batch loss: 0.635145  [192000/900000]\n",
      "current batch loss: 0.679491  [198400/900000]\n",
      "current batch loss: 0.628907  [204800/900000]\n",
      "current batch loss: 0.612922  [211200/900000]\n",
      "current batch loss: 0.620677  [217600/900000]\n",
      "current batch loss: 0.629932  [224000/900000]\n",
      "current batch loss: 0.574855  [230400/900000]\n",
      "current batch loss: 0.628751  [236800/900000]\n",
      "current batch loss: 0.647063  [243200/900000]\n",
      "current batch loss: 0.665546  [249600/900000]\n",
      "current batch loss: 0.601548  [256000/900000]\n",
      "current batch loss: 0.685394  [262400/900000]\n",
      "current batch loss: 0.650369  [268800/900000]\n",
      "current batch loss: 0.605418  [275200/900000]\n",
      "current batch loss: 0.614791  [281600/900000]\n",
      "current batch loss: 0.633537  [288000/900000]\n",
      "current batch loss: 0.638216  [294400/900000]\n",
      "current batch loss: 0.750818  [300800/900000]\n",
      "current batch loss: 0.660757  [307200/900000]\n",
      "current batch loss: 0.638769  [313600/900000]\n",
      "current batch loss: 0.598907  [320000/900000]\n",
      "current batch loss: 0.536110  [326400/900000]\n",
      "current batch loss: 0.626128  [332800/900000]\n",
      "current batch loss: 0.638176  [339200/900000]\n",
      "current batch loss: 0.647772  [345600/900000]\n",
      "current batch loss: 0.595866  [352000/900000]\n",
      "current batch loss: 0.607280  [358400/900000]\n",
      "current batch loss: 0.667790  [364800/900000]\n",
      "current batch loss: 0.595065  [371200/900000]\n",
      "current batch loss: 0.688870  [377600/900000]\n",
      "current batch loss: 0.651079  [384000/900000]\n",
      "current batch loss: 0.596305  [390400/900000]\n",
      "current batch loss: 0.631343  [396800/900000]\n",
      "current batch loss: 0.615603  [403200/900000]\n",
      "current batch loss: 0.547809  [409600/900000]\n",
      "current batch loss: 0.596139  [416000/900000]\n",
      "current batch loss: 0.664153  [422400/900000]\n",
      "current batch loss: 0.640303  [428800/900000]\n",
      "current batch loss: 0.671677  [435200/900000]\n",
      "current batch loss: 0.577056  [441600/900000]\n",
      "current batch loss: 0.611772  [448000/900000]\n",
      "current batch loss: 0.676180  [454400/900000]\n",
      "current batch loss: 0.610148  [460800/900000]\n",
      "current batch loss: 0.663895  [467200/900000]\n",
      "current batch loss: 0.593528  [473600/900000]\n",
      "current batch loss: 0.639514  [480000/900000]\n",
      "current batch loss: 0.615025  [486400/900000]\n",
      "current batch loss: 0.600774  [492800/900000]\n",
      "current batch loss: 0.589302  [499200/900000]\n",
      "current batch loss: 0.627099  [505600/900000]\n",
      "current batch loss: 0.592065  [512000/900000]\n",
      "current batch loss: 0.610389  [518400/900000]\n",
      "current batch loss: 0.666819  [524800/900000]\n",
      "current batch loss: 0.603728  [531200/900000]\n",
      "current batch loss: 0.664240  [537600/900000]\n",
      "current batch loss: 0.658802  [544000/900000]\n",
      "current batch loss: 0.714701  [550400/900000]\n",
      "current batch loss: 0.643426  [556800/900000]\n",
      "current batch loss: 0.658710  [563200/900000]\n",
      "current batch loss: 0.649750  [569600/900000]\n",
      "current batch loss: 0.618053  [576000/900000]\n",
      "current batch loss: 0.620290  [582400/900000]\n",
      "current batch loss: 0.659674  [588800/900000]\n",
      "current batch loss: 0.621678  [595200/900000]\n",
      "current batch loss: 0.696999  [601600/900000]\n",
      "current batch loss: 0.535455  [608000/900000]\n",
      "current batch loss: 0.660177  [614400/900000]\n",
      "current batch loss: 0.651543  [620800/900000]\n",
      "current batch loss: 0.709918  [627200/900000]\n",
      "current batch loss: 0.639510  [633600/900000]\n",
      "current batch loss: 0.615037  [640000/900000]\n",
      "current batch loss: 0.595533  [646400/900000]\n",
      "current batch loss: 0.610710  [652800/900000]\n",
      "current batch loss: 0.651257  [659200/900000]\n",
      "current batch loss: 0.717876  [665600/900000]\n",
      "current batch loss: 0.677746  [672000/900000]\n",
      "current batch loss: 0.600191  [678400/900000]\n",
      "current batch loss: 0.576211  [684800/900000]\n",
      "current batch loss: 0.656619  [691200/900000]\n",
      "current batch loss: 0.636507  [697600/900000]\n",
      "current batch loss: 0.645268  [704000/900000]\n",
      "current batch loss: 0.677213  [710400/900000]\n",
      "current batch loss: 0.619298  [716800/900000]\n",
      "current batch loss: 0.655189  [723200/900000]\n",
      "current batch loss: 0.649681  [729600/900000]\n",
      "current batch loss: 0.561637  [736000/900000]\n",
      "current batch loss: 0.637152  [742400/900000]\n",
      "current batch loss: 0.645468  [748800/900000]\n",
      "current batch loss: 0.679786  [755200/900000]\n",
      "current batch loss: 0.636039  [761600/900000]\n",
      "current batch loss: 0.621038  [768000/900000]\n",
      "current batch loss: 0.658238  [774400/900000]\n",
      "current batch loss: 0.574062  [780800/900000]\n",
      "current batch loss: 0.645740  [787200/900000]\n",
      "current batch loss: 0.641051  [793600/900000]\n",
      "current batch loss: 0.669636  [800000/900000]\n",
      "current batch loss: 0.612927  [806400/900000]\n",
      "current batch loss: 0.613083  [812800/900000]\n",
      "current batch loss: 0.593733  [819200/900000]\n",
      "current batch loss: 0.574524  [825600/900000]\n",
      "current batch loss: 0.547862  [832000/900000]\n",
      "current batch loss: 0.594208  [838400/900000]\n",
      "current batch loss: 0.635482  [844800/900000]\n",
      "current batch loss: 0.624933  [851200/900000]\n",
      "current batch loss: 0.619634  [857600/900000]\n",
      "current batch loss: 0.602006  [864000/900000]\n",
      "current batch loss: 0.662994  [870400/900000]\n",
      "current batch loss: 0.692790  [876800/900000]\n",
      "current batch loss: 0.646208  [883200/900000]\n",
      "current batch loss: 0.579311  [889600/900000]\n",
      "current batch loss: 0.611478  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.631780\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.630939\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 30\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.610996  [    0/900000]\n",
      "current batch loss: 0.646446  [ 6400/900000]\n",
      "current batch loss: 0.648588  [12800/900000]\n",
      "current batch loss: 0.585075  [19200/900000]\n",
      "current batch loss: 0.670325  [25600/900000]\n",
      "current batch loss: 0.600903  [32000/900000]\n",
      "current batch loss: 0.659446  [38400/900000]\n",
      "current batch loss: 0.666020  [44800/900000]\n",
      "current batch loss: 0.617378  [51200/900000]\n",
      "current batch loss: 0.648394  [57600/900000]\n",
      "current batch loss: 0.599076  [64000/900000]\n",
      "current batch loss: 0.588609  [70400/900000]\n",
      "current batch loss: 0.656041  [76800/900000]\n",
      "current batch loss: 0.666335  [83200/900000]\n",
      "current batch loss: 0.607378  [89600/900000]\n",
      "current batch loss: 0.642129  [96000/900000]\n",
      "current batch loss: 0.650528  [102400/900000]\n",
      "current batch loss: 0.572125  [108800/900000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.659017  [115200/900000]\n",
      "current batch loss: 0.609932  [121600/900000]\n",
      "current batch loss: 0.625385  [128000/900000]\n",
      "current batch loss: 0.593050  [134400/900000]\n",
      "current batch loss: 0.636572  [140800/900000]\n",
      "current batch loss: 0.623379  [147200/900000]\n",
      "current batch loss: 0.673931  [153600/900000]\n",
      "current batch loss: 0.559183  [160000/900000]\n",
      "current batch loss: 0.645224  [166400/900000]\n",
      "current batch loss: 0.640048  [172800/900000]\n",
      "current batch loss: 0.724343  [179200/900000]\n",
      "current batch loss: 0.681911  [185600/900000]\n",
      "current batch loss: 0.697805  [192000/900000]\n",
      "current batch loss: 0.585932  [198400/900000]\n",
      "current batch loss: 0.627474  [204800/900000]\n",
      "current batch loss: 0.653747  [211200/900000]\n",
      "current batch loss: 0.577201  [217600/900000]\n",
      "current batch loss: 0.638288  [224000/900000]\n",
      "current batch loss: 0.626728  [230400/900000]\n",
      "current batch loss: 0.535752  [236800/900000]\n",
      "current batch loss: 0.601192  [243200/900000]\n",
      "current batch loss: 0.600101  [249600/900000]\n",
      "current batch loss: 0.659019  [256000/900000]\n",
      "current batch loss: 0.643485  [262400/900000]\n",
      "current batch loss: 0.648763  [268800/900000]\n",
      "current batch loss: 0.539958  [275200/900000]\n",
      "current batch loss: 0.586174  [281600/900000]\n",
      "current batch loss: 0.613307  [288000/900000]\n",
      "current batch loss: 0.676686  [294400/900000]\n",
      "current batch loss: 0.649443  [300800/900000]\n",
      "current batch loss: 0.569593  [307200/900000]\n",
      "current batch loss: 0.638314  [313600/900000]\n",
      "current batch loss: 0.638315  [320000/900000]\n",
      "current batch loss: 0.626423  [326400/900000]\n",
      "current batch loss: 0.649712  [332800/900000]\n",
      "current batch loss: 0.609284  [339200/900000]\n",
      "current batch loss: 0.654219  [345600/900000]\n",
      "current batch loss: 0.665660  [352000/900000]\n",
      "current batch loss: 0.629685  [358400/900000]\n",
      "current batch loss: 0.685207  [364800/900000]\n",
      "current batch loss: 0.619992  [371200/900000]\n",
      "current batch loss: 0.604083  [377600/900000]\n",
      "current batch loss: 0.539842  [384000/900000]\n",
      "current batch loss: 0.602260  [390400/900000]\n",
      "current batch loss: 0.584108  [396800/900000]\n",
      "current batch loss: 0.661210  [403200/900000]\n",
      "current batch loss: 0.579941  [409600/900000]\n",
      "current batch loss: 0.670439  [416000/900000]\n",
      "current batch loss: 0.671091  [422400/900000]\n",
      "current batch loss: 0.639399  [428800/900000]\n",
      "current batch loss: 0.635980  [435200/900000]\n",
      "current batch loss: 0.612400  [441600/900000]\n",
      "current batch loss: 0.697672  [448000/900000]\n",
      "current batch loss: 0.668464  [454400/900000]\n",
      "current batch loss: 0.669317  [460800/900000]\n",
      "current batch loss: 0.637696  [467200/900000]\n",
      "current batch loss: 0.650520  [473600/900000]\n",
      "current batch loss: 0.627664  [480000/900000]\n",
      "current batch loss: 0.578772  [486400/900000]\n",
      "current batch loss: 0.653589  [492800/900000]\n",
      "current batch loss: 0.610543  [499200/900000]\n",
      "current batch loss: 0.639817  [505600/900000]\n",
      "current batch loss: 0.635032  [512000/900000]\n",
      "current batch loss: 0.624177  [518400/900000]\n",
      "current batch loss: 0.618721  [524800/900000]\n",
      "current batch loss: 0.677796  [531200/900000]\n",
      "current batch loss: 0.621983  [537600/900000]\n",
      "current batch loss: 0.632732  [544000/900000]\n",
      "current batch loss: 0.580577  [550400/900000]\n",
      "current batch loss: 0.667244  [556800/900000]\n",
      "current batch loss: 0.668348  [563200/900000]\n",
      "current batch loss: 0.607312  [569600/900000]\n",
      "current batch loss: 0.561847  [576000/900000]\n",
      "current batch loss: 0.664081  [582400/900000]\n",
      "current batch loss: 0.531647  [588800/900000]\n",
      "current batch loss: 0.635607  [595200/900000]\n",
      "current batch loss: 0.643751  [601600/900000]\n",
      "current batch loss: 0.608989  [608000/900000]\n",
      "current batch loss: 0.638204  [614400/900000]\n",
      "current batch loss: 0.566886  [620800/900000]\n",
      "current batch loss: 0.640314  [627200/900000]\n",
      "current batch loss: 0.663024  [633600/900000]\n",
      "current batch loss: 0.644103  [640000/900000]\n",
      "current batch loss: 0.594369  [646400/900000]\n",
      "current batch loss: 0.687280  [652800/900000]\n",
      "current batch loss: 0.703057  [659200/900000]\n",
      "current batch loss: 0.655758  [665600/900000]\n",
      "current batch loss: 0.685543  [672000/900000]\n",
      "current batch loss: 0.640289  [678400/900000]\n",
      "current batch loss: 0.685211  [684800/900000]\n",
      "current batch loss: 0.681783  [691200/900000]\n",
      "current batch loss: 0.575114  [697600/900000]\n",
      "current batch loss: 0.585885  [704000/900000]\n",
      "current batch loss: 0.624411  [710400/900000]\n",
      "current batch loss: 0.614432  [716800/900000]\n",
      "current batch loss: 0.646112  [723200/900000]\n",
      "current batch loss: 0.656157  [729600/900000]\n",
      "current batch loss: 0.617540  [736000/900000]\n",
      "current batch loss: 0.641812  [742400/900000]\n",
      "current batch loss: 0.559511  [748800/900000]\n",
      "current batch loss: 0.628880  [755200/900000]\n",
      "current batch loss: 0.628898  [761600/900000]\n",
      "current batch loss: 0.654162  [768000/900000]\n",
      "current batch loss: 0.683185  [774400/900000]\n",
      "current batch loss: 0.588497  [780800/900000]\n",
      "current batch loss: 0.567866  [787200/900000]\n",
      "current batch loss: 0.643907  [793600/900000]\n",
      "current batch loss: 0.563959  [800000/900000]\n",
      "current batch loss: 0.709122  [806400/900000]\n",
      "current batch loss: 0.652711  [812800/900000]\n",
      "current batch loss: 0.668538  [819200/900000]\n",
      "current batch loss: 0.578706  [825600/900000]\n",
      "current batch loss: 0.553211  [832000/900000]\n",
      "current batch loss: 0.743475  [838400/900000]\n",
      "current batch loss: 0.614594  [844800/900000]\n",
      "current batch loss: 0.659323  [851200/900000]\n",
      "current batch loss: 0.562974  [857600/900000]\n",
      "current batch loss: 0.602161  [864000/900000]\n",
      "current batch loss: 0.566897  [870400/900000]\n",
      "current batch loss: 0.635491  [876800/900000]\n",
      "current batch loss: 0.629549  [883200/900000]\n",
      "current batch loss: 0.610678  [889600/900000]\n",
      "current batch loss: 0.601612  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.630982\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.629965\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 31\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.598563  [    0/900000]\n",
      "current batch loss: 0.691843  [ 6400/900000]\n",
      "current batch loss: 0.661720  [12800/900000]\n",
      "current batch loss: 0.578390  [19200/900000]\n",
      "current batch loss: 0.617077  [25600/900000]\n",
      "current batch loss: 0.566560  [32000/900000]\n",
      "current batch loss: 0.555991  [38400/900000]\n",
      "current batch loss: 0.513573  [44800/900000]\n",
      "current batch loss: 0.620877  [51200/900000]\n",
      "current batch loss: 0.601131  [57600/900000]\n",
      "current batch loss: 0.602809  [64000/900000]\n",
      "current batch loss: 0.585956  [70400/900000]\n",
      "current batch loss: 0.609233  [76800/900000]\n",
      "current batch loss: 0.652833  [83200/900000]\n",
      "current batch loss: 0.608338  [89600/900000]\n",
      "current batch loss: 0.632823  [96000/900000]\n",
      "current batch loss: 0.637940  [102400/900000]\n",
      "current batch loss: 0.642179  [108800/900000]\n",
      "current batch loss: 0.624180  [115200/900000]\n",
      "current batch loss: 0.602121  [121600/900000]\n",
      "current batch loss: 0.599251  [128000/900000]\n",
      "current batch loss: 0.673578  [134400/900000]\n",
      "current batch loss: 0.596590  [140800/900000]\n",
      "current batch loss: 0.677254  [147200/900000]\n",
      "current batch loss: 0.614596  [153600/900000]\n",
      "current batch loss: 0.663904  [160000/900000]\n",
      "current batch loss: 0.672199  [166400/900000]\n",
      "current batch loss: 0.629663  [172800/900000]\n",
      "current batch loss: 0.653026  [179200/900000]\n",
      "current batch loss: 0.633083  [185600/900000]\n",
      "current batch loss: 0.596358  [192000/900000]\n",
      "current batch loss: 0.640677  [198400/900000]\n",
      "current batch loss: 0.548017  [204800/900000]\n",
      "current batch loss: 0.595913  [211200/900000]\n",
      "current batch loss: 0.646015  [217600/900000]\n",
      "current batch loss: 0.665260  [224000/900000]\n",
      "current batch loss: 0.587050  [230400/900000]\n",
      "current batch loss: 0.603217  [236800/900000]\n",
      "current batch loss: 0.613579  [243200/900000]\n",
      "current batch loss: 0.564448  [249600/900000]\n",
      "current batch loss: 0.626399  [256000/900000]\n",
      "current batch loss: 0.610139  [262400/900000]\n",
      "current batch loss: 0.620776  [268800/900000]\n",
      "current batch loss: 0.636820  [275200/900000]\n",
      "current batch loss: 0.591248  [281600/900000]\n",
      "current batch loss: 0.612291  [288000/900000]\n",
      "current batch loss: 0.564533  [294400/900000]\n",
      "current batch loss: 0.645489  [300800/900000]\n",
      "current batch loss: 0.675859  [307200/900000]\n",
      "current batch loss: 0.618655  [313600/900000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.658365  [320000/900000]\n",
      "current batch loss: 0.669624  [326400/900000]\n",
      "current batch loss: 0.660182  [332800/900000]\n",
      "current batch loss: 0.663393  [339200/900000]\n",
      "current batch loss: 0.637292  [345600/900000]\n",
      "current batch loss: 0.594259  [352000/900000]\n",
      "current batch loss: 0.727298  [358400/900000]\n",
      "current batch loss: 0.643037  [364800/900000]\n",
      "current batch loss: 0.568675  [371200/900000]\n",
      "current batch loss: 0.615879  [377600/900000]\n",
      "current batch loss: 0.654950  [384000/900000]\n",
      "current batch loss: 0.657214  [390400/900000]\n",
      "current batch loss: 0.593046  [396800/900000]\n",
      "current batch loss: 0.645787  [403200/900000]\n",
      "current batch loss: 0.613276  [409600/900000]\n",
      "current batch loss: 0.670702  [416000/900000]\n",
      "current batch loss: 0.651408  [422400/900000]\n",
      "current batch loss: 0.585734  [428800/900000]\n",
      "current batch loss: 0.719685  [435200/900000]\n",
      "current batch loss: 0.595971  [441600/900000]\n",
      "current batch loss: 0.676998  [448000/900000]\n",
      "current batch loss: 0.574793  [454400/900000]\n",
      "current batch loss: 0.601856  [460800/900000]\n",
      "current batch loss: 0.613922  [467200/900000]\n",
      "current batch loss: 0.581461  [473600/900000]\n",
      "current batch loss: 0.644583  [480000/900000]\n",
      "current batch loss: 0.684562  [486400/900000]\n",
      "current batch loss: 0.595450  [492800/900000]\n",
      "current batch loss: 0.558822  [499200/900000]\n",
      "current batch loss: 0.593779  [505600/900000]\n",
      "current batch loss: 0.688656  [512000/900000]\n",
      "current batch loss: 0.645485  [518400/900000]\n",
      "current batch loss: 0.630728  [524800/900000]\n",
      "current batch loss: 0.570661  [531200/900000]\n",
      "current batch loss: 0.671223  [537600/900000]\n",
      "current batch loss: 0.610298  [544000/900000]\n",
      "current batch loss: 0.597612  [550400/900000]\n",
      "current batch loss: 0.599401  [556800/900000]\n",
      "current batch loss: 0.692922  [563200/900000]\n",
      "current batch loss: 0.555866  [569600/900000]\n",
      "current batch loss: 0.684941  [576000/900000]\n",
      "current batch loss: 0.629636  [582400/900000]\n",
      "current batch loss: 0.649961  [588800/900000]\n",
      "current batch loss: 0.579297  [595200/900000]\n",
      "current batch loss: 0.671490  [601600/900000]\n",
      "current batch loss: 0.614340  [608000/900000]\n",
      "current batch loss: 0.592824  [614400/900000]\n",
      "current batch loss: 0.651596  [620800/900000]\n",
      "current batch loss: 0.686112  [627200/900000]\n",
      "current batch loss: 0.595925  [633600/900000]\n",
      "current batch loss: 0.702994  [640000/900000]\n",
      "current batch loss: 0.653957  [646400/900000]\n",
      "current batch loss: 0.582358  [652800/900000]\n",
      "current batch loss: 0.673411  [659200/900000]\n",
      "current batch loss: 0.648977  [665600/900000]\n",
      "current batch loss: 0.645832  [672000/900000]\n",
      "current batch loss: 0.646077  [678400/900000]\n",
      "current batch loss: 0.677333  [684800/900000]\n",
      "current batch loss: 0.577781  [691200/900000]\n",
      "current batch loss: 0.592798  [697600/900000]\n",
      "current batch loss: 0.640707  [704000/900000]\n",
      "current batch loss: 0.634563  [710400/900000]\n",
      "current batch loss: 0.619263  [716800/900000]\n",
      "current batch loss: 0.687227  [723200/900000]\n",
      "current batch loss: 0.708496  [729600/900000]\n",
      "current batch loss: 0.618294  [736000/900000]\n",
      "current batch loss: 0.620862  [742400/900000]\n",
      "current batch loss: 0.704670  [748800/900000]\n",
      "current batch loss: 0.746491  [755200/900000]\n",
      "current batch loss: 0.672688  [761600/900000]\n",
      "current batch loss: 0.628986  [768000/900000]\n",
      "current batch loss: 0.621790  [774400/900000]\n",
      "current batch loss: 0.626934  [780800/900000]\n",
      "current batch loss: 0.621339  [787200/900000]\n",
      "current batch loss: 0.704468  [793600/900000]\n",
      "current batch loss: 0.685148  [800000/900000]\n",
      "current batch loss: 0.664202  [806400/900000]\n",
      "current batch loss: 0.657553  [812800/900000]\n",
      "current batch loss: 0.579063  [819200/900000]\n",
      "current batch loss: 0.674802  [825600/900000]\n",
      "current batch loss: 0.596747  [832000/900000]\n",
      "current batch loss: 0.647228  [838400/900000]\n",
      "current batch loss: 0.565528  [844800/900000]\n",
      "current batch loss: 0.673276  [851200/900000]\n",
      "current batch loss: 0.640422  [857600/900000]\n",
      "current batch loss: 0.664401  [864000/900000]\n",
      "current batch loss: 0.617908  [870400/900000]\n",
      "current batch loss: 0.624041  [876800/900000]\n",
      "current batch loss: 0.612588  [883200/900000]\n",
      "current batch loss: 0.614252  [889600/900000]\n",
      "current batch loss: 0.617752  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.630207\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.629181\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 32\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.678748  [    0/900000]\n",
      "current batch loss: 0.632837  [ 6400/900000]\n",
      "current batch loss: 0.630445  [12800/900000]\n",
      "current batch loss: 0.584971  [19200/900000]\n",
      "current batch loss: 0.722758  [25600/900000]\n",
      "current batch loss: 0.574477  [32000/900000]\n",
      "current batch loss: 0.596026  [38400/900000]\n",
      "current batch loss: 0.668508  [44800/900000]\n",
      "current batch loss: 0.649529  [51200/900000]\n",
      "current batch loss: 0.654667  [57600/900000]\n",
      "current batch loss: 0.684782  [64000/900000]\n",
      "current batch loss: 0.571569  [70400/900000]\n",
      "current batch loss: 0.596194  [76800/900000]\n",
      "current batch loss: 0.630894  [83200/900000]\n",
      "current batch loss: 0.629841  [89600/900000]\n",
      "current batch loss: 0.625468  [96000/900000]\n",
      "current batch loss: 0.656587  [102400/900000]\n",
      "current batch loss: 0.607819  [108800/900000]\n",
      "current batch loss: 0.637433  [115200/900000]\n",
      "current batch loss: 0.700756  [121600/900000]\n",
      "current batch loss: 0.637186  [128000/900000]\n",
      "current batch loss: 0.632157  [134400/900000]\n",
      "current batch loss: 0.632882  [140800/900000]\n",
      "current batch loss: 0.592154  [147200/900000]\n",
      "current batch loss: 0.618276  [153600/900000]\n",
      "current batch loss: 0.652237  [160000/900000]\n",
      "current batch loss: 0.685300  [166400/900000]\n",
      "current batch loss: 0.639958  [172800/900000]\n",
      "current batch loss: 0.618210  [179200/900000]\n",
      "current batch loss: 0.618153  [185600/900000]\n",
      "current batch loss: 0.591261  [192000/900000]\n",
      "current batch loss: 0.612970  [198400/900000]\n",
      "current batch loss: 0.572494  [204800/900000]\n",
      "current batch loss: 0.641022  [211200/900000]\n",
      "current batch loss: 0.648565  [217600/900000]\n",
      "current batch loss: 0.619785  [224000/900000]\n",
      "current batch loss: 0.611628  [230400/900000]\n",
      "current batch loss: 0.643412  [236800/900000]\n",
      "current batch loss: 0.661260  [243200/900000]\n",
      "current batch loss: 0.629316  [249600/900000]\n",
      "current batch loss: 0.593492  [256000/900000]\n",
      "current batch loss: 0.659576  [262400/900000]\n",
      "current batch loss: 0.674274  [268800/900000]\n",
      "current batch loss: 0.646881  [275200/900000]\n",
      "current batch loss: 0.653583  [281600/900000]\n",
      "current batch loss: 0.601589  [288000/900000]\n",
      "current batch loss: 0.632695  [294400/900000]\n",
      "current batch loss: 0.638293  [300800/900000]\n",
      "current batch loss: 0.648693  [307200/900000]\n",
      "current batch loss: 0.609829  [313600/900000]\n",
      "current batch loss: 0.612439  [320000/900000]\n",
      "current batch loss: 0.630877  [326400/900000]\n",
      "current batch loss: 0.570349  [332800/900000]\n",
      "current batch loss: 0.651881  [339200/900000]\n",
      "current batch loss: 0.638397  [345600/900000]\n",
      "current batch loss: 0.667120  [352000/900000]\n",
      "current batch loss: 0.622914  [358400/900000]\n",
      "current batch loss: 0.640751  [364800/900000]\n",
      "current batch loss: 0.654479  [371200/900000]\n",
      "current batch loss: 0.600238  [377600/900000]\n",
      "current batch loss: 0.617519  [384000/900000]\n",
      "current batch loss: 0.656663  [390400/900000]\n",
      "current batch loss: 0.637331  [396800/900000]\n",
      "current batch loss: 0.557619  [403200/900000]\n",
      "current batch loss: 0.597482  [409600/900000]\n",
      "current batch loss: 0.581427  [416000/900000]\n",
      "current batch loss: 0.551077  [422400/900000]\n",
      "current batch loss: 0.696584  [428800/900000]\n",
      "current batch loss: 0.643917  [435200/900000]\n",
      "current batch loss: 0.579413  [441600/900000]\n",
      "current batch loss: 0.611394  [448000/900000]\n",
      "current batch loss: 0.603207  [454400/900000]\n",
      "current batch loss: 0.602722  [460800/900000]\n",
      "current batch loss: 0.618956  [467200/900000]\n",
      "current batch loss: 0.690693  [473600/900000]\n",
      "current batch loss: 0.572488  [480000/900000]\n",
      "current batch loss: 0.646493  [486400/900000]\n",
      "current batch loss: 0.643742  [492800/900000]\n",
      "current batch loss: 0.654053  [499200/900000]\n",
      "current batch loss: 0.594511  [505600/900000]\n",
      "current batch loss: 0.618912  [512000/900000]\n",
      "current batch loss: 0.632520  [518400/900000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.644926  [524800/900000]\n",
      "current batch loss: 0.646813  [531200/900000]\n",
      "current batch loss: 0.663760  [537600/900000]\n",
      "current batch loss: 0.573442  [544000/900000]\n",
      "current batch loss: 0.679655  [550400/900000]\n",
      "current batch loss: 0.736238  [556800/900000]\n",
      "current batch loss: 0.631241  [563200/900000]\n",
      "current batch loss: 0.687688  [569600/900000]\n",
      "current batch loss: 0.643460  [576000/900000]\n",
      "current batch loss: 0.618052  [582400/900000]\n",
      "current batch loss: 0.658161  [588800/900000]\n",
      "current batch loss: 0.648070  [595200/900000]\n",
      "current batch loss: 0.610785  [601600/900000]\n",
      "current batch loss: 0.632162  [608000/900000]\n",
      "current batch loss: 0.638506  [614400/900000]\n",
      "current batch loss: 0.704975  [620800/900000]\n",
      "current batch loss: 0.698257  [627200/900000]\n",
      "current batch loss: 0.640963  [633600/900000]\n",
      "current batch loss: 0.700126  [640000/900000]\n",
      "current batch loss: 0.715437  [646400/900000]\n",
      "current batch loss: 0.625253  [652800/900000]\n",
      "current batch loss: 0.595194  [659200/900000]\n",
      "current batch loss: 0.667080  [665600/900000]\n",
      "current batch loss: 0.626751  [672000/900000]\n",
      "current batch loss: 0.629386  [678400/900000]\n",
      "current batch loss: 0.642144  [684800/900000]\n",
      "current batch loss: 0.643565  [691200/900000]\n",
      "current batch loss: 0.621812  [697600/900000]\n",
      "current batch loss: 0.648350  [704000/900000]\n",
      "current batch loss: 0.691457  [710400/900000]\n",
      "current batch loss: 0.611471  [716800/900000]\n",
      "current batch loss: 0.586161  [723200/900000]\n",
      "current batch loss: 0.612591  [729600/900000]\n",
      "current batch loss: 0.646362  [736000/900000]\n",
      "current batch loss: 0.614893  [742400/900000]\n",
      "current batch loss: 0.668189  [748800/900000]\n",
      "current batch loss: 0.666769  [755200/900000]\n",
      "current batch loss: 0.656061  [761600/900000]\n",
      "current batch loss: 0.563914  [768000/900000]\n",
      "current batch loss: 0.669322  [774400/900000]\n",
      "current batch loss: 0.663227  [780800/900000]\n",
      "current batch loss: 0.615679  [787200/900000]\n",
      "current batch loss: 0.603480  [793600/900000]\n",
      "current batch loss: 0.621223  [800000/900000]\n",
      "current batch loss: 0.631091  [806400/900000]\n",
      "current batch loss: 0.607403  [812800/900000]\n",
      "current batch loss: 0.608058  [819200/900000]\n",
      "current batch loss: 0.631904  [825600/900000]\n",
      "current batch loss: 0.705004  [832000/900000]\n",
      "current batch loss: 0.563096  [838400/900000]\n",
      "current batch loss: 0.661575  [844800/900000]\n",
      "current batch loss: 0.627541  [851200/900000]\n",
      "current batch loss: 0.699059  [857600/900000]\n",
      "current batch loss: 0.682855  [864000/900000]\n",
      "current batch loss: 0.613423  [870400/900000]\n",
      "current batch loss: 0.625760  [876800/900000]\n",
      "current batch loss: 0.609617  [883200/900000]\n",
      "current batch loss: 0.572778  [889600/900000]\n",
      "current batch loss: 0.660158  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.635002\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.633981\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 33\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.606003  [    0/900000]\n",
      "current batch loss: 0.619764  [ 6400/900000]\n",
      "current batch loss: 0.614391  [12800/900000]\n",
      "current batch loss: 0.592008  [19200/900000]\n",
      "current batch loss: 0.709286  [25600/900000]\n",
      "current batch loss: 0.680947  [32000/900000]\n",
      "current batch loss: 0.584635  [38400/900000]\n",
      "current batch loss: 0.666525  [44800/900000]\n",
      "current batch loss: 0.623649  [51200/900000]\n",
      "current batch loss: 0.650975  [57600/900000]\n",
      "current batch loss: 0.617135  [64000/900000]\n",
      "current batch loss: 0.632854  [70400/900000]\n",
      "current batch loss: 0.654833  [76800/900000]\n",
      "current batch loss: 0.648788  [83200/900000]\n",
      "current batch loss: 0.615801  [89600/900000]\n",
      "current batch loss: 0.570405  [96000/900000]\n",
      "current batch loss: 0.628628  [102400/900000]\n",
      "current batch loss: 0.642587  [108800/900000]\n",
      "current batch loss: 0.615818  [115200/900000]\n",
      "current batch loss: 0.591976  [121600/900000]\n",
      "current batch loss: 0.577549  [128000/900000]\n",
      "current batch loss: 0.665799  [134400/900000]\n",
      "current batch loss: 0.678036  [140800/900000]\n",
      "current batch loss: 0.595729  [147200/900000]\n",
      "current batch loss: 0.595973  [153600/900000]\n",
      "current batch loss: 0.677434  [160000/900000]\n",
      "current batch loss: 0.667643  [166400/900000]\n",
      "current batch loss: 0.646114  [172800/900000]\n",
      "current batch loss: 0.600481  [179200/900000]\n",
      "current batch loss: 0.646996  [185600/900000]\n",
      "current batch loss: 0.593103  [192000/900000]\n",
      "current batch loss: 0.597248  [198400/900000]\n",
      "current batch loss: 0.582314  [204800/900000]\n",
      "current batch loss: 0.663476  [211200/900000]\n",
      "current batch loss: 0.592196  [217600/900000]\n",
      "current batch loss: 0.624822  [224000/900000]\n",
      "current batch loss: 0.605919  [230400/900000]\n",
      "current batch loss: 0.700446  [236800/900000]\n",
      "current batch loss: 0.663972  [243200/900000]\n",
      "current batch loss: 0.597462  [249600/900000]\n",
      "current batch loss: 0.667012  [256000/900000]\n",
      "current batch loss: 0.630499  [262400/900000]\n",
      "current batch loss: 0.605912  [268800/900000]\n",
      "current batch loss: 0.576375  [275200/900000]\n",
      "current batch loss: 0.664255  [281600/900000]\n",
      "current batch loss: 0.674792  [288000/900000]\n",
      "current batch loss: 0.606019  [294400/900000]\n",
      "current batch loss: 0.699100  [300800/900000]\n",
      "current batch loss: 0.664977  [307200/900000]\n",
      "current batch loss: 0.652384  [313600/900000]\n",
      "current batch loss: 0.673690  [320000/900000]\n",
      "current batch loss: 0.584865  [326400/900000]\n",
      "current batch loss: 0.604387  [332800/900000]\n",
      "current batch loss: 0.625455  [339200/900000]\n",
      "current batch loss: 0.587225  [345600/900000]\n",
      "current batch loss: 0.615731  [352000/900000]\n",
      "current batch loss: 0.582177  [358400/900000]\n",
      "current batch loss: 0.702915  [364800/900000]\n",
      "current batch loss: 0.686234  [371200/900000]\n",
      "current batch loss: 0.643786  [377600/900000]\n",
      "current batch loss: 0.610223  [384000/900000]\n",
      "current batch loss: 0.609117  [390400/900000]\n",
      "current batch loss: 0.555443  [396800/900000]\n",
      "current batch loss: 0.697164  [403200/900000]\n",
      "current batch loss: 0.600578  [409600/900000]\n",
      "current batch loss: 0.740333  [416000/900000]\n",
      "current batch loss: 0.595501  [422400/900000]\n",
      "current batch loss: 0.622450  [428800/900000]\n",
      "current batch loss: 0.624192  [435200/900000]\n",
      "current batch loss: 0.555007  [441600/900000]\n",
      "current batch loss: 0.580815  [448000/900000]\n",
      "current batch loss: 0.642611  [454400/900000]\n",
      "current batch loss: 0.636823  [460800/900000]\n",
      "current batch loss: 0.710678  [467200/900000]\n",
      "current batch loss: 0.581735  [473600/900000]\n",
      "current batch loss: 0.761224  [480000/900000]\n",
      "current batch loss: 0.593399  [486400/900000]\n",
      "current batch loss: 0.629256  [492800/900000]\n",
      "current batch loss: 0.718301  [499200/900000]\n",
      "current batch loss: 0.663519  [505600/900000]\n",
      "current batch loss: 0.540024  [512000/900000]\n",
      "current batch loss: 0.641035  [518400/900000]\n",
      "current batch loss: 0.698076  [524800/900000]\n",
      "current batch loss: 0.672226  [531200/900000]\n",
      "current batch loss: 0.642562  [537600/900000]\n",
      "current batch loss: 0.693849  [544000/900000]\n",
      "current batch loss: 0.539996  [550400/900000]\n",
      "current batch loss: 0.641933  [556800/900000]\n",
      "current batch loss: 0.616854  [563200/900000]\n",
      "current batch loss: 0.606728  [569600/900000]\n",
      "current batch loss: 0.583623  [576000/900000]\n",
      "current batch loss: 0.663048  [582400/900000]\n",
      "current batch loss: 0.640006  [588800/900000]\n",
      "current batch loss: 0.661877  [595200/900000]\n",
      "current batch loss: 0.614178  [601600/900000]\n",
      "current batch loss: 0.709102  [608000/900000]\n",
      "current batch loss: 0.631026  [614400/900000]\n",
      "current batch loss: 0.697273  [620800/900000]\n",
      "current batch loss: 0.572701  [627200/900000]\n",
      "current batch loss: 0.703925  [633600/900000]\n",
      "current batch loss: 0.633701  [640000/900000]\n",
      "current batch loss: 0.551833  [646400/900000]\n",
      "current batch loss: 0.674494  [652800/900000]\n",
      "current batch loss: 0.615995  [659200/900000]\n",
      "current batch loss: 0.672596  [665600/900000]\n",
      "current batch loss: 0.707870  [672000/900000]\n",
      "current batch loss: 0.596245  [678400/900000]\n",
      "current batch loss: 0.621020  [684800/900000]\n",
      "current batch loss: 0.599156  [691200/900000]\n",
      "current batch loss: 0.595302  [697600/900000]\n",
      "current batch loss: 0.588925  [704000/900000]\n",
      "current batch loss: 0.635242  [710400/900000]\n",
      "current batch loss: 0.631737  [716800/900000]\n",
      "current batch loss: 0.651365  [723200/900000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.655326  [729600/900000]\n",
      "current batch loss: 0.709490  [736000/900000]\n",
      "current batch loss: 0.692667  [742400/900000]\n",
      "current batch loss: 0.666770  [748800/900000]\n",
      "current batch loss: 0.656853  [755200/900000]\n",
      "current batch loss: 0.599947  [761600/900000]\n",
      "current batch loss: 0.738348  [768000/900000]\n",
      "current batch loss: 0.579940  [774400/900000]\n",
      "current batch loss: 0.637007  [780800/900000]\n",
      "current batch loss: 0.610693  [787200/900000]\n",
      "current batch loss: 0.638121  [793600/900000]\n",
      "current batch loss: 0.646473  [800000/900000]\n",
      "current batch loss: 0.639719  [806400/900000]\n",
      "current batch loss: 0.586504  [812800/900000]\n",
      "current batch loss: 0.678371  [819200/900000]\n",
      "current batch loss: 0.613971  [825600/900000]\n",
      "current batch loss: 0.555177  [832000/900000]\n",
      "current batch loss: 0.666143  [838400/900000]\n",
      "current batch loss: 0.574024  [844800/900000]\n",
      "current batch loss: 0.681694  [851200/900000]\n",
      "current batch loss: 0.721461  [857600/900000]\n",
      "current batch loss: 0.661102  [864000/900000]\n",
      "current batch loss: 0.639585  [870400/900000]\n",
      "current batch loss: 0.607651  [876800/900000]\n",
      "current batch loss: 0.642752  [883200/900000]\n",
      "current batch loss: 0.606792  [889600/900000]\n",
      "current batch loss: 0.564340  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.630262\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.629183\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 34\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.644206  [    0/900000]\n",
      "current batch loss: 0.657477  [ 6400/900000]\n",
      "current batch loss: 0.601477  [12800/900000]\n",
      "current batch loss: 0.620347  [19200/900000]\n",
      "current batch loss: 0.678834  [25600/900000]\n",
      "current batch loss: 0.590612  [32000/900000]\n",
      "current batch loss: 0.692260  [38400/900000]\n",
      "current batch loss: 0.664752  [44800/900000]\n",
      "current batch loss: 0.621693  [51200/900000]\n",
      "current batch loss: 0.638542  [57600/900000]\n",
      "current batch loss: 0.625328  [64000/900000]\n",
      "current batch loss: 0.588519  [70400/900000]\n",
      "current batch loss: 0.582470  [76800/900000]\n",
      "current batch loss: 0.619416  [83200/900000]\n",
      "current batch loss: 0.608949  [89600/900000]\n",
      "current batch loss: 0.648523  [96000/900000]\n",
      "current batch loss: 0.611798  [102400/900000]\n",
      "current batch loss: 0.653379  [108800/900000]\n",
      "current batch loss: 0.606838  [115200/900000]\n",
      "current batch loss: 0.638683  [121600/900000]\n",
      "current batch loss: 0.699850  [128000/900000]\n",
      "current batch loss: 0.626435  [134400/900000]\n",
      "current batch loss: 0.589119  [140800/900000]\n",
      "current batch loss: 0.588319  [147200/900000]\n",
      "current batch loss: 0.630443  [153600/900000]\n",
      "current batch loss: 0.652745  [160000/900000]\n",
      "current batch loss: 0.671386  [166400/900000]\n",
      "current batch loss: 0.599259  [172800/900000]\n",
      "current batch loss: 0.625012  [179200/900000]\n",
      "current batch loss: 0.639559  [185600/900000]\n",
      "current batch loss: 0.597430  [192000/900000]\n",
      "current batch loss: 0.604934  [198400/900000]\n",
      "current batch loss: 0.649253  [204800/900000]\n",
      "current batch loss: 0.658740  [211200/900000]\n",
      "current batch loss: 0.577574  [217600/900000]\n",
      "current batch loss: 0.602794  [224000/900000]\n",
      "current batch loss: 0.582258  [230400/900000]\n",
      "current batch loss: 0.684405  [236800/900000]\n",
      "current batch loss: 0.634848  [243200/900000]\n",
      "current batch loss: 0.588716  [249600/900000]\n",
      "current batch loss: 0.548976  [256000/900000]\n",
      "current batch loss: 0.675074  [262400/900000]\n",
      "current batch loss: 0.616764  [268800/900000]\n",
      "current batch loss: 0.606415  [275200/900000]\n",
      "current batch loss: 0.637491  [281600/900000]\n",
      "current batch loss: 0.681128  [288000/900000]\n",
      "current batch loss: 0.668736  [294400/900000]\n",
      "current batch loss: 0.656469  [300800/900000]\n",
      "current batch loss: 0.590888  [307200/900000]\n",
      "current batch loss: 0.650493  [313600/900000]\n",
      "current batch loss: 0.565149  [320000/900000]\n",
      "current batch loss: 0.667741  [326400/900000]\n",
      "current batch loss: 0.602340  [332800/900000]\n",
      "current batch loss: 0.590012  [339200/900000]\n",
      "current batch loss: 0.599008  [345600/900000]\n",
      "current batch loss: 0.624496  [352000/900000]\n",
      "current batch loss: 0.612394  [358400/900000]\n",
      "current batch loss: 0.644653  [364800/900000]\n",
      "current batch loss: 0.590208  [371200/900000]\n",
      "current batch loss: 0.643006  [377600/900000]\n",
      "current batch loss: 0.635930  [384000/900000]\n",
      "current batch loss: 0.624838  [390400/900000]\n",
      "current batch loss: 0.634595  [396800/900000]\n",
      "current batch loss: 0.638933  [403200/900000]\n",
      "current batch loss: 0.646713  [409600/900000]\n",
      "current batch loss: 0.625750  [416000/900000]\n",
      "current batch loss: 0.727598  [422400/900000]\n",
      "current batch loss: 0.603719  [428800/900000]\n",
      "current batch loss: 0.651593  [435200/900000]\n",
      "current batch loss: 0.663374  [441600/900000]\n",
      "current batch loss: 0.646577  [448000/900000]\n",
      "current batch loss: 0.650826  [454400/900000]\n",
      "current batch loss: 0.563061  [460800/900000]\n",
      "current batch loss: 0.673183  [467200/900000]\n",
      "current batch loss: 0.611609  [473600/900000]\n",
      "current batch loss: 0.632789  [480000/900000]\n",
      "current batch loss: 0.539150  [486400/900000]\n",
      "current batch loss: 0.601192  [492800/900000]\n",
      "current batch loss: 0.678228  [499200/900000]\n",
      "current batch loss: 0.666648  [505600/900000]\n",
      "current batch loss: 0.614553  [512000/900000]\n",
      "current batch loss: 0.671263  [518400/900000]\n",
      "current batch loss: 0.667582  [524800/900000]\n",
      "current batch loss: 0.636951  [531200/900000]\n",
      "current batch loss: 0.678947  [537600/900000]\n",
      "current batch loss: 0.628778  [544000/900000]\n",
      "current batch loss: 0.638290  [550400/900000]\n",
      "current batch loss: 0.634423  [556800/900000]\n",
      "current batch loss: 0.623104  [563200/900000]\n",
      "current batch loss: 0.600803  [569600/900000]\n",
      "current batch loss: 0.638521  [576000/900000]\n",
      "current batch loss: 0.592024  [582400/900000]\n",
      "current batch loss: 0.594428  [588800/900000]\n",
      "current batch loss: 0.660596  [595200/900000]\n",
      "current batch loss: 0.593504  [601600/900000]\n",
      "current batch loss: 0.593657  [608000/900000]\n",
      "current batch loss: 0.621630  [614400/900000]\n",
      "current batch loss: 0.628959  [620800/900000]\n",
      "current batch loss: 0.563074  [627200/900000]\n",
      "current batch loss: 0.596564  [633600/900000]\n",
      "current batch loss: 0.639405  [640000/900000]\n",
      "current batch loss: 0.635466  [646400/900000]\n",
      "current batch loss: 0.580338  [652800/900000]\n",
      "current batch loss: 0.657673  [659200/900000]\n",
      "current batch loss: 0.674199  [665600/900000]\n",
      "current batch loss: 0.611819  [672000/900000]\n",
      "current batch loss: 0.705326  [678400/900000]\n",
      "current batch loss: 0.650853  [684800/900000]\n",
      "current batch loss: 0.608740  [691200/900000]\n",
      "current batch loss: 0.644902  [697600/900000]\n",
      "current batch loss: 0.605610  [704000/900000]\n",
      "current batch loss: 0.665411  [710400/900000]\n",
      "current batch loss: 0.624523  [716800/900000]\n",
      "current batch loss: 0.617359  [723200/900000]\n",
      "current batch loss: 0.620060  [729600/900000]\n",
      "current batch loss: 0.606754  [736000/900000]\n",
      "current batch loss: 0.587780  [742400/900000]\n",
      "current batch loss: 0.632523  [748800/900000]\n",
      "current batch loss: 0.611198  [755200/900000]\n",
      "current batch loss: 0.591974  [761600/900000]\n",
      "current batch loss: 0.658776  [768000/900000]\n",
      "current batch loss: 0.672802  [774400/900000]\n",
      "current batch loss: 0.605689  [780800/900000]\n",
      "current batch loss: 0.570752  [787200/900000]\n",
      "current batch loss: 0.675987  [793600/900000]\n",
      "current batch loss: 0.617228  [800000/900000]\n",
      "current batch loss: 0.566830  [806400/900000]\n",
      "current batch loss: 0.660698  [812800/900000]\n",
      "current batch loss: 0.598189  [819200/900000]\n",
      "current batch loss: 0.653773  [825600/900000]\n",
      "current batch loss: 0.600764  [832000/900000]\n",
      "current batch loss: 0.708063  [838400/900000]\n",
      "current batch loss: 0.621337  [844800/900000]\n",
      "current batch loss: 0.609361  [851200/900000]\n",
      "current batch loss: 0.665916  [857600/900000]\n",
      "current batch loss: 0.589354  [864000/900000]\n",
      "current batch loss: 0.597546  [870400/900000]\n",
      "current batch loss: 0.654514  [876800/900000]\n",
      "current batch loss: 0.640211  [883200/900000]\n",
      "current batch loss: 0.661829  [889600/900000]\n",
      "current batch loss: 0.666009  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.632317\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.631494\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 35\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.595554  [    0/900000]\n",
      "current batch loss: 0.636829  [ 6400/900000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.617931  [12800/900000]\n",
      "current batch loss: 0.609215  [19200/900000]\n",
      "current batch loss: 0.681854  [25600/900000]\n",
      "current batch loss: 0.579651  [32000/900000]\n",
      "current batch loss: 0.636148  [38400/900000]\n",
      "current batch loss: 0.601141  [44800/900000]\n",
      "current batch loss: 0.670786  [51200/900000]\n",
      "current batch loss: 0.654602  [57600/900000]\n",
      "current batch loss: 0.641475  [64000/900000]\n",
      "current batch loss: 0.659912  [70400/900000]\n",
      "current batch loss: 0.603672  [76800/900000]\n",
      "current batch loss: 0.686860  [83200/900000]\n",
      "current batch loss: 0.677620  [89600/900000]\n",
      "current batch loss: 0.574677  [96000/900000]\n",
      "current batch loss: 0.643226  [102400/900000]\n",
      "current batch loss: 0.606298  [108800/900000]\n",
      "current batch loss: 0.653729  [115200/900000]\n",
      "current batch loss: 0.667464  [121600/900000]\n",
      "current batch loss: 0.683776  [128000/900000]\n",
      "current batch loss: 0.658025  [134400/900000]\n",
      "current batch loss: 0.639583  [140800/900000]\n",
      "current batch loss: 0.584142  [147200/900000]\n",
      "current batch loss: 0.649049  [153600/900000]\n",
      "current batch loss: 0.656319  [160000/900000]\n",
      "current batch loss: 0.685653  [166400/900000]\n",
      "current batch loss: 0.658805  [172800/900000]\n",
      "current batch loss: 0.712030  [179200/900000]\n",
      "current batch loss: 0.641289  [185600/900000]\n",
      "current batch loss: 0.576398  [192000/900000]\n",
      "current batch loss: 0.683052  [198400/900000]\n",
      "current batch loss: 0.625571  [204800/900000]\n",
      "current batch loss: 0.667684  [211200/900000]\n",
      "current batch loss: 0.668935  [217600/900000]\n",
      "current batch loss: 0.746856  [224000/900000]\n",
      "current batch loss: 0.643331  [230400/900000]\n",
      "current batch loss: 0.684867  [236800/900000]\n",
      "current batch loss: 0.724553  [243200/900000]\n",
      "current batch loss: 0.604498  [249600/900000]\n",
      "current batch loss: 0.655478  [256000/900000]\n",
      "current batch loss: 0.612564  [262400/900000]\n",
      "current batch loss: 0.644820  [268800/900000]\n",
      "current batch loss: 0.632682  [275200/900000]\n",
      "current batch loss: 0.637079  [281600/900000]\n",
      "current batch loss: 0.670458  [288000/900000]\n",
      "current batch loss: 0.643565  [294400/900000]\n",
      "current batch loss: 0.587017  [300800/900000]\n",
      "current batch loss: 0.615704  [307200/900000]\n",
      "current batch loss: 0.727873  [313600/900000]\n",
      "current batch loss: 0.616194  [320000/900000]\n",
      "current batch loss: 0.654124  [326400/900000]\n",
      "current batch loss: 0.635475  [332800/900000]\n",
      "current batch loss: 0.711411  [339200/900000]\n",
      "current batch loss: 0.595334  [345600/900000]\n",
      "current batch loss: 0.615068  [352000/900000]\n",
      "current batch loss: 0.580800  [358400/900000]\n",
      "current batch loss: 0.619650  [364800/900000]\n",
      "current batch loss: 0.598480  [371200/900000]\n",
      "current batch loss: 0.631678  [377600/900000]\n",
      "current batch loss: 0.609908  [384000/900000]\n",
      "current batch loss: 0.646762  [390400/900000]\n",
      "current batch loss: 0.697358  [396800/900000]\n",
      "current batch loss: 0.556049  [403200/900000]\n",
      "current batch loss: 0.641589  [409600/900000]\n",
      "current batch loss: 0.644029  [416000/900000]\n",
      "current batch loss: 0.717793  [422400/900000]\n",
      "current batch loss: 0.650708  [428800/900000]\n",
      "current batch loss: 0.647901  [435200/900000]\n",
      "current batch loss: 0.658583  [441600/900000]\n",
      "current batch loss: 0.636229  [448000/900000]\n",
      "current batch loss: 0.574123  [454400/900000]\n",
      "current batch loss: 0.654918  [460800/900000]\n",
      "current batch loss: 0.646820  [467200/900000]\n",
      "current batch loss: 0.692922  [473600/900000]\n",
      "current batch loss: 0.574125  [480000/900000]\n",
      "current batch loss: 0.594168  [486400/900000]\n",
      "current batch loss: 0.689483  [492800/900000]\n",
      "current batch loss: 0.700372  [499200/900000]\n",
      "current batch loss: 0.668429  [505600/900000]\n",
      "current batch loss: 0.651529  [512000/900000]\n",
      "current batch loss: 0.590301  [518400/900000]\n",
      "current batch loss: 0.607934  [524800/900000]\n",
      "current batch loss: 0.566584  [531200/900000]\n",
      "current batch loss: 0.657450  [537600/900000]\n",
      "current batch loss: 0.568938  [544000/900000]\n",
      "current batch loss: 0.624173  [550400/900000]\n",
      "current batch loss: 0.627917  [556800/900000]\n",
      "current batch loss: 0.609028  [563200/900000]\n",
      "current batch loss: 0.615302  [569600/900000]\n",
      "current batch loss: 0.578639  [576000/900000]\n",
      "current batch loss: 0.658825  [582400/900000]\n",
      "current batch loss: 0.600835  [588800/900000]\n",
      "current batch loss: 0.587881  [595200/900000]\n",
      "current batch loss: 0.628986  [601600/900000]\n",
      "current batch loss: 0.706606  [608000/900000]\n",
      "current batch loss: 0.619741  [614400/900000]\n",
      "current batch loss: 0.657989  [620800/900000]\n",
      "current batch loss: 0.637766  [627200/900000]\n",
      "current batch loss: 0.677593  [633600/900000]\n",
      "current batch loss: 0.674859  [640000/900000]\n",
      "current batch loss: 0.652776  [646400/900000]\n",
      "current batch loss: 0.570456  [652800/900000]\n",
      "current batch loss: 0.634248  [659200/900000]\n",
      "current batch loss: 0.658820  [665600/900000]\n",
      "current batch loss: 0.605482  [672000/900000]\n",
      "current batch loss: 0.624588  [678400/900000]\n",
      "current batch loss: 0.599860  [684800/900000]\n",
      "current batch loss: 0.564439  [691200/900000]\n",
      "current batch loss: 0.677264  [697600/900000]\n",
      "current batch loss: 0.611581  [704000/900000]\n",
      "current batch loss: 0.643114  [710400/900000]\n",
      "current batch loss: 0.570418  [716800/900000]\n",
      "current batch loss: 0.611137  [723200/900000]\n",
      "current batch loss: 0.673445  [729600/900000]\n",
      "current batch loss: 0.675815  [736000/900000]\n",
      "current batch loss: 0.513236  [742400/900000]\n",
      "current batch loss: 0.603730  [748800/900000]\n",
      "current batch loss: 0.606483  [755200/900000]\n",
      "current batch loss: 0.622843  [761600/900000]\n",
      "current batch loss: 0.676789  [768000/900000]\n",
      "current batch loss: 0.675349  [774400/900000]\n",
      "current batch loss: 0.650802  [780800/900000]\n",
      "current batch loss: 0.598880  [787200/900000]\n",
      "current batch loss: 0.692066  [793600/900000]\n",
      "current batch loss: 0.603950  [800000/900000]\n",
      "current batch loss: 0.567958  [806400/900000]\n",
      "current batch loss: 0.705850  [812800/900000]\n",
      "current batch loss: 0.587192  [819200/900000]\n",
      "current batch loss: 0.653971  [825600/900000]\n",
      "current batch loss: 0.641773  [832000/900000]\n",
      "current batch loss: 0.569566  [838400/900000]\n",
      "current batch loss: 0.615522  [844800/900000]\n",
      "current batch loss: 0.536307  [851200/900000]\n",
      "current batch loss: 0.612986  [857600/900000]\n",
      "current batch loss: 0.661196  [864000/900000]\n",
      "current batch loss: 0.660730  [870400/900000]\n",
      "current batch loss: 0.633474  [876800/900000]\n",
      "current batch loss: 0.613604  [883200/900000]\n",
      "current batch loss: 0.672986  [889600/900000]\n",
      "current batch loss: 0.582579  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.630596\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.629656\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 36\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.664646  [    0/900000]\n",
      "current batch loss: 0.659179  [ 6400/900000]\n",
      "current batch loss: 0.630291  [12800/900000]\n",
      "current batch loss: 0.623244  [19200/900000]\n",
      "current batch loss: 0.644260  [25600/900000]\n",
      "current batch loss: 0.677666  [32000/900000]\n",
      "current batch loss: 0.666356  [38400/900000]\n",
      "current batch loss: 0.582406  [44800/900000]\n",
      "current batch loss: 0.679388  [51200/900000]\n",
      "current batch loss: 0.663078  [57600/900000]\n",
      "current batch loss: 0.592491  [64000/900000]\n",
      "current batch loss: 0.656370  [70400/900000]\n",
      "current batch loss: 0.609446  [76800/900000]\n",
      "current batch loss: 0.631105  [83200/900000]\n",
      "current batch loss: 0.622994  [89600/900000]\n",
      "current batch loss: 0.570342  [96000/900000]\n",
      "current batch loss: 0.642798  [102400/900000]\n",
      "current batch loss: 0.640138  [108800/900000]\n",
      "current batch loss: 0.587680  [115200/900000]\n",
      "current batch loss: 0.654881  [121600/900000]\n",
      "current batch loss: 0.607106  [128000/900000]\n",
      "current batch loss: 0.609734  [134400/900000]\n",
      "current batch loss: 0.625919  [140800/900000]\n",
      "current batch loss: 0.620395  [147200/900000]\n",
      "current batch loss: 0.656501  [153600/900000]\n",
      "current batch loss: 0.643206  [160000/900000]\n",
      "current batch loss: 0.604098  [166400/900000]\n",
      "current batch loss: 0.513288  [172800/900000]\n",
      "current batch loss: 0.595617  [179200/900000]\n",
      "current batch loss: 0.650678  [185600/900000]\n",
      "current batch loss: 0.654073  [192000/900000]\n",
      "current batch loss: 0.613986  [198400/900000]\n",
      "current batch loss: 0.622797  [204800/900000]\n",
      "current batch loss: 0.616924  [211200/900000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.671727  [217600/900000]\n",
      "current batch loss: 0.588934  [224000/900000]\n",
      "current batch loss: 0.562696  [230400/900000]\n",
      "current batch loss: 0.662685  [236800/900000]\n",
      "current batch loss: 0.619306  [243200/900000]\n",
      "current batch loss: 0.650234  [249600/900000]\n",
      "current batch loss: 0.696833  [256000/900000]\n",
      "current batch loss: 0.650067  [262400/900000]\n",
      "current batch loss: 0.640521  [268800/900000]\n",
      "current batch loss: 0.656935  [275200/900000]\n",
      "current batch loss: 0.585577  [281600/900000]\n",
      "current batch loss: 0.689017  [288000/900000]\n",
      "current batch loss: 0.655480  [294400/900000]\n",
      "current batch loss: 0.604040  [300800/900000]\n",
      "current batch loss: 0.645399  [307200/900000]\n",
      "current batch loss: 0.656560  [313600/900000]\n",
      "current batch loss: 0.634905  [320000/900000]\n",
      "current batch loss: 0.658514  [326400/900000]\n",
      "current batch loss: 0.678044  [332800/900000]\n",
      "current batch loss: 0.646849  [339200/900000]\n",
      "current batch loss: 0.644661  [345600/900000]\n",
      "current batch loss: 0.611316  [352000/900000]\n",
      "current batch loss: 0.681086  [358400/900000]\n",
      "current batch loss: 0.633323  [364800/900000]\n",
      "current batch loss: 0.653340  [371200/900000]\n",
      "current batch loss: 0.689056  [377600/900000]\n",
      "current batch loss: 0.640742  [384000/900000]\n",
      "current batch loss: 0.648955  [390400/900000]\n",
      "current batch loss: 0.677518  [396800/900000]\n",
      "current batch loss: 0.618361  [403200/900000]\n",
      "current batch loss: 0.681585  [409600/900000]\n",
      "current batch loss: 0.590180  [416000/900000]\n",
      "current batch loss: 0.595559  [422400/900000]\n",
      "current batch loss: 0.600746  [428800/900000]\n",
      "current batch loss: 0.686266  [435200/900000]\n",
      "current batch loss: 0.629750  [441600/900000]\n",
      "current batch loss: 0.590614  [448000/900000]\n",
      "current batch loss: 0.609266  [454400/900000]\n",
      "current batch loss: 0.606872  [460800/900000]\n",
      "current batch loss: 0.633714  [467200/900000]\n",
      "current batch loss: 0.642142  [473600/900000]\n",
      "current batch loss: 0.654007  [480000/900000]\n",
      "current batch loss: 0.681967  [486400/900000]\n",
      "current batch loss: 0.623693  [492800/900000]\n",
      "current batch loss: 0.634343  [499200/900000]\n",
      "current batch loss: 0.639812  [505600/900000]\n",
      "current batch loss: 0.653408  [512000/900000]\n",
      "current batch loss: 0.650853  [518400/900000]\n",
      "current batch loss: 0.622816  [524800/900000]\n",
      "current batch loss: 0.621162  [531200/900000]\n",
      "current batch loss: 0.570321  [537600/900000]\n",
      "current batch loss: 0.615105  [544000/900000]\n",
      "current batch loss: 0.655695  [550400/900000]\n",
      "current batch loss: 0.600672  [556800/900000]\n",
      "current batch loss: 0.658549  [563200/900000]\n",
      "current batch loss: 0.597181  [569600/900000]\n",
      "current batch loss: 0.660308  [576000/900000]\n",
      "current batch loss: 0.615361  [582400/900000]\n",
      "current batch loss: 0.659943  [588800/900000]\n",
      "current batch loss: 0.614806  [595200/900000]\n",
      "current batch loss: 0.675635  [601600/900000]\n",
      "current batch loss: 0.619915  [608000/900000]\n",
      "current batch loss: 0.707425  [614400/900000]\n",
      "current batch loss: 0.645336  [620800/900000]\n",
      "current batch loss: 0.602671  [627200/900000]\n",
      "current batch loss: 0.621902  [633600/900000]\n",
      "current batch loss: 0.595429  [640000/900000]\n",
      "current batch loss: 0.594918  [646400/900000]\n",
      "current batch loss: 0.567295  [652800/900000]\n",
      "current batch loss: 0.747684  [659200/900000]\n",
      "current batch loss: 0.606134  [665600/900000]\n",
      "current batch loss: 0.638143  [672000/900000]\n",
      "current batch loss: 0.634468  [678400/900000]\n",
      "current batch loss: 0.616134  [684800/900000]\n",
      "current batch loss: 0.576340  [691200/900000]\n",
      "current batch loss: 0.586444  [697600/900000]\n",
      "current batch loss: 0.608796  [704000/900000]\n",
      "current batch loss: 0.595509  [710400/900000]\n",
      "current batch loss: 0.702680  [716800/900000]\n",
      "current batch loss: 0.663318  [723200/900000]\n",
      "current batch loss: 0.647197  [729600/900000]\n",
      "current batch loss: 0.609544  [736000/900000]\n",
      "current batch loss: 0.649077  [742400/900000]\n",
      "current batch loss: 0.669666  [748800/900000]\n",
      "current batch loss: 0.648629  [755200/900000]\n",
      "current batch loss: 0.641209  [761600/900000]\n",
      "current batch loss: 0.573324  [768000/900000]\n",
      "current batch loss: 0.672151  [774400/900000]\n",
      "current batch loss: 0.572456  [780800/900000]\n",
      "current batch loss: 0.697282  [787200/900000]\n",
      "current batch loss: 0.635836  [793600/900000]\n",
      "current batch loss: 0.669546  [800000/900000]\n",
      "current batch loss: 0.665586  [806400/900000]\n",
      "current batch loss: 0.606921  [812800/900000]\n",
      "current batch loss: 0.583458  [819200/900000]\n",
      "current batch loss: 0.656759  [825600/900000]\n",
      "current batch loss: 0.615085  [832000/900000]\n",
      "current batch loss: 0.617139  [838400/900000]\n",
      "current batch loss: 0.679334  [844800/900000]\n",
      "current batch loss: 0.611492  [851200/900000]\n",
      "current batch loss: 0.663143  [857600/900000]\n",
      "current batch loss: 0.606278  [864000/900000]\n",
      "current batch loss: 0.638732  [870400/900000]\n",
      "current batch loss: 0.616026  [876800/900000]\n",
      "current batch loss: 0.631906  [883200/900000]\n",
      "current batch loss: 0.669875  [889600/900000]\n",
      "current batch loss: 0.655185  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.632417\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.631496\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 37\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.585051  [    0/900000]\n",
      "current batch loss: 0.574966  [ 6400/900000]\n",
      "current batch loss: 0.617837  [12800/900000]\n",
      "current batch loss: 0.594621  [19200/900000]\n",
      "current batch loss: 0.637142  [25600/900000]\n",
      "current batch loss: 0.651924  [32000/900000]\n",
      "current batch loss: 0.654352  [38400/900000]\n",
      "current batch loss: 0.618798  [44800/900000]\n",
      "current batch loss: 0.675230  [51200/900000]\n",
      "current batch loss: 0.650963  [57600/900000]\n",
      "current batch loss: 0.656223  [64000/900000]\n",
      "current batch loss: 0.598778  [70400/900000]\n",
      "current batch loss: 0.597107  [76800/900000]\n",
      "current batch loss: 0.632184  [83200/900000]\n",
      "current batch loss: 0.660918  [89600/900000]\n",
      "current batch loss: 0.586450  [96000/900000]\n",
      "current batch loss: 0.632386  [102400/900000]\n",
      "current batch loss: 0.667437  [108800/900000]\n",
      "current batch loss: 0.665653  [115200/900000]\n",
      "current batch loss: 0.704769  [121600/900000]\n",
      "current batch loss: 0.634933  [128000/900000]\n",
      "current batch loss: 0.653373  [134400/900000]\n",
      "current batch loss: 0.592057  [140800/900000]\n",
      "current batch loss: 0.690669  [147200/900000]\n",
      "current batch loss: 0.688145  [153600/900000]\n",
      "current batch loss: 0.686497  [160000/900000]\n",
      "current batch loss: 0.676233  [166400/900000]\n",
      "current batch loss: 0.696196  [172800/900000]\n",
      "current batch loss: 0.614737  [179200/900000]\n",
      "current batch loss: 0.584022  [185600/900000]\n",
      "current batch loss: 0.599232  [192000/900000]\n",
      "current batch loss: 0.614713  [198400/900000]\n",
      "current batch loss: 0.602080  [204800/900000]\n",
      "current batch loss: 0.635031  [211200/900000]\n",
      "current batch loss: 0.636125  [217600/900000]\n",
      "current batch loss: 0.645302  [224000/900000]\n",
      "current batch loss: 0.657303  [230400/900000]\n",
      "current batch loss: 0.612323  [236800/900000]\n",
      "current batch loss: 0.691363  [243200/900000]\n",
      "current batch loss: 0.690507  [249600/900000]\n",
      "current batch loss: 0.571117  [256000/900000]\n",
      "current batch loss: 0.636918  [262400/900000]\n",
      "current batch loss: 0.660915  [268800/900000]\n",
      "current batch loss: 0.691283  [275200/900000]\n",
      "current batch loss: 0.597592  [281600/900000]\n",
      "current batch loss: 0.640871  [288000/900000]\n",
      "current batch loss: 0.605445  [294400/900000]\n",
      "current batch loss: 0.679668  [300800/900000]\n",
      "current batch loss: 0.620373  [307200/900000]\n",
      "current batch loss: 0.635258  [313600/900000]\n",
      "current batch loss: 0.606647  [320000/900000]\n",
      "current batch loss: 0.562734  [326400/900000]\n",
      "current batch loss: 0.659161  [332800/900000]\n",
      "current batch loss: 0.648572  [339200/900000]\n",
      "current batch loss: 0.668722  [345600/900000]\n",
      "current batch loss: 0.551484  [352000/900000]\n",
      "current batch loss: 0.644889  [358400/900000]\n",
      "current batch loss: 0.567043  [364800/900000]\n",
      "current batch loss: 0.571217  [371200/900000]\n",
      "current batch loss: 0.607001  [377600/900000]\n",
      "current batch loss: 0.629638  [384000/900000]\n",
      "current batch loss: 0.647921  [390400/900000]\n",
      "current batch loss: 0.635249  [396800/900000]\n",
      "current batch loss: 0.628827  [403200/900000]\n",
      "current batch loss: 0.628813  [409600/900000]\n",
      "current batch loss: 0.616003  [416000/900000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.669778  [422400/900000]\n",
      "current batch loss: 0.646121  [428800/900000]\n",
      "current batch loss: 0.647250  [435200/900000]\n",
      "current batch loss: 0.593177  [441600/900000]\n",
      "current batch loss: 0.636528  [448000/900000]\n",
      "current batch loss: 0.616257  [454400/900000]\n",
      "current batch loss: 0.600442  [460800/900000]\n",
      "current batch loss: 0.598808  [467200/900000]\n",
      "current batch loss: 0.652362  [473600/900000]\n",
      "current batch loss: 0.685961  [480000/900000]\n",
      "current batch loss: 0.670729  [486400/900000]\n",
      "current batch loss: 0.601560  [492800/900000]\n",
      "current batch loss: 0.704699  [499200/900000]\n",
      "current batch loss: 0.569292  [505600/900000]\n",
      "current batch loss: 0.600983  [512000/900000]\n",
      "current batch loss: 0.572957  [518400/900000]\n",
      "current batch loss: 0.642478  [524800/900000]\n",
      "current batch loss: 0.552037  [531200/900000]\n",
      "current batch loss: 0.613147  [537600/900000]\n",
      "current batch loss: 0.650591  [544000/900000]\n",
      "current batch loss: 0.625030  [550400/900000]\n",
      "current batch loss: 0.581012  [556800/900000]\n",
      "current batch loss: 0.548990  [563200/900000]\n",
      "current batch loss: 0.662928  [569600/900000]\n",
      "current batch loss: 0.626351  [576000/900000]\n",
      "current batch loss: 0.670665  [582400/900000]\n",
      "current batch loss: 0.664556  [588800/900000]\n",
      "current batch loss: 0.649876  [595200/900000]\n",
      "current batch loss: 0.666127  [601600/900000]\n",
      "current batch loss: 0.632308  [608000/900000]\n",
      "current batch loss: 0.625904  [614400/900000]\n",
      "current batch loss: 0.563505  [620800/900000]\n",
      "current batch loss: 0.683414  [627200/900000]\n",
      "current batch loss: 0.608939  [633600/900000]\n",
      "current batch loss: 0.644321  [640000/900000]\n",
      "current batch loss: 0.588107  [646400/900000]\n",
      "current batch loss: 0.691284  [652800/900000]\n",
      "current batch loss: 0.671010  [659200/900000]\n",
      "current batch loss: 0.559980  [665600/900000]\n",
      "current batch loss: 0.614182  [672000/900000]\n",
      "current batch loss: 0.672166  [678400/900000]\n",
      "current batch loss: 0.746912  [684800/900000]\n",
      "current batch loss: 0.579524  [691200/900000]\n",
      "current batch loss: 0.665530  [697600/900000]\n",
      "current batch loss: 0.600208  [704000/900000]\n",
      "current batch loss: 0.591209  [710400/900000]\n",
      "current batch loss: 0.560822  [716800/900000]\n",
      "current batch loss: 0.586495  [723200/900000]\n",
      "current batch loss: 0.626651  [729600/900000]\n",
      "current batch loss: 0.506160  [736000/900000]\n",
      "current batch loss: 0.610391  [742400/900000]\n",
      "current batch loss: 0.676339  [748800/900000]\n",
      "current batch loss: 0.647969  [755200/900000]\n",
      "current batch loss: 0.669026  [761600/900000]\n",
      "current batch loss: 0.645799  [768000/900000]\n",
      "current batch loss: 0.675676  [774400/900000]\n",
      "current batch loss: 0.651814  [780800/900000]\n",
      "current batch loss: 0.598461  [787200/900000]\n",
      "current batch loss: 0.649518  [793600/900000]\n",
      "current batch loss: 0.562705  [800000/900000]\n",
      "current batch loss: 0.617977  [806400/900000]\n",
      "current batch loss: 0.657908  [812800/900000]\n",
      "current batch loss: 0.597002  [819200/900000]\n",
      "current batch loss: 0.639739  [825600/900000]\n",
      "current batch loss: 0.634293  [832000/900000]\n",
      "current batch loss: 0.610261  [838400/900000]\n",
      "current batch loss: 0.651052  [844800/900000]\n",
      "current batch loss: 0.654797  [851200/900000]\n",
      "current batch loss: 0.723785  [857600/900000]\n",
      "current batch loss: 0.539069  [864000/900000]\n",
      "current batch loss: 0.629159  [870400/900000]\n",
      "current batch loss: 0.684483  [876800/900000]\n",
      "current batch loss: 0.576413  [883200/900000]\n",
      "current batch loss: 0.637257  [889600/900000]\n",
      "current batch loss: 0.662432  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.630743\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.629700\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 38\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.651081  [    0/900000]\n",
      "current batch loss: 0.633092  [ 6400/900000]\n",
      "current batch loss: 0.601788  [12800/900000]\n",
      "current batch loss: 0.714204  [19200/900000]\n",
      "current batch loss: 0.637741  [25600/900000]\n",
      "current batch loss: 0.649915  [32000/900000]\n",
      "current batch loss: 0.573533  [38400/900000]\n",
      "current batch loss: 0.583056  [44800/900000]\n",
      "current batch loss: 0.615727  [51200/900000]\n",
      "current batch loss: 0.660225  [57600/900000]\n",
      "current batch loss: 0.698512  [64000/900000]\n",
      "current batch loss: 0.675991  [70400/900000]\n",
      "current batch loss: 0.636691  [76800/900000]\n",
      "current batch loss: 0.649460  [83200/900000]\n",
      "current batch loss: 0.629610  [89600/900000]\n",
      "current batch loss: 0.558685  [96000/900000]\n",
      "current batch loss: 0.581821  [102400/900000]\n",
      "current batch loss: 0.689582  [108800/900000]\n",
      "current batch loss: 0.767808  [115200/900000]\n",
      "current batch loss: 0.557501  [121600/900000]\n",
      "current batch loss: 0.565250  [128000/900000]\n",
      "current batch loss: 0.639418  [134400/900000]\n",
      "current batch loss: 0.629223  [140800/900000]\n",
      "current batch loss: 0.689690  [147200/900000]\n",
      "current batch loss: 0.571924  [153600/900000]\n",
      "current batch loss: 0.634653  [160000/900000]\n",
      "current batch loss: 0.603562  [166400/900000]\n",
      "current batch loss: 0.697299  [172800/900000]\n",
      "current batch loss: 0.664784  [179200/900000]\n",
      "current batch loss: 0.618934  [185600/900000]\n",
      "current batch loss: 0.692631  [192000/900000]\n",
      "current batch loss: 0.761075  [198400/900000]\n",
      "current batch loss: 0.619664  [204800/900000]\n",
      "current batch loss: 0.627642  [211200/900000]\n",
      "current batch loss: 0.646884  [217600/900000]\n",
      "current batch loss: 0.605474  [224000/900000]\n",
      "current batch loss: 0.685834  [230400/900000]\n",
      "current batch loss: 0.607688  [236800/900000]\n",
      "current batch loss: 0.612850  [243200/900000]\n",
      "current batch loss: 0.653099  [249600/900000]\n",
      "current batch loss: 0.638057  [256000/900000]\n",
      "current batch loss: 0.643490  [262400/900000]\n",
      "current batch loss: 0.652043  [268800/900000]\n",
      "current batch loss: 0.598225  [275200/900000]\n",
      "current batch loss: 0.656778  [281600/900000]\n",
      "current batch loss: 0.595447  [288000/900000]\n",
      "current batch loss: 0.564393  [294400/900000]\n",
      "current batch loss: 0.629484  [300800/900000]\n",
      "current batch loss: 0.675792  [307200/900000]\n",
      "current batch loss: 0.649434  [313600/900000]\n",
      "current batch loss: 0.581765  [320000/900000]\n",
      "current batch loss: 0.614329  [326400/900000]\n",
      "current batch loss: 0.613086  [332800/900000]\n",
      "current batch loss: 0.555600  [339200/900000]\n",
      "current batch loss: 0.615669  [345600/900000]\n",
      "current batch loss: 0.544488  [352000/900000]\n",
      "current batch loss: 0.674113  [358400/900000]\n",
      "current batch loss: 0.743392  [364800/900000]\n",
      "current batch loss: 0.568098  [371200/900000]\n",
      "current batch loss: 0.625828  [377600/900000]\n",
      "current batch loss: 0.619867  [384000/900000]\n",
      "current batch loss: 0.647421  [390400/900000]\n",
      "current batch loss: 0.640488  [396800/900000]\n",
      "current batch loss: 0.609126  [403200/900000]\n",
      "current batch loss: 0.634842  [409600/900000]\n",
      "current batch loss: 0.622953  [416000/900000]\n",
      "current batch loss: 0.636313  [422400/900000]\n",
      "current batch loss: 0.644958  [428800/900000]\n",
      "current batch loss: 0.698090  [435200/900000]\n",
      "current batch loss: 0.530255  [441600/900000]\n",
      "current batch loss: 0.624093  [448000/900000]\n",
      "current batch loss: 0.634213  [454400/900000]\n",
      "current batch loss: 0.641979  [460800/900000]\n",
      "current batch loss: 0.597927  [467200/900000]\n",
      "current batch loss: 0.641040  [473600/900000]\n",
      "current batch loss: 0.668277  [480000/900000]\n",
      "current batch loss: 0.662327  [486400/900000]\n",
      "current batch loss: 0.699877  [492800/900000]\n",
      "current batch loss: 0.600688  [499200/900000]\n",
      "current batch loss: 0.583333  [505600/900000]\n",
      "current batch loss: 0.686169  [512000/900000]\n",
      "current batch loss: 0.587717  [518400/900000]\n",
      "current batch loss: 0.629887  [524800/900000]\n",
      "current batch loss: 0.633048  [531200/900000]\n",
      "current batch loss: 0.628254  [537600/900000]\n",
      "current batch loss: 0.622965  [544000/900000]\n",
      "current batch loss: 0.626184  [550400/900000]\n",
      "current batch loss: 0.634319  [556800/900000]\n",
      "current batch loss: 0.751946  [563200/900000]\n",
      "current batch loss: 0.629887  [569600/900000]\n",
      "current batch loss: 0.674193  [576000/900000]\n",
      "current batch loss: 0.696731  [582400/900000]\n",
      "current batch loss: 0.643466  [588800/900000]\n",
      "current batch loss: 0.628548  [595200/900000]\n",
      "current batch loss: 0.592884  [601600/900000]\n",
      "current batch loss: 0.610661  [608000/900000]\n",
      "current batch loss: 0.743832  [614400/900000]\n",
      "current batch loss: 0.591983  [620800/900000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.668695  [627200/900000]\n",
      "current batch loss: 0.722458  [633600/900000]\n",
      "current batch loss: 0.670944  [640000/900000]\n",
      "current batch loss: 0.676915  [646400/900000]\n",
      "current batch loss: 0.590723  [652800/900000]\n",
      "current batch loss: 0.610142  [659200/900000]\n",
      "current batch loss: 0.620663  [665600/900000]\n",
      "current batch loss: 0.641001  [672000/900000]\n",
      "current batch loss: 0.610551  [678400/900000]\n",
      "current batch loss: 0.548027  [684800/900000]\n",
      "current batch loss: 0.629788  [691200/900000]\n",
      "current batch loss: 0.625668  [697600/900000]\n",
      "current batch loss: 0.607571  [704000/900000]\n",
      "current batch loss: 0.612764  [710400/900000]\n",
      "current batch loss: 0.546668  [716800/900000]\n",
      "current batch loss: 0.626370  [723200/900000]\n",
      "current batch loss: 0.558896  [729600/900000]\n",
      "current batch loss: 0.661484  [736000/900000]\n",
      "current batch loss: 0.603515  [742400/900000]\n",
      "current batch loss: 0.582378  [748800/900000]\n",
      "current batch loss: 0.618824  [755200/900000]\n",
      "current batch loss: 0.629740  [761600/900000]\n",
      "current batch loss: 0.623336  [768000/900000]\n",
      "current batch loss: 0.639882  [774400/900000]\n",
      "current batch loss: 0.582710  [780800/900000]\n",
      "current batch loss: 0.583971  [787200/900000]\n",
      "current batch loss: 0.580670  [793600/900000]\n",
      "current batch loss: 0.631715  [800000/900000]\n",
      "current batch loss: 0.624885  [806400/900000]\n",
      "current batch loss: 0.628346  [812800/900000]\n",
      "current batch loss: 0.536163  [819200/900000]\n",
      "current batch loss: 0.685954  [825600/900000]\n",
      "current batch loss: 0.591854  [832000/900000]\n",
      "current batch loss: 0.578990  [838400/900000]\n",
      "current batch loss: 0.600459  [844800/900000]\n",
      "current batch loss: 0.660249  [851200/900000]\n",
      "current batch loss: 0.572682  [857600/900000]\n",
      "current batch loss: 0.591838  [864000/900000]\n",
      "current batch loss: 0.619354  [870400/900000]\n",
      "current batch loss: 0.599519  [876800/900000]\n",
      "current batch loss: 0.633673  [883200/900000]\n",
      "current batch loss: 0.639179  [889600/900000]\n",
      "current batch loss: 0.636486  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.630061\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.628949\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 39\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.623554  [    0/900000]\n",
      "current batch loss: 0.613666  [ 6400/900000]\n",
      "current batch loss: 0.601934  [12800/900000]\n",
      "current batch loss: 0.635808  [19200/900000]\n",
      "current batch loss: 0.646444  [25600/900000]\n",
      "current batch loss: 0.573218  [32000/900000]\n",
      "current batch loss: 0.626262  [38400/900000]\n",
      "current batch loss: 0.683872  [44800/900000]\n",
      "current batch loss: 0.601600  [51200/900000]\n",
      "current batch loss: 0.637223  [57600/900000]\n",
      "current batch loss: 0.654033  [64000/900000]\n",
      "current batch loss: 0.589410  [70400/900000]\n",
      "current batch loss: 0.604618  [76800/900000]\n",
      "current batch loss: 0.600692  [83200/900000]\n",
      "current batch loss: 0.588126  [89600/900000]\n",
      "current batch loss: 0.699767  [96000/900000]\n",
      "current batch loss: 0.630374  [102400/900000]\n",
      "current batch loss: 0.563376  [108800/900000]\n",
      "current batch loss: 0.669863  [115200/900000]\n",
      "current batch loss: 0.662468  [121600/900000]\n",
      "current batch loss: 0.619831  [128000/900000]\n",
      "current batch loss: 0.630585  [134400/900000]\n",
      "current batch loss: 0.670001  [140800/900000]\n",
      "current batch loss: 0.621662  [147200/900000]\n",
      "current batch loss: 0.649291  [153600/900000]\n",
      "current batch loss: 0.600733  [160000/900000]\n",
      "current batch loss: 0.633116  [166400/900000]\n",
      "current batch loss: 0.610511  [172800/900000]\n",
      "current batch loss: 0.714225  [179200/900000]\n",
      "current batch loss: 0.639317  [185600/900000]\n",
      "current batch loss: 0.599319  [192000/900000]\n",
      "current batch loss: 0.653423  [198400/900000]\n",
      "current batch loss: 0.604510  [204800/900000]\n",
      "current batch loss: 0.622364  [211200/900000]\n",
      "current batch loss: 0.590302  [217600/900000]\n",
      "current batch loss: 0.634327  [224000/900000]\n",
      "current batch loss: 0.647234  [230400/900000]\n",
      "current batch loss: 0.617362  [236800/900000]\n",
      "current batch loss: 0.711950  [243200/900000]\n",
      "current batch loss: 0.630685  [249600/900000]\n",
      "current batch loss: 0.540666  [256000/900000]\n",
      "current batch loss: 0.772896  [262400/900000]\n",
      "current batch loss: 0.612908  [268800/900000]\n",
      "current batch loss: 0.611054  [275200/900000]\n",
      "current batch loss: 0.614689  [281600/900000]\n",
      "current batch loss: 0.602145  [288000/900000]\n",
      "current batch loss: 0.652679  [294400/900000]\n",
      "current batch loss: 0.615019  [300800/900000]\n",
      "current batch loss: 0.638845  [307200/900000]\n",
      "current batch loss: 0.714559  [313600/900000]\n",
      "current batch loss: 0.560652  [320000/900000]\n",
      "current batch loss: 0.566587  [326400/900000]\n",
      "current batch loss: 0.640325  [332800/900000]\n",
      "current batch loss: 0.599975  [339200/900000]\n",
      "current batch loss: 0.620561  [345600/900000]\n",
      "current batch loss: 0.620635  [352000/900000]\n",
      "current batch loss: 0.621077  [358400/900000]\n",
      "current batch loss: 0.692464  [364800/900000]\n",
      "current batch loss: 0.568032  [371200/900000]\n",
      "current batch loss: 0.579103  [377600/900000]\n",
      "current batch loss: 0.647112  [384000/900000]\n",
      "current batch loss: 0.586521  [390400/900000]\n",
      "current batch loss: 0.641651  [396800/900000]\n",
      "current batch loss: 0.618990  [403200/900000]\n",
      "current batch loss: 0.626960  [409600/900000]\n",
      "current batch loss: 0.636502  [416000/900000]\n",
      "current batch loss: 0.645495  [422400/900000]\n",
      "current batch loss: 0.656874  [428800/900000]\n",
      "current batch loss: 0.686482  [435200/900000]\n",
      "current batch loss: 0.567944  [441600/900000]\n",
      "current batch loss: 0.591704  [448000/900000]\n",
      "current batch loss: 0.680387  [454400/900000]\n",
      "current batch loss: 0.595292  [460800/900000]\n",
      "current batch loss: 0.611802  [467200/900000]\n",
      "current batch loss: 0.624724  [473600/900000]\n",
      "current batch loss: 0.675966  [480000/900000]\n",
      "current batch loss: 0.685393  [486400/900000]\n",
      "current batch loss: 0.758140  [492800/900000]\n",
      "current batch loss: 0.643580  [499200/900000]\n",
      "current batch loss: 0.479134  [505600/900000]\n",
      "current batch loss: 0.630454  [512000/900000]\n",
      "current batch loss: 0.640643  [518400/900000]\n",
      "current batch loss: 0.655524  [524800/900000]\n",
      "current batch loss: 0.609249  [531200/900000]\n",
      "current batch loss: 0.598528  [537600/900000]\n",
      "current batch loss: 0.638582  [544000/900000]\n",
      "current batch loss: 0.651347  [550400/900000]\n",
      "current batch loss: 0.561728  [556800/900000]\n",
      "current batch loss: 0.597052  [563200/900000]\n",
      "current batch loss: 0.609182  [569600/900000]\n",
      "current batch loss: 0.628658  [576000/900000]\n",
      "current batch loss: 0.628767  [582400/900000]\n",
      "current batch loss: 0.551405  [588800/900000]\n",
      "current batch loss: 0.623272  [595200/900000]\n",
      "current batch loss: 0.677646  [601600/900000]\n",
      "current batch loss: 0.603924  [608000/900000]\n",
      "current batch loss: 0.669906  [614400/900000]\n",
      "current batch loss: 0.712063  [620800/900000]\n",
      "current batch loss: 0.666686  [627200/900000]\n",
      "current batch loss: 0.622387  [633600/900000]\n",
      "current batch loss: 0.698740  [640000/900000]\n",
      "current batch loss: 0.647491  [646400/900000]\n",
      "current batch loss: 0.684694  [652800/900000]\n",
      "current batch loss: 0.639216  [659200/900000]\n",
      "current batch loss: 0.620676  [665600/900000]\n",
      "current batch loss: 0.703685  [672000/900000]\n",
      "current batch loss: 0.663611  [678400/900000]\n",
      "current batch loss: 0.645722  [684800/900000]\n",
      "current batch loss: 0.588999  [691200/900000]\n",
      "current batch loss: 0.618819  [697600/900000]\n",
      "current batch loss: 0.568968  [704000/900000]\n",
      "current batch loss: 0.612022  [710400/900000]\n",
      "current batch loss: 0.678577  [716800/900000]\n",
      "current batch loss: 0.615675  [723200/900000]\n",
      "current batch loss: 0.617175  [729600/900000]\n",
      "current batch loss: 0.617231  [736000/900000]\n",
      "current batch loss: 0.583544  [742400/900000]\n",
      "current batch loss: 0.581539  [748800/900000]\n",
      "current batch loss: 0.559203  [755200/900000]\n",
      "current batch loss: 0.655185  [761600/900000]\n",
      "current batch loss: 0.587431  [768000/900000]\n",
      "current batch loss: 0.665255  [774400/900000]\n",
      "current batch loss: 0.688731  [780800/900000]\n",
      "current batch loss: 0.675240  [787200/900000]\n",
      "current batch loss: 0.595394  [793600/900000]\n",
      "current batch loss: 0.586254  [800000/900000]\n",
      "current batch loss: 0.616608  [806400/900000]\n",
      "current batch loss: 0.590339  [812800/900000]\n",
      "current batch loss: 0.733983  [819200/900000]\n",
      "current batch loss: 0.672044  [825600/900000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss: 0.658765  [832000/900000]\n",
      "current batch loss: 0.629716  [838400/900000]\n",
      "current batch loss: 0.675282  [844800/900000]\n",
      "current batch loss: 0.610084  [851200/900000]\n",
      "current batch loss: 0.628360  [857600/900000]\n",
      "current batch loss: 0.625560  [864000/900000]\n",
      "current batch loss: 0.581482  [870400/900000]\n",
      "current batch loss: 0.626643  [876800/900000]\n",
      "current batch loss: 0.628500  [883200/900000]\n",
      "current batch loss: 0.688079  [889600/900000]\n",
      "current batch loss: 0.593920  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.630711\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.629651\n",
      "-----------------------------------------------\n",
      "|\n",
      "-----------------------------------------------\n",
      "Epoch 40\n",
      "-----------------------------------------------\n",
      "current batch loss: 0.561900  [    0/900000]\n",
      "current batch loss: 0.586282  [ 6400/900000]\n",
      "current batch loss: 0.629438  [12800/900000]\n",
      "current batch loss: 0.578002  [19200/900000]\n",
      "current batch loss: 0.620551  [25600/900000]\n",
      "current batch loss: 0.656781  [32000/900000]\n",
      "current batch loss: 0.614256  [38400/900000]\n",
      "current batch loss: 0.668923  [44800/900000]\n",
      "current batch loss: 0.620617  [51200/900000]\n",
      "current batch loss: 0.578350  [57600/900000]\n",
      "current batch loss: 0.634621  [64000/900000]\n",
      "current batch loss: 0.648462  [70400/900000]\n",
      "current batch loss: 0.709539  [76800/900000]\n",
      "current batch loss: 0.644349  [83200/900000]\n",
      "current batch loss: 0.643621  [89600/900000]\n",
      "current batch loss: 0.581599  [96000/900000]\n",
      "current batch loss: 0.677843  [102400/900000]\n",
      "current batch loss: 0.594289  [108800/900000]\n",
      "current batch loss: 0.680062  [115200/900000]\n",
      "current batch loss: 0.741621  [121600/900000]\n",
      "current batch loss: 0.607819  [128000/900000]\n",
      "current batch loss: 0.584334  [134400/900000]\n",
      "current batch loss: 0.654528  [140800/900000]\n",
      "current batch loss: 0.642793  [147200/900000]\n",
      "current batch loss: 0.647438  [153600/900000]\n",
      "current batch loss: 0.642108  [160000/900000]\n",
      "current batch loss: 0.614221  [166400/900000]\n",
      "current batch loss: 0.628275  [172800/900000]\n",
      "current batch loss: 0.603998  [179200/900000]\n",
      "current batch loss: 0.676517  [185600/900000]\n",
      "current batch loss: 0.664736  [192000/900000]\n",
      "current batch loss: 0.600242  [198400/900000]\n",
      "current batch loss: 0.637172  [204800/900000]\n",
      "current batch loss: 0.676151  [211200/900000]\n",
      "current batch loss: 0.658619  [217600/900000]\n",
      "current batch loss: 0.660344  [224000/900000]\n",
      "current batch loss: 0.607794  [230400/900000]\n",
      "current batch loss: 0.688371  [236800/900000]\n",
      "current batch loss: 0.672134  [243200/900000]\n",
      "current batch loss: 0.701414  [249600/900000]\n",
      "current batch loss: 0.655810  [256000/900000]\n",
      "current batch loss: 0.625910  [262400/900000]\n",
      "current batch loss: 0.621844  [268800/900000]\n",
      "current batch loss: 0.622450  [275200/900000]\n",
      "current batch loss: 0.600147  [281600/900000]\n",
      "current batch loss: 0.660479  [288000/900000]\n",
      "current batch loss: 0.618534  [294400/900000]\n",
      "current batch loss: 0.600744  [300800/900000]\n",
      "current batch loss: 0.662037  [307200/900000]\n",
      "current batch loss: 0.613557  [313600/900000]\n",
      "current batch loss: 0.700329  [320000/900000]\n",
      "current batch loss: 0.575128  [326400/900000]\n",
      "current batch loss: 0.630318  [332800/900000]\n",
      "current batch loss: 0.592095  [339200/900000]\n",
      "current batch loss: 0.610851  [345600/900000]\n",
      "current batch loss: 0.567276  [352000/900000]\n",
      "current batch loss: 0.744037  [358400/900000]\n",
      "current batch loss: 0.636347  [364800/900000]\n",
      "current batch loss: 0.637159  [371200/900000]\n",
      "current batch loss: 0.589542  [377600/900000]\n",
      "current batch loss: 0.687942  [384000/900000]\n",
      "current batch loss: 0.649046  [390400/900000]\n",
      "current batch loss: 0.639674  [396800/900000]\n",
      "current batch loss: 0.683531  [403200/900000]\n",
      "current batch loss: 0.555662  [409600/900000]\n",
      "current batch loss: 0.623058  [416000/900000]\n",
      "current batch loss: 0.655450  [422400/900000]\n",
      "current batch loss: 0.635673  [428800/900000]\n",
      "current batch loss: 0.626972  [435200/900000]\n",
      "current batch loss: 0.643469  [441600/900000]\n",
      "current batch loss: 0.710663  [448000/900000]\n",
      "current batch loss: 0.674517  [454400/900000]\n",
      "current batch loss: 0.647429  [460800/900000]\n",
      "current batch loss: 0.643522  [467200/900000]\n",
      "current batch loss: 0.612609  [473600/900000]\n",
      "current batch loss: 0.624270  [480000/900000]\n",
      "current batch loss: 0.626622  [486400/900000]\n",
      "current batch loss: 0.597590  [492800/900000]\n",
      "current batch loss: 0.628250  [499200/900000]\n",
      "current batch loss: 0.709701  [505600/900000]\n",
      "current batch loss: 0.597263  [512000/900000]\n",
      "current batch loss: 0.567902  [518400/900000]\n",
      "current batch loss: 0.651603  [524800/900000]\n",
      "current batch loss: 0.619528  [531200/900000]\n",
      "current batch loss: 0.627384  [537600/900000]\n",
      "current batch loss: 0.669324  [544000/900000]\n",
      "current batch loss: 0.703903  [550400/900000]\n",
      "current batch loss: 0.675149  [556800/900000]\n",
      "current batch loss: 0.572609  [563200/900000]\n",
      "current batch loss: 0.620138  [569600/900000]\n",
      "current batch loss: 0.636568  [576000/900000]\n",
      "current batch loss: 0.613135  [582400/900000]\n",
      "current batch loss: 0.589427  [588800/900000]\n",
      "current batch loss: 0.633353  [595200/900000]\n",
      "current batch loss: 0.652355  [601600/900000]\n",
      "current batch loss: 0.579663  [608000/900000]\n",
      "current batch loss: 0.568891  [614400/900000]\n",
      "current batch loss: 0.581215  [620800/900000]\n",
      "current batch loss: 0.617497  [627200/900000]\n",
      "current batch loss: 0.588343  [633600/900000]\n",
      "current batch loss: 0.647871  [640000/900000]\n",
      "current batch loss: 0.654548  [646400/900000]\n",
      "current batch loss: 0.588568  [652800/900000]\n",
      "current batch loss: 0.620268  [659200/900000]\n",
      "current batch loss: 0.572131  [665600/900000]\n",
      "current batch loss: 0.642825  [672000/900000]\n",
      "current batch loss: 0.638670  [678400/900000]\n",
      "current batch loss: 0.683965  [684800/900000]\n",
      "current batch loss: 0.632971  [691200/900000]\n",
      "current batch loss: 0.615152  [697600/900000]\n",
      "current batch loss: 0.652731  [704000/900000]\n",
      "current batch loss: 0.660927  [710400/900000]\n",
      "current batch loss: 0.621436  [716800/900000]\n",
      "current batch loss: 0.576604  [723200/900000]\n",
      "current batch loss: 0.638122  [729600/900000]\n",
      "current batch loss: 0.607412  [736000/900000]\n",
      "current batch loss: 0.674130  [742400/900000]\n",
      "current batch loss: 0.556243  [748800/900000]\n",
      "current batch loss: 0.632471  [755200/900000]\n",
      "current batch loss: 0.598121  [761600/900000]\n",
      "current batch loss: 0.595667  [768000/900000]\n",
      "current batch loss: 0.615013  [774400/900000]\n",
      "current batch loss: 0.684548  [780800/900000]\n",
      "current batch loss: 0.637525  [787200/900000]\n",
      "current batch loss: 0.630537  [793600/900000]\n",
      "current batch loss: 0.642371  [800000/900000]\n",
      "current batch loss: 0.563895  [806400/900000]\n",
      "current batch loss: 0.598138  [812800/900000]\n",
      "current batch loss: 0.654158  [819200/900000]\n",
      "current batch loss: 0.622712  [825600/900000]\n",
      "current batch loss: 0.594146  [832000/900000]\n",
      "current batch loss: 0.562985  [838400/900000]\n",
      "current batch loss: 0.652159  [844800/900000]\n",
      "current batch loss: 0.599869  [851200/900000]\n",
      "current batch loss: 0.698826  [857600/900000]\n",
      "current batch loss: 0.594486  [864000/900000]\n",
      "current batch loss: 0.615696  [870400/900000]\n",
      "current batch loss: 0.647738  [876800/900000]\n",
      "current batch loss: 0.615896  [883200/900000]\n",
      "current batch loss: 0.649610  [889600/900000]\n",
      "current batch loss: 0.635021  [896000/900000]\n",
      "-----------------------------------------------\n",
      "avg trn loss per batch: 0.630528\n",
      "-----------------------------------------------\n",
      "avg val loss per batch: 0.629463\n",
      "-----------------------------------------------\n",
      "|\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# a useful function to present things clearer\n",
    "def separator():\n",
    "    print( \"-----------------------------------------------\" )\n",
    "\n",
    "epochs = 40\n",
    "\n",
    "# re-initialise the model and the optimizer\n",
    "model_oc = cwolaNet( 4, 64 ).to(device)\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam( model_oc.parameters(), lr=learning_rate )\n",
    "separator()\n",
    "print( \"model architecture \")\n",
    "separator()\n",
    "print( model )\n",
    "\n",
    "# track train and val losses\n",
    "trn_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for t in range(epochs):\n",
    "    separator()\n",
    "    print( f\"Epoch {t+1}\" )\n",
    "    separator()\n",
    "    train_epoch( trn_dataloader, model_oc, loss_fn, optimizer )\n",
    "    separator()\n",
    "    trn_loss = trn_pass( trn_dataloader, model_oc, loss_fn )\n",
    "    trn_losses.append( trn_loss )\n",
    "    separator()\n",
    "    val_loss = val_pass( val_dataloader, model_oc, loss_fn )\n",
    "    val_losses.append( val_loss )\n",
    "    separator()\n",
    "    print( \"|\" )\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how it performs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_oc = model_oc( X_test_p ).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(14,5))\n",
    "\n",
    "fpr, tpr, th = roc_curve( y_test, test_pred_oc )\n",
    "auc_score = roc_auc_score( y_test, test_pred_oc )\n",
    "rnd_class = np.linspace(0, 1, 100)\n",
    "\n",
    "ax[0].plot(fpr, tpr, label='AUC = {:.2f}'.format(auc_score) )\n",
    "ax[0].plot(rnd_class, rnd_class, '--', label='Rnd classifier')\n",
    "ax[1].plot(tpr, 1/fpr, label='AUC = {:.2f}\\n $1/\\epsilon_{{bkg}}$(0.3) = {:.0f}'.format(auc_score, 1/fpr[closest_point(tpr, tpr_p=0.3)]))\n",
    "ax[1].plot(rnd_class, 1/rnd_class, '--', label='Rnd classifier')\n",
    "ax[1].set_yscale('log')\n",
    "\n",
    "ax[0].set_xlabel('$\\epsilon_{bkg}$ - FPR', fontproperties=axislabelfont)\n",
    "ax[0].set_ylabel('$\\epsilon_{s}$ - TPR', fontproperties=axislabelfont)\n",
    "\n",
    "ax[1].set_xlabel('$\\epsilon_{s}$ - TPR', fontproperties=axislabelfont)\n",
    "ax[1].set_ylabel('1/$\\epsilon_{bkg}$ - Inverse FPR', fontproperties=axislabelfont)\n",
    "for i in range(len(ax)):\n",
    "    ax[i].legend(prop=axislabelfont)\n",
    "    ax[i].tick_params(labelsize=axisfontsize)\n",
    "    ax[i].grid('on')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots( figsize=(7,5))\n",
    "\n",
    "fpr_sb10pc, tpr_sb10pc, th_sb10pc = roc_curve( y_test, test_pred )\n",
    "fpr_oc, tpr_oc, th_oc = roc_curve( y_test, test_pred_oc )\n",
    "\n",
    "auc_score_sb10pc = roc_auc_score( y_test, test_pred )\n",
    "auc_score_oc = roc_auc_score( y_test, test_pred_oc )\n",
    "\n",
    "rnd_class = np.linspace(0, 1, 100)\n",
    "\n",
    "ax.plot(tpr_sb10pc, 1/fpr_sb10pc, label='CWoLa, AUC = {:.2f}'.format(auc_score_sb10pc))\n",
    "ax.plot(tpr_oc, 1/fpr_oc, label='Optimal CWoLa, AUC = {:.2f}'.format(auc_score_oc))\n",
    "#ax.plot(rnd_class, 1/rnd_class, '--', label='Rnd classifier')\n",
    "ax.set_yscale('log')\n",
    "\n",
    "ax.set_xlabel('$\\epsilon_{s}$ - TPR', fontproperties=axislabelfont)\n",
    "ax.set_ylabel('1/$\\epsilon_{bkg}$ - Inverse FPR', fontproperties=axislabelfont)\n",
    "ax.legend(prop=axislabelfont)\n",
    "ax.tick_params(labelsize=axisfontsize)\n",
    "ax.grid('on')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an experiment we want to use all of the data, but we also need to validate and test.  We can do this with cross-validation.\n",
    "\n",
    "1. split data into $k$ clusters\n",
    "2. use $k\\!-\\!1$ of the clusters to construct training and validation data\n",
    "3. optimise the network and test on the $k^{\\text{th}}$ cluster\n",
    "4. repeat steps 2 and 3 $k$ times, iterating through the cluster used for testing\n",
    "5. aggregate the results so that all of the data has been tested on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
